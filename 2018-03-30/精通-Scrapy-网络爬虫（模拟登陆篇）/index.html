<!doctype html><html class="theme-next mist use-motion" lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><script src="//cdn.bootcss.com/pace/1.0.2/pace.min.js"></script><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><meta name="google-site-verification" content="evAx-EdEyUEd-j9zqy2DLYlZNcDZYig2uwZF6K6SOSU"><meta name="baidu-site-verification" content="PqQ6XK9DFN"><link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"><link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css"><link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css"><link href="/css/main.css?v=5.1.1" rel="stylesheet" type="text/css"><meta name="keywords" content="Scrapy,"><link rel="alternate" href="/atom.xml" title="那小子真帅" type="application/atom+xml"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.1"><meta name="description" content="没事了，约约几个老伙计，喝喝茶，下下棋"><meta name="keywords" content="Scrapy"><meta property="og:type" content="article"><meta property="og:title" content="精通 Scrapy 网络爬虫（模拟登陆篇）"><meta property="og:url" content="http://blog.dongfei.xin/2018-03-30/精通-Scrapy-网络爬虫（模拟登陆篇）/index.html"><meta property="og:site_name" content="那小子真帅"><meta property="og:description" content="没事了，约约几个老伙计，喝喝茶，下下棋"><meta property="og:locale" content="zh-Hans"><meta property="og:image" content="https://dongfei.oss-cn-shanghai.aliyuncs.com/Example%20web%20scraping%20website%20.jpg"><meta property="og:image" content="https://dongfei.oss-cn-shanghai.aliyuncs.com/Example%20web%20scraping%20website%20123.jpg"><meta property="og:image" content="https://dongfei.oss-cn-shanghai.aliyuncs.com/Example%20web%20scraping%20website%2012.jpg"><meta property="og:image" content="https://dongfei.oss-cn-shanghai.aliyuncs.com/cqke.jpg"><meta property="og:image" content="https://dongfei.oss-cn-shanghai.aliyuncs.com/sdcsd.jpg"><meta property="og:image" content="https://dongfei.oss-cn-shanghai.aliyuncs.com/sdcsd.jpg"><meta property="og:image" content="https://dongfei.oss-cn-shanghai.aliyuncs.com/%E7%9F%A5%E4%B9%8E%E4%B9%8B.jpg"><meta property="og:updated_time" content="2018-05-02T03:15:15.073Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="精通 Scrapy 网络爬虫（模拟登陆篇）"><meta name="twitter:description" content="没事了，约约几个老伙计，喝喝茶，下下棋"><meta name="twitter:image" content="https://dongfei.oss-cn-shanghai.aliyuncs.com/Example%20web%20scraping%20website%20.jpg"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Mist",sidebar:{position:"right",display:"post",offset:12,offset_float:0,b2t:!1,scrollpercent:!1},fancybox:!0,motion:!0,duoshuo:{userId:"0",author:"博主"},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><link rel="canonical" href="http://blog.dongfei.xin/2018-03-30/精通-Scrapy-网络爬虫（模拟登陆篇）/"><script type="text/javascript" src="https://dongfei.oss-cn-shanghai.aliyuncs.com/high/high-animation.js"></script><script type="text/javascript" src="https://dongfei.oss-cn-shanghai.aliyuncs.com/blog/blog.dongfei.xin.js"></script><title>精通 Scrapy 网络爬虫（模拟登陆篇） | 那小子真帅</title></head><body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans"><script>!function(e,a,t,n,g,c,o){e.GoogleAnalyticsObject=g,e.ga=e.ga||function(){(e.ga.q=e.ga.q||[]).push(arguments)},e.ga.l=1*new Date,c=a.createElement(t),o=a.getElementsByTagName(t)[0],c.async=1,c.src="https://www.google-analytics.com/analytics.js",o.parentNode.insertBefore(c,o)}(window,document,"script",0,"ga"),ga("create","100009466-1","auto"),ga("send","pageview")</script><script type="text/javascript">var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?8feab02e675ff80af4ab58058d0fcd46";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><div class="container sidebar-position-right page-post-detail"><div class="headband"></div><a href="https://github.com/GFigure"><img style="position:absolute;top:0;left:0;border:0" src="https://dongfei.oss-cn-shanghai.aliyuncs.com/forkme_left_orange_ff7600.png" alt="Fork me on GitHub"></a><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-wrapper"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">那小子真帅</span> <span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle">码农，程序猿，技术控</p></div><div class="site-nav-toggle"><button><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>首页</a></li><li class="menu-item menu-item-archives"><a href="/archives" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>归档</a></li><li class="menu-item menu-item-categories"><a href="/categories" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i><br>分类</a></li><li class="menu-item menu-item-tags"><a href="/tags" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br>标签</a></li><li class="menu-item menu-item-commonweal"><a href="/404.html" rel="section"><i class="menu-item-icon fa fa-fw fa-heartbeat"></i><br>公益404</a></li><li class="menu-item menu-item-about"><a href="/about" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i><br>关于</a></li><li class="menu-item menu-item-search"><a title="小high一下~" style="color:red" rel="alternate" class="mw-harlem_shake_slow wobble shake" href="javascript:shake()"><i class="fa fa-music"></i> &nbsp;&nbsp;High</a></li><li class="menu-item menu-item-search"><a href="javascript:;" class="popup-trigger"><i class="menu-item-icon fa fa-search fa-fw"></i><br>搜索</a></li></ul><div class="site-search"><div class="popup search-popup local-search-popup"><div class="local-search-header clearfix"><span class="search-icon"><i class="fa fa-search"></i> </span><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span><div class="local-search-input-wrapper"><input autocapitalize="off" autocomplete="off" autocorrect="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input"></div></div><div id="local-search-result"></div></div></div></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://blog.dongfei.xin/2018-03-30/精通-Scrapy-网络爬虫（模拟登陆篇）/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="那小子真帅"><meta itemprop="description" content=""><meta itemprop="image" content="http://oqiflua2i.bkt.clouddn.com/%E5%9B%BE%E7%89%878.png"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="那小子真帅"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">精通 Scrapy 网络爬虫（模拟登陆篇）</h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-03-30T18:33:10+08:00">2018-03-30 </time></span><span class="post-category"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Python/" itemprop="url" rel="index"><span itemprop="name">Python</span> </a></span>， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Python/Scrapy/" itemprop="url" rel="index"><span itemprop="name">Scrapy</span> </a></span></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a class="cloud-tie-join-count" href="/2018-03-30/精通-Scrapy-网络爬虫（模拟登陆篇）/#comments" itemprop="discussionUrl"><span class="post-comments-count join-count" itemprop="commentCount"></span> </a></span><span id="/2018-03-30/精通-Scrapy-网络爬虫（模拟登陆篇）/" class="leancloud_visitors" data-flag-title="精通 Scrapy 网络爬虫（模拟登陆篇）"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">阅读次数 </span><span class="leancloud-visitors-count"></span></span></div></header><div class="post-body" itemprop="articleBody"><blockquote class="blockquote-center" style="">没事了，约约几个老伙计，喝喝茶，下下棋<br></blockquote><p><img src="https://dongfei.oss-cn-shanghai.aliyuncs.com/Example%20web%20scraping%20website%20.jpg" alt="http://example.webscraping.com/"></p><a id="more"></a><h5 id="模拟登陆"><a href="#模拟登陆" class="headerlink" title="模拟登陆"></a>模拟登陆</h5><p>登录的实质是向服务器发送含有登录表单数据的 HTTP 请求（通常是 POST）</p><h6 id="使用-FromRequest"><a href="#使用-FromRequest" class="headerlink" title="使用 FromRequest"></a>使用 FromRequest</h6><p>Scrapy 提供了一个 FormRequest 类（Request的子类），专门用于构造含有表单数据的请求，FormRequest 的构造器方法有一个formdata 参数，接收字典形式的表单数据</p><p>在scrapy shell 环境下演示如何使用FormRequest 模拟登录。首先爬取登录页面：<a href="http://example.webscraping.com/places/default/user/login?_next=/places/default/index" target="_blank" rel="noopener">链接</a></p><p><img src="https://dongfei.oss-cn-shanghai.aliyuncs.com/Example%20web%20scraping%20website%20123.jpg" alt=""></p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;</span>&gt; scrapy shell <span class="symbol">http:</span>/<span class="regexp">/example.webscraping.com/places</span><span class="regexp">/default/user</span><span class="regexp">/login?_next=/places</span><span class="regexp">/default/index</span></span><br><span class="line"></span><br><span class="line">表单数据应包含的信息：帐号和密码信息，再加 <span class="number">3</span>个隐藏 &lt;input&gt; 中的信息（&lt;div style=<span class="string">"display:none"</span>）。</span><br><span class="line">先把这些信息收集到一个字典中，然后使用这个表单数据字典构造 FormRequest 对象</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; sel = response.xpath(<span class="string">'//div[@style]/input'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; sel</span><br><span class="line">[&lt;Selector xpath=<span class="string">'//div[@style]/input'</span> data=<span class="string">'&lt;input name="_next" type="hidden" value='</span>&gt;，</span><br><span class="line">&lt;Selector xpath=<span class="string">'//div[@style]/input'</span> data=<span class="string">'&lt;input name="_formkey" type="hidden" value='</span>&gt;,</span><br><span class="line">&lt;Selector xpathe=<span class="string">'//div[@style]/input'</span> data=<span class="string">'&lt;input name="_formname" type="hidden" value='</span>&gt;]</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt;<span class="comment"># 构造表单数据字典</span></span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; fd = dict(zip(sel.xpath(<span class="string">'./@name'</span>).extract(),sel.xpath(<span class="string">'./@value'</span>).extract()))</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; fd</span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">'_formkey'</span>: <span class="string">'432dcb0c-0d8s443fbb50-9644cft212b'</span>,</span><br><span class="line">    <span class="string">'_formname'</span>: <span class="string">'login'</span>,</span><br><span class="line">    <span class="string">'_next'</span>: <span class="string">'/'</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; <span class="comment"># 填写账号和密码信息</span></span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; fd[<span class="string">"email"</span>] = <span class="string">"3543503058@qq.com"</span></span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; fd[<span class="string">"password"</span>] = <span class="string">"webscraping.com"</span></span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; fd</span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">'_formkey'</span>: <span class="string">'432dcb0c-0d8s443fbb50-9644cft212b'</span>,</span><br><span class="line">    <span class="string">'_formname'</span>: <span class="string">'login'</span>,</span><br><span class="line">    <span class="string">'_next'</span>: <span class="string">'/'</span>,</span><br><span class="line">    <span class="string">'email'</span>: <span class="string">'3543503058@qq.com'</span>,</span><br><span class="line">    <span class="string">'password'</span>: <span class="string">'webscraping.com'</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; from scrapy.http import FormRequest</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; request = FormRequest(<span class="string">'http://example.webscraping.com/places/default/user/login?_next=/places/default/index'</span>, formdata = fd)</span><br></pre></td></tr></table></figure><p>以上指直接构造 FormRequest 对象的方式，除此之外还有一种更为简单的方式，即调用 FormRequest 的 from_response 方法，调用时需要传入一个 Response 对象作为第一参数，该方法会解析 Response 对象所包含页面中的 <code>&lt;form&gt;</code> 元素，帮助用户创建 FromRequest 对象，并将隐藏 <code>&lt;input&gt;</code> 中的信息自动填入表单数据，使用这种方式，我们只需通过 formdata 参数填写账号和密码即可，代码如下：</p><figure class="highlight xquery"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; scrapy shell http://example.webscraping.com/places/<span class="keyword">default</span>/user/login?_<span class="keyword">next</span>=/places/<span class="keyword">default</span>/index</span><br><span class="line">&gt;&gt;&gt; from scrapy.http <span class="keyword">import</span> FormRequest</span><br><span class="line">&gt;&gt;&gt; fd = &#123;<span class="string">'email'</span>:<span class="string">"3543503058@qq.com"</span>, <span class="string">'password'</span>:<span class="string">"webscraping.com"</span>&#125;</span><br><span class="line">&gt;&gt;&gt; request = FormRequest.from_response(response, formdata = fd )</span><br><span class="line">&gt;&gt;&gt; fetch(request)</span><br><span class="line"><span class="number">2018</span>-<span class="number">03</span>-<span class="number">30</span> <span class="number">21</span>:<span class="number">16</span>:<span class="number">51</span> [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (<span class="number">303</span>) <span class="keyword">to</span> &lt;GET http://example.webscraping.com/places/<span class="keyword">default</span>/index&gt; from &lt;POST http://example.webscraping.com/places/<span class="keyword">default</span>/user/login?_<span class="keyword">next</span>=<span class="meta">%2Fplaces</span><span class="meta">%2Fdefault</span><span class="meta">%2Findex</span>&gt;</span><br><span class="line"><span class="number">2018</span>-<span class="number">03</span>-<span class="number">30</span> <span class="number">21</span>:<span class="number">16</span>:<span class="number">51</span> [scrapy.core.engine] DEBUG: Crawled (<span class="number">200</span>) &lt;GET http://example.webscraping.com/places/<span class="keyword">default</span>/index&gt; (referer: None)</span><br></pre></td></tr></table></figure><p>在 log 信息中，可以看到和浏览器提交表单时类似的情形：POST 请求的响应状态码为 303，之后 Scrapy 自动再发送下一个 GET 请求下载跳转页面。此时，可以通过在页面中查找特殊字符串或在浏览器中查看页面是否登录成功。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">'Welcome jason'</span> <span class="keyword">in</span> response.text</span><br><span class="line"><span class="keyword">True</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>view(response)</span><br><span class="line"><span class="keyword">True</span></span><br></pre></td></tr></table></figure><p>验证结果表明模拟登录成功了。显然，Scrapy 发送的第 2 个 GET 请求携带了第 1 个 POST 请求获取的 Cookie 信息，为请求附加 Cookie 信息的工作是由 Scrapy 内置的下载中间件 CookiesMiddleware 自动完成的。现在，我们可以继续发送请求，爬取那些只有登录后才能获取的信息了，这里以爬取用户个人信息为例：</p><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; fetch(<span class="string">"http://example.webscraping.com/places/default/user/profile"</span>)</span><br><span class="line"><span class="number">2018</span><span class="number">-03</span><span class="number">-30</span> <span class="number">21</span>:<span class="number">31</span>:<span class="number">04</span> [scrapy.core.engine] <span class="type">DEBUG</span>: <span class="type">Crawled</span> (<span class="number">200</span>) &lt;<span class="type">GET</span> http://example.webscraping.com/places/<span class="keyword">default</span>/user/profile&gt; (<span class="title">referer</span>: <span class="type">None</span>)</span><br><span class="line">&gt;&gt;&gt; view(response)</span><br><span class="line"><span class="type">True</span></span><br></pre></td></tr></table></figure><p><img src="https://dongfei.oss-cn-shanghai.aliyuncs.com/Example%20web%20scraping%20website%2012.jpg" alt="个人信息"></p><p><strong>实现登录 LoginSpider</strong></p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line">__author_<span class="number">_</span> = <span class="string">"東飛"</span></span><br><span class="line">__date_<span class="number">_</span> = <span class="string">"2017-11-29"</span></span><br><span class="line"></span><br><span class="line">import scrapy</span><br><span class="line">from scrapy.http import Request, FormRequest</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LoginSpider</span>(<span class="title">scrapy</span>.<span class="title">Spider</span>):</span></span><br><span class="line">    name = <span class="string">'login'</span></span><br><span class="line">    allowed_domains = [<span class="string">'example.webscraping.com'</span>]</span><br><span class="line">    start_urls = [<span class="string">'http://example.webscraping.com/places/default/user/profile'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(<span class="keyword">self</span>, response)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="comment"># 解析登录后下载的页面，此例中为用户个人信息页面</span></span><br><span class="line">        keys = response.css(<span class="string">'table label::text'</span>).re(<span class="string">'(.+):'</span>)</span><br><span class="line">        values = response.css(<span class="string">'table td.w2p_fw::text'</span>).extract()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">yield</span> dict(zip(keys, values))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># ------------------------登录-----------------------</span></span><br><span class="line">    <span class="comment"># 登录页面的url</span></span><br><span class="line">    login_url = <span class="string">'http://example.webscraping.com/places/default/user/login?_next=/places/default/index'</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">start_requests</span><span class="params">(<span class="keyword">self</span>)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">yield</span> Request(<span class="keyword">self</span>.login_url, callback=<span class="keyword">self</span>.login)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">login</span><span class="params">(<span class="keyword">self</span>, response)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="comment"># 登录页面的解析函数，构造FormRequest 对象提交表单</span></span><br><span class="line">        fd = &#123;</span><br><span class="line">            <span class="string">'email'</span>: <span class="string">"3543503058@qq.com"</span>,</span><br><span class="line">            <span class="string">'password'</span>: <span class="string">"webscraping.com"</span></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">yield</span> FormRequest.from_response(</span><br><span class="line">                response,</span><br><span class="line">                formdata=fd,</span><br><span class="line">                callback=<span class="keyword">self</span>.parse_login</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_login</span><span class="params">(<span class="keyword">self</span>, response)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="comment"># 登录成功后，继续爬取 start_urls 中的页面</span></span><br><span class="line">        <span class="keyword">if</span> <span class="string">'Welcome jason'</span> <span class="keyword">in</span> response.<span class="symbol">text:</span></span><br><span class="line">            <span class="keyword">yield</span> from <span class="keyword">super</span>().start_requests()  <span class="comment"># Python 3 语法</span></span><br></pre></td></tr></table></figure><p>解释上述代马如下:</p><ul><li><p>覆写基类的 start_requests 方法，最先请求登录页面。</p></li><li><p>login 方法为登录页面的解析函数，在该方法中进行模拟登录，构造表单请求并提交。</p></li><li><p>parse_login 方法为表单请求的响应处理函数，在该方法中通过在页面查找特殊字符串 ‘Welcome jason’ 判断是否登录成功，如果成功，调用基类的 start_requests 方法，继续爬取 start_urls 中的页面</p></li></ul><h5 id="识别验证码"><a href="#识别验证码" class="headerlink" title="识别验证码"></a>识别验证码</h5><h6 id="OCR识别"><a href="#OCR识别" class="headerlink" title="OCR识别"></a>OCR识别</h6><p>OCR 是光学字符识别的缩写，用于在图像中提取文本信息，tesseract-ocr 是利用该技术实现的一个验证码识别库，在 Python 中可以通过第三方库 pytesseract 调用它。下面介绍如何使用 pytesseract 识别验证码：<br><img src="https://dongfei.oss-cn-shanghai.aliyuncs.com/cqke.jpg" alt="cqke"></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">首先安装 tesseract-ocr,在Ubuntu 下可以使用apt-get 安装:</span><br><span class="line">sudo apt-get install tesseract-ocr</span><br><span class="line"></span><br><span class="line">安装 pytesseract，依赖于 Python 图像处理库 PIL 或 Pillow PIL （PIL 和 Pillow PIL 功能类似，任选其一）。</span><br><span class="line">使用 pip 安装:</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> pip install pillow</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> pip install pytesseract</span></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; from PIL import Image</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; import pytesseract</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; img = Image.open(<span class="string">'code.png'</span>)</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; img = img.convert(<span class="string">'L'</span>)</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; pytesseract.image_to_string(img)</span></span><br><span class="line">'cqKE'</span><br></pre></td></tr></table></figure><p>上面的代码中，先使用 Image.open 打开图片，为了提高识别率，调用 Image 对象的 convert 方法把图片转换为黑白图，然后将黑白图传递给 <code>pytesseract.image_to_string</code> 方法进行识别，这里我们幸运地识别成功了。经测试，此段代码对于 X 网站中的验证码识别率可以达到 72%，这已经足够高了。</p><p>下面我们以之前的 LoginSpider 为模板实现一个使用 pytesseract 识别验证码登录的 Spider：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line">__author__ = <span class="string">"東飛"</span></span><br><span class="line">__date__ = <span class="string">"2017-11-29"</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> Request, FormRequest</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> io <span class="keyword">import</span> BytesIO</span><br><span class="line"><span class="keyword">import</span> pytesseract</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> scrapy.log <span class="keyword">import</span> logger</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CaptchaLoginSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">"login_captcha"</span></span><br><span class="line">    start_urls = [<span class="string">'http://XXX.com/'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">    <span class="comment"># X 网站登录页面的 url (虚构的)</span></span><br><span class="line">    login_url = <span class="string">'http://XXX.com/login'</span></span><br><span class="line">    user = <span class="string">'jasonme@XXX.com'</span></span><br><span class="line">    password = <span class="string">'12345678'</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">start_requests</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">yield</span> Request(self.login_url, callback=self.login, dont_filter=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">login</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="comment"># 该方法既是登录页面的解析函数，又是下载验证码图片的响应处理函数</span></span><br><span class="line">        <span class="comment"># 如果 response.meta['login_response'] 存在，当前 response 为验证码图片的响应</span></span><br><span class="line">        <span class="comment"># 否则当前 response 为登录页面的响应</span></span><br><span class="line">        login_response = response.meta.get[<span class="string">'login_response'</span>]</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> login_response:</span><br><span class="line">            <span class="comment"># Step 1:</span></span><br><span class="line">            <span class="comment"># 此时 response 为登录页面的响应，从中提取验证码图片的 url，下载验证码图片</span></span><br><span class="line"></span><br><span class="line">            captchaUrl = response.css(<span class="string">'label.field.prepend-icon img::attr(src)'</span>).extract_first()</span><br><span class="line"></span><br><span class="line">            captchaUrl = response.urljoin(captchaUrl)</span><br><span class="line">            <span class="comment"># 构造 Request 时，将当前 response 保存到 meta 字典中</span></span><br><span class="line">            <span class="keyword">yield</span> Request(</span><br><span class="line">                    captchaUrl,</span><br><span class="line">                    callback=self.login,</span><br><span class="line">                    meta=&#123;</span><br><span class="line">                        login_response: response</span><br><span class="line">                    &#125;,</span><br><span class="line">                    dont_filter=<span class="keyword">True</span></span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># Step 2:</span></span><br><span class="line">            <span class="comment"># 此时，response 为验证码图片的响应，response.body 是图片二进制数据</span></span><br><span class="line">            <span class="comment"># login_response 为登录页面的响应，用其构造表单请求并发送</span></span><br><span class="line">            formdata = &#123;</span><br><span class="line">                <span class="string">'email'</span>: self.user,</span><br><span class="line">                <span class="string">'pass'</span>: self.password,</span><br><span class="line">                <span class="comment"># 使用OCR 识别</span></span><br><span class="line">                <span class="string">'code'</span>: self.get_captcha_by_OCR(response.body),</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">yield</span> FormRequest.from_response(</span><br><span class="line">                    login_response,</span><br><span class="line">                    callback=self.parse_login,</span><br><span class="line">                    formdata=formdata,</span><br><span class="line">                    dont_filter = <span class="keyword">True</span></span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_login</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="comment"># 根据响应结果判断是否登录成功</span></span><br><span class="line">        info = json.loads(response.text)</span><br><span class="line">        <span class="keyword">if</span> info[<span class="string">'error'</span>] == <span class="string">'0'</span>:</span><br><span class="line">            logger.info(<span class="string">'登录成功:-)'</span>)</span><br><span class="line">        <span class="keyword">return</span> super().start_requests()</span><br><span class="line"></span><br><span class="line">        logger.info(<span class="string">'登录失败:-(, 重新登录....'</span>)</span><br><span class="line">        <span class="keyword">return</span> self.start_requests()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_captcha_by_OCR</span><span class="params">(self, data)</span>:</span></span><br><span class="line">        <span class="comment"># OCR 识别</span></span><br><span class="line">        img = Image.open(BytesIO(data))</span><br><span class="line">        img = img.convert(<span class="string">'L'</span>)</span><br><span class="line">        captcha = pytesseract.image_to_string(img)</span><br><span class="line">        img.close()</span><br><span class="line">        <span class="keyword">return</span> captcha</span><br></pre></td></tr></table></figure><p>解释上述代码如下:</p><ul><li>login 方法</li></ul><p>带有登证玛的登录，需要额外发送一个 HTTP 请求来获取验证码图片，这里的 login 方法既处理下载登录页面的响应，又处理下载验证码图片的响应。</p><p>解析登录页面时，提取验证码图片的 url ，发送请求下载图片，并将登录页面的<br>Response 对象保存到 Request 对象的 meta 字典中。</p><p>处理下载验证码图片的响应时，调用 get_captcha_by_OCR 方法识别图片中的验证码，然后将之前保存的登录页面的 Response 对象取出，构造 FormRequest 对象并提交。</p><ul><li>get_captcha_by_OCR 方法</li></ul><p>参教 data 是验证码图片的二进制数据，类型为 bytes ，想要使用 Image.open 函数构造 Image 对象先要把图片的二进制数据转换成某种类文件对象，这里使用 BytesIO<br>进行包裹，获得 Image 对象后先将其转换成黑白图，然后调用 pytesseract.image_to_string 方法进行识别。</p><ul><li>parse_login 方法</li></ul><p>处理表单请求的响应。响应正文是一个 json 串，其中包含了用户验证的结果，先调用 json.loads 将正文转换为 Python 宇典，然后依据其中 error 字段的值判断登录是否<br>成功，若登录成功，则从 start_urls 中的页面开始爬取；若登录失败，则重新登录。</p><h6 id="网络平台识别"><a href="#网络平台识别" class="headerlink" title="网络平台识别"></a>网络平台识别</h6><p>使用 pytesseract 识别的验证码比较简单，对于某些复杂的验证码，pytesseract 的识别率很低或者无法识别。目前，有很多网站专门提供验证码识别服务，可以识别较为复杂的验证码（有些是人工处理的），它们被称之为验证码识别平台，这平台多数是付费使用的，价格大约为 1元钱识别 100 个验证码，平台提供了 HTTP 服务接口，用户可以通过 HTTP 请求将验证码图片发送给平台，平台识别后将结果通过 HTTP 响应返回响应务接口，</p><p>在阿里云市场可以找到很多验证码识别平台：<br><img src="https://dongfei.oss-cn-shanghai.aliyuncs.com/sdcsd.jpg" alt="验证码识别平台"></p><p><img src="https://dongfei.oss-cn-shanghai.aliyuncs.com/sdcsd.jpg" alt="验证码识别平台"></p><p>阅读 API 文档，实现代码如下：</p><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">import urllib, urllib2, sys</span><br><span class="line"></span><br><span class="line">host = 'http://ali-make-mark.showapi.com'</span><br><span class="line">path = '/make-mark-img'</span><br><span class="line">method = 'GET'</span><br><span class="line">appcode = '你自己的AppCode'</span><br><span class="line">querys = 'border=yes&amp;border_color=105%2C179%2C90&amp;border_thickness=1&amp;image_height=50&amp;image_width=200&amp;noise_color=black&amp;obscurificator_impl=com.google.code.kaptcha.impl.WaterRipple&amp;textproducer_char_length=5&amp;textproducer_char_space=2&amp;textproducer_char_string=abcde2345678gfynmnpwx&amp;textproducer_font_color=black&amp;textproducer_font_names=Arial%2C+Courier&amp;textproducer_font_size=40'</span><br><span class="line">bodys = &#123;&#125;</span><br><span class="line">url = host + path + '?' + querys</span><br><span class="line"></span><br><span class="line">request = urllib2.Request(url)</span><br><span class="line">request.add_header('Authorization', 'APPCODE ' + appcode)</span><br><span class="line">response = urllib2.urlopen(request)</span><br><span class="line">content = response.read()</span><br><span class="line">if (content):</span><br><span class="line">    print(content)</span><br></pre></td></tr></table></figure><h6 id="人工识别"><a href="#人工识别" class="headerlink" title="人工识别"></a>人工识别</h6><p>通常网站只需登录一次便可爬取，在其他识别方式不管用时，人工识别一次验证码也是可行的，其实现也非常简单————在 Scrapy 下载完验证码图片后，调用 Image.show 方法将图片显示出来，然后调用 Python 内置的 input 函数，等待用户肉眼识别后输入识别结果。<br>实现三种方式整体代码如下：</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CaptchaLoginspider</span>(<span class="title">scrapy</span>.<span class="title">Spider</span>):</span></span><br><span class="line"></span><br><span class="line">    .....</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_captcha_by_OCR</span><span class="params">(data)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="comment"># OCR 识别</span></span><br><span class="line">        img = Image.open(BytesIO(data))</span><br><span class="line">        img = img.convert(<span class="string">'L'</span>)</span><br><span class="line">        captcha = pytesseract.image_to_string(img)</span><br><span class="line">        img.close()</span><br><span class="line">        <span class="keyword">return</span> captcha</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_captcha_by_network</span><span class="params">(<span class="keyword">self</span>, data)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="comment"># 平台识别</span></span><br><span class="line">        import requests</span><br><span class="line"></span><br><span class="line">        url = <span class="string">"http://ali-checkcode.showapi.com/checkcode"</span></span><br><span class="line">        appcode = <span class="string">'f23cca37f287418a90e2f9226492734"</span></span><br><span class="line"><span class="string">        form = &#123;&#125;</span></span><br><span class="line"><span class="string">        form['</span>convert_to_jpg<span class="string">'] = '</span><span class="number">0</span><span class="string">'</span></span><br><span class="line"><span class="string">        form['</span>img_base64<span class="string">'] = base64.b64encode(data)</span></span><br><span class="line"><span class="string">        form['</span>typeId<span class="string">'] = '</span><span class="number">3040</span><span class="string">'</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        headers = &#123;'</span>Authorization<span class="string">': '</span>APPCODE<span class="string">' + appcode&#125;</span></span><br><span class="line"><span class="string">        response = requests.post(url, headers=headers, data=form&#125;</span></span><br><span class="line"><span class="string">        res = response.json()</span></span><br><span class="line"><span class="string">        if res['</span>showapi_res_body<span class="string">'] == 0:</span></span><br><span class="line"><span class="string">            return res['</span>showapi_res_body<span class="string">']['</span>Result<span class="string">']</span></span><br><span class="line"><span class="string">        return '</span><span class="string">'</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    def getcaptcha_by_user(self, data):</span></span><br><span class="line"><span class="string">        # 人工识别</span></span><br><span class="line"><span class="string">        img = Image.open(BytesIO(data))</span></span><br><span class="line"><span class="string">        img.show()</span></span><br><span class="line"><span class="string">        captcha = input("输入验证码:")</span></span><br><span class="line"><span class="string">        img.close()</span></span><br><span class="line"><span class="string">        return captcha</span></span><br></pre></td></tr></table></figure><h5 id="Cookie-登录"><a href="#Cookie-登录" class="headerlink" title="Cookie 登录"></a>Cookie 登录</h5><p>目前有的网站验证码越来越复杂，已经复杂到人类难以识别的程度，有些时候提交表单登录的路子难以走通。此时，我们可以换一种登录爬取的思路，在使用浏览器登录网站后，包含用户身份信息的 Cookie 会被浏览器保存在本地，如果 Scrapy 爬虫能直接使用浏览器中的 Cookie 发送 HTTP 请求，就可以绕过提交表单登录的步骤。</p><h6 id="获取浏览器-Cookie"><a href="#获取浏览器-Cookie" class="headerlink" title="获取浏览器 Cookie"></a>获取浏览器 Cookie</h6><p>使用第三方 Python 库 browsercookie 可以获取 Chrome 和 Firefox 浏览器 中的 Cookie。使用 pip 安装 browsercookie<br></p><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip <span class="keyword">install</span> browsercookie</span><br></pre></td></tr></table></figure><p></p><p>需要安装 pycrypto 依赖，默认不支持 Python3 版本，解决参考：<a href="https://blog.csdn.net/a624806998/article/details/78596543" target="_blank" rel="noopener">https://blog.csdn.net/a624806998/article/details/78596543</a></p><p>browsercookie 的使用非常简单，示例代码如下:<br></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; import browsercookie</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; chrome_cookiejar = browsercookie.chrome()  <span class="comment"># 获取 Chrome 浏览器中的 Cookie</span></span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; firefox_cookiejar = browsercookie.firefox()   <span class="comment"># 获取 Firefox 浏览器中的 Cookie</span></span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; <span class="built_in">type</span>(chrome_cookiejar)</span></span><br><span class="line">&lt;class 'http.cookiejar.CookieJar'&gt;</span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; from pprint import pprint</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; <span class="keyword">for</span> cookie <span class="keyword">in</span> chrome_cookiejar:</span></span><br><span class="line">...     pprint(cookie)</span><br></pre></td></tr></table></figure><p></p><p>browsercookie 的 chrome 和 firefox 方法分别返回 Chomne 和 Firefox 浏览器中的<br>cookie，返回值是一个 http.cookiejar.CookieJar 对象，对 CookieJar 对象进行选代，可以访问其中的每个 Cookie 对象。</p><h6 id="CookiesMiddleware-源码分析"><a href="#CookiesMiddleware-源码分析" class="headerlink" title="CookiesMiddleware 源码分析"></a>CookiesMiddleware 源码分析</h6><p><strong>源码：Lib/site-packages/scrapy/downloadermiddlewares/cookies.py</strong></p><p>Scrapy 爬虫所使用的 Cookie 由内置下载中间件 CookiesMiddleware<br>自动处理。</p><p>其中几个核心方法如下：</p><ul><li>from_crawler 方法</li></ul><p>从配置文件中读取 COOKIES ENABLED，决定是否启用该中间件。如果启用，调<br>用构造器创建对象，否则抛出 NotConfigured 异常，Scrapy 将忽略该中间件。</p><ul><li>__init__ 方法</li></ul><p>使用标准库中的 collections.defaultdict 创建一个默认字典 self.jar，该字典中每一项的值都是一个 scrapy.http.cookies.CookieJar 对象，CookiesMiddleware 可以让 Scrapy 爬虫同时使用多个不同的 Cookiclar，<br>例如，在某网站我们注册了两个账号 account1 和 account2，假设一个爬虫想同时登录两个账号对网站进行爬取，为了<br>了避免 Cookie 冲突（通俗地讲，登录一个会替换掉另一个），此时可以让每个账号发送的 HTTP 请求使用不同的 CookieJar，在构造 Request 对象时，可以通过 meta 参數的。<br>cookieJar 字段指定所要使用的 CookieJar，如：<br></p><figure class="highlight vala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># 账号 account1 发送的请求</span></span><br><span class="line">Request(url1, meta=&#123;<span class="string">'cookiejar'</span>:<span class="string">'account1'</span>&#125;)</span><br><span class="line">Request(url2, meta=&#123;<span class="string">'cookiejar'</span>:<span class="string">'account1'</span>&#125;)</span><br><span class="line">Request(url3, meta=&#123;<span class="string">'cookiejar'</span>:<span class="string">'account1'</span>&#125;)  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta"># 账号 account2 发送的请求</span></span><br><span class="line">Request(url1, meta=&#123;<span class="string">'cookiejar'</span>:<span class="string">'account2'</span>&#125;)</span><br><span class="line">Request(url2, meta=&#123;<span class="string">'cookiejar'</span>:<span class="string">'account2'</span>&#125;)</span><br><span class="line">Request(url3, meta=&#123;<span class="string">'cookiejar'</span>:<span class="string">'account2'</span>&#125;)</span><br></pre></td></tr></table></figure><p></p><ul><li>process_request 方法</li></ul><p>处理每一个待发送的 Request 对象，尝试从 request.meta[‘cookiejar’]获取用户指定使<br>用的 CookieJar，如果用户未指定，就使用默认的 CookieJar(self.jars[None]) 。调用<br>self._get_request_cookies 方法获取发送请求 request 应携带的 Cookie 信息，填写到 HTTP 请求头部。</p><ul><li>process_response 方法</li></ul><p>处理每一个 Response 对象，依然通过 request.meta[‘cookiejar’] 获取 Cookielar 对象，调用 extract_cookies 方法将 HTTP 响应头部中的 Cookie 信息保存到 CookieJar 对象中。</p><p>另外需要注意的是，这里的 CookieJar 是 scrapy.http.cookies.CookieJar 而在之前通过 browsercookie 获取浏览器中的 CookieJar 是标准库中的 http.cookiejar.CookieJar，它们是不同的类，前者对后者进行了包装，两者可以相互转化。</p><h6 id="实现-BrowserCookiesMiddleware"><a href="#实现-BrowserCookiesMiddleware" class="headerlink" title="实现 BrowserCookiesMiddleware"></a>实现 BrowserCookiesMiddleware</h6><p>CookiesMiddleware 自动处理 Cookie 的特性给用户提供了便利，但它不能使用浏览器的 Cookie，我们可以利用 browsercookie 进行改良，实现使用浏览器 Cookie 的中间件，代码如下：</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line">__author_<span class="number">_</span> = <span class="string">"東飛"</span></span><br><span class="line">__date_<span class="number">_</span> = <span class="string">"2017-11-29"</span></span><br><span class="line"></span><br><span class="line">import browsercookie</span><br><span class="line">from scrapy.downloadermiddlewares.cookies import CookiesMiddleware</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BrowserCookiesMiddleware</span>(<span class="title">CookiesMiddleware</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(<span class="keyword">self</span>, debug=False)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">super</span>().__init_<span class="number">_</span>(debug)</span><br><span class="line">        <span class="keyword">self</span>.load_browser_cookies()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">load_browser_cookies</span><span class="params">(<span class="keyword">self</span>)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="comment"># 加载 Chrome 浏览器中的 Cookie</span></span><br><span class="line">        jar = <span class="keyword">self</span>.jars[<span class="string">'chrome'</span>]</span><br><span class="line">        chrome_cookiejar = browsercookie.chrome()</span><br><span class="line">        <span class="keyword">for</span> cookie <span class="keyword">in</span> <span class="symbol">chrome_cookiejar:</span></span><br><span class="line">            jar.set_cookie(cookie)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 加载 Firefox 浏览器中的 Cookie</span></span><br><span class="line">        firefox_cookiejar = browsercookie.firefox()</span><br><span class="line">        jar = <span class="keyword">self</span>.jars[<span class="string">'firefox'</span>]</span><br><span class="line">        <span class="keyword">for</span> cookie <span class="keyword">in</span> <span class="symbol">firefox_cookiejar:</span></span><br><span class="line">            jar.set_cookie(cookie)</span><br></pre></td></tr></table></figure><p>了解了 CookiesMiddleware 的工作原理，便不难理解 BrowserCookiesMiddleware 的<br>实现了，其核心思想是：在构造 BrowserCookiesMiddleware 对象时，使用 browsercookie 将浏览器中的 Cookie 提取，存储到 CookieJar 字典 self.jars 中，解释代码如下:</p><ul><li><p>继承 CookiesMiddleware 并实现构造器方法，在构造器方法中先调用基类的构造器<br>方法，然后调用 self.load_browser_cookies 方法加载浏览器 Cookie。</p></li><li><p>在 load_browser_cookies 方法中，使用 self.jars[‘chrome’] 和 self.jars[‘firefox’] 从默认字典中获得两个 CookieJar 对象，然后调用 chrome 和 firefox 方法，分别获取两个浏览器中的 Cookie，将它们填入各自的 CookieJar对象中。</p></li></ul><h6 id="爬取知乎个人信息"><a href="#爬取知乎个人信息" class="headerlink" title="爬取知乎个人信息"></a>爬取知乎个人信息</h6><p>通过使用 BrowserCookiesMiddleware 获取知乎用户的个人信息<br><img src="https://dongfei.oss-cn-shanghai.aliyuncs.com/%E7%9F%A5%E4%B9%8E%E4%B9%8B.jpg" alt="zhihuzhi"></p><p>将 BrowserCookiesMiddleware 源码复制到项目下的 middlewares.py 中，<br>在配置文件 settings.py 中添加如下配置：</p><figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># 伪装成常规浏览器</span></span><br><span class="line">USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.<span class="number">3325.14</span>6 Safari/537.36'</span><br><span class="line"></span><br><span class="line">用 BrowserCookiesMiddleware 替代 CookiesMiddleware 启用前者，关团后者</span><br><span class="line"></span><br><span class="line">DOWNLOADER_MIDDLEWARES = &#123;</span><br><span class="line">    'ArticleSpider.middlewares.BrowserCookiesMiddleware': <span class="number">701</span>,</span><br><span class="line">    'scrapy.downloadermiddlewares.cookies.CookiesMiddleware': None,    </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>由于需求非常简单，因此不再编写 Spider，直接在scrapy shell 环境中进行演示。意，为了使用项目中的配置，需要在 <strong>项目目录</strong>下启动scrapy shell 命令:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">F:\VirtualEnv\jobboleArticle\ArticleSpider</span><br><span class="line">(jobboleArticle) $ scrapy shell</span><br><span class="line">............</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; from scrapy import Request</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; url = <span class="string">"https://www.zhihu.com/settings/profile"</span></span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; fetch(Request(url,meta=&#123;<span class="string">'cookiejar'</span>:<span class="string">'chrome'</span>&#125;))</span></span><br><span class="line">2018-03-31 15:28:23 [scrapy.core.engine] INFO: Spider opened</span><br><span class="line">2018-03-31 15:28:23 [scrapy.downloadermiddlewares.cookies] DEBUG: Sending cookies to: &lt;GET https://www.zhihu.com/settings/profile&gt;</span><br><span class="line">Cookie: __DAYU_PP=Izvyf3MInbqQ6AzMZNeM2826864c6ffb; __DAYU_PP=Yee7A2aUjvjMIZEJfQYz2db6dcd51be2; __utma=51854390.1271732890.1522479670.1522479670.1522479670.1; __utmb=51854390.0.10.1522479670; __utmv=51854390.100-1|2=registration_date=20151218=1^3=entry_date=20151218=1; __utmz=51854390.1522479670.1.1.utmcsr=zhihu.com|utmccn=(referral)|utmcmd=referral|utmcct=/people/geng-dong-fei/activities; _zap=ae9aa6a5-49bc-4480-a722-e6be1565a91c; d_c0="AGDC4NfPzwyPTrS7n_0PM5L0qcPMAK_vqyc=|1512884449"; q_c1=166834564cdd4bf79f4d349fcb16e920|1521010814000|1512884448000; z_c0="2|1:0|10:1514289890|4:z_c0|92:Mi4xV1Zoa0FnQUFBQUFBWU1MZzE4X1BEQ1lBQUFCZ0FsVk40b2d2V3dEMS16cFo0MXlmNlVVbzE5N2JZNlZGTkpGQ013|648affd73f7b115cfb0a0d5e0e844d3325ad0dfb94c1de102f790606f5dcd9c3"</span><br><span class="line"></span><br><span class="line">2018-03-31 15:28:23 [scrapy.downloadermiddlewares.cookies] DEBUG: Received cookies from: &lt;200 https://www.zhihu.com/settings/profile&gt;</span><br><span class="line">Set-Cookie: _xsrf=bad5bdecf7b3c52016b3a774309cfa82; expires=Mon, 30 Apr 2018 07:28:20 GMT; Path=/</span><br><span class="line"></span><br><span class="line">2018-03-31 15:28:23 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https://www.zhihu.com/settings/profile&gt; (referer: None)</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; view(response)</span></span><br><span class="line">True</span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt;<span class="comment"># 提取页面中的姓名和个性域名信息</span></span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt;response.css(<span class="string">'span.name::text'</span>).extract_first()</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt;response.css(<span class="string">'input.zg-form-text-input::attr(value)'</span>).extract_first()</span></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt;response.xpath(<span class="string">'//input[@class="zg-form-text-input"]/@value'</span>).extract_first()</span></span><br></pre></td></tr></table></figure></div><div></div><div><div style="padding:10px 0;margin:20px auto;width:90%;text-align:center"><div>坚持原创技术分享，您的支持将鼓励我继续创作！</div><button id="rewardButton" disable="enable" onclick='var qr=document.getElementById("QR");"none"===qr.style.display?qr.style.display="block":qr.style.display="none"'><span>赏</span></button><div id="QR" style="display:none"><div id="wechat" style="display:inline-block"><img id="wechat_qr" src="http://oqiflua2i.bkt.clouddn.com/微信支付.png" alt="那小子真帅 WeChat Pay"><p>微信打赏</p></div><div id="alipay" style="display:inline-block"><img id="alipay_qr" src="http://oqiflua2i.bkt.clouddn.com/支付宝支付.png" alt="那小子真帅 Alipay"><p>支付宝打赏</p></div></div></div></div><div></div><footer class="post-footer"><div class="post-tags"><a href="/tags/Scrapy/" rel="tag"># Scrapy</a></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/2018-03-24/爬虫参考指南/" rel="next" title="爬虫参考指南"><i class="fa fa-chevron-left"></i> 爬虫参考指南</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"><a href="/2018-04-01/数据分析基础/" rel="prev" title="数据分析基础">数据分析基础 <i class="fa fa-chevron-right"></i></a></div></div></footer></article><div class="post-spread"><div class="jiathis_style"><a class="jiathis_button_tsina"></a> <a class="jiathis_button_tqq"></a> <a class="jiathis_button_weixin"></a> <a class="jiathis_button_cqq"></a> <a class="jiathis_button_douban"></a> <a class="jiathis_button_renren"></a> <a class="jiathis_button_qzone"></a> <a class="jiathis_button_kaixin001"></a> <a class="jiathis_button_copy"></a> <a href="http://www.jiathis.com/share" class="jiathis jiathis_txt jiathis_separator jtico jtico_jiathis" target="_blank"></a> <a class="jiathis_counter_style"></a></div><script type="text/javascript">var jiathis_config={hideMore:!1}</script><script type="text/javascript" src="http://v3.jiathis.com/code/jia.js" charset="utf-8"></script></div></div></div><div class="comments" id="comments"><div id="cloud-tie-wrapper" class="cloud-tie-wrapper"></div><script src="https://img1.cache.netease.com/f2e/tie/yun/sdk/loader.js"></script><script>var cloudTieConfig={url:document.location.href,sourceId:"",productKey:"f466c18e6eea4e24bc686a8ae4514dae",target:"cloud-tie-wrapper"},yunManualLoad=!0;Tie.loader("aHR0cHM6Ly9hcGkuZ2VudGllLjE2My5jb20vcGMvbGl2ZXNjcmlwdC5odG1s",!0)</script></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">文章目录</li><li class="sidebar-nav-overview" data-target="site-overview">站点概览</li></ul><section class="site-overview sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" src="http://oqiflua2i.bkt.clouddn.com/%E5%9B%BE%E7%89%878.png" alt="那小子真帅"><p class="site-author-name" itemprop="name">那小子真帅</p><p class="site-description motion-element" itemprop="description">有酒有肉有朋友，能贫能笑能干架</p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives"><span class="site-state-item-count">121</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/index.html"><span class="site-state-item-count">33</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/index.html"><span class="site-state-item-count">43</span> <span class="site-state-item-name">标签</span></a></div></nav><div class="feed-link motion-element"><a href="/atom.xml" rel="alternate"><i class="fa fa-rss"></i> RSS</a></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/GFigure" target="_blank" title="GitHub"><i class="fa fa-fw fa-globe"></i> GitHub </a></span><span class="links-of-author-item"><a href="https://www.zhihu.com/people/geng-dong-fei" target="_blank" title="知乎"><i class="fa fa-fw fa-bandcamp"></i> 知乎 </a></span><span class="links-of-author-item"><a href="http://www.jianshu.com/u/507e68ab9a5a" target="_blank" title="简书"><i class="fa fa-fw fa-book"></i> 简书 </a></span><span class="links-of-author-item"><a href="mailto:iamdongfei@foxmail.com" target="_blank" title="邮箱"><i class="fa fa-fw fa-envelope"></i> 邮箱 </a></span><span class="links-of-author-item"><a href="https://www.facebook.com/profile.php?id=100012742063569" target="_blank" title="Facebook"><i class="fa fa-fw fa-facebook-official"></i> Facebook </a></span><span class="links-of-author-item"><a href="https://plus.google.com/u/0/" target="_blank" title="Google+"><i class="fa fa-fw fa-google-plus-square"></i> Google+ </a></span><span class="links-of-author-item"><a title="嗨一下" style="underline:none;color:red" rel="alternate" class="mw-harlem_shake_slow wobble shake" href="javascript:shake()"><i class="fa fa-music"></i> &nbsp;&nbsp;High</a></span></div><div class="links-of-blogroll motion-element links-of-blogroll-inline"><div class="links-of-blogroll-title"><i class="fa fa-fw fa-globe"></i> 友情链接</div><ul class="links-of-blogroll-list"><li class="links-of-blogroll-item"><a href="http://www.csdn.net/" title="CSDN" target="_blank">CSDN</a></li><li class="links-of-blogroll-item"><a href="https://www.github.com" title="Github" target="_blank">Github</a></li><li class="links-of-blogroll-item"><a href="https://segmentfault.com/" title="Segmentfault" target="_blank">Segmentfault</a></li><li class="links-of-blogroll-item"><a href="https://www.oschina.net/" title="OsChina" target="_blank">OsChina</a></li></ul></div></section><section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class="post-toc"><div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-5"><a class="nav-link" href="#模拟登陆"><span class="nav-number">1.</span> <span class="nav-text">模拟登陆</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#使用-FromRequest"><span class="nav-number">1.1.</span> <span class="nav-text">使用 FromRequest</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#识别验证码"><span class="nav-number">2.</span> <span class="nav-text">识别验证码</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#OCR识别"><span class="nav-number">2.1.</span> <span class="nav-text">OCR识别</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#网络平台识别"><span class="nav-number">2.2.</span> <span class="nav-text">网络平台识别</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#人工识别"><span class="nav-number">2.3.</span> <span class="nav-text">人工识别</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Cookie-登录"><span class="nav-number">3.</span> <span class="nav-text">Cookie 登录</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#获取浏览器-Cookie"><span class="nav-number">3.1.</span> <span class="nav-text">获取浏览器 Cookie</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#CookiesMiddleware-源码分析"><span class="nav-number">3.2.</span> <span class="nav-text">CookiesMiddleware 源码分析</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#实现-BrowserCookiesMiddleware"><span class="nav-number">3.3.</span> <span class="nav-text">实现 BrowserCookiesMiddleware</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#爬取知乎个人信息"><span class="nav-number">3.4.</span> <span class="nav-text">爬取知乎个人信息</span></a></li></ol></li></ol></div></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2018</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">那小子真帅</span></div><div class="powered-by">由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动</div><div class="theme-info">主题 - <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a></div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script><script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script><script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script><script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script><script type="text/javascript" src="/lib/three/three.min.js"></script><script type="text/javascript" src="/lib/three/three-waves.min.js"></script><script type="text/javascript" src="/js/src/utils.js?v=5.1.1"></script><script type="text/javascript" src="/js/src/motion.js?v=5.1.1"></script><script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.1"></script><script type="text/javascript" src="/js/src/post-details.js?v=5.1.1"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.1"></script><script type="text/javascript">var isfetched=!1,search_path="search.xml";0==search_path.length&&(search_path="search.xml");var path="/"+search_path;function proceedsearch(){$("body").append('<div class="search-popup-overlay local-search-pop-overlay"></div>').css("overflow","hidden"),$(".popup").toggle()}var searchFunc=function(e,c,s){"use strict";$.ajax({url:e,dataType:"xml",async:!0,success:function(e){isfetched=!0,$(".popup").detach().appendTo(".header-inner");var t=$("entry",e).map(function(){return{title:$("title",this).text(),content:$("content",this).text(),url:$("url",this).text()}}).get(),a=document.getElementById(c),r=document.getElementById(s);a.addEventListener("input",function(){var u=0,d='<ul class="search-result-list">',f=this.value.trim().toLowerCase().split(/[\s\-]+/);r.innerHTML="",1<this.value.trim().length&&t.forEach(function(e){var a=!1,r=e.title.trim().toLowerCase(),c=e.content.trim().replace(/<[^>]+>/g,"").toLowerCase(),t=decodeURIComponent(e.url),s=-1,o=-1,n=-1;if(""!=r&&f.forEach(function(e,t){s=r.indexOf(e),o=c.indexOf(e),(0<=s||0<=o)&&(a=!0,0==t&&(n=o))}),a){u+=1,d+="<li><a href='"+t+"' class='search-result-title'>"+r+"</a>";var i=e.content.trim().replace(/<[^>]+>/g,"");if(0<=n){var l=n-20,p=n+80;l<0&&(l=0),0==l&&(p=50),p>i.length&&(p=i.length);var h=i.substring(l,p);f.forEach(function(e){var t=new RegExp(e,"gi");h=h.replace(t,'<b class="search-keyword">'+e+"</b>")}),d+='<p class="search-result">'+h+"...</p>"}d+="</li>"}}),d+="</ul>",0==u&&(d='<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'),""==f&&(d='<div id="no-result"><i class="fa fa-search fa-5x" /></div>'),r.innerHTML=d}),proceedsearch()}})};$(".popup-trigger").click(function(e){e.stopPropagation(),0==isfetched?searchFunc(path,"local-search-input","local-search-result"):proceedsearch()}),$(".popup-btn-close").click(function(e){$(".popup").hide(),$(".local-search-pop-overlay").remove(),$("body").css("overflow","")}),$(".popup").click(function(e){e.stopPropagation()})</script><script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.1.js"></script><script>AV.initialize("cmVXt6URwS0fSRTsfStnNLTi-gzGzoHsz","O8mac9LovG7JWC2ucYMbj7pM")</script><script>function showTime(e){var t=new AV.Query(e),c=[],u=$(".leancloud_visitors");u.each(function(){c.push($(this).attr("id").trim())}),t.containedIn("url",c),t.find().done(function(e){var t=".leancloud-visitors-count";if(0!==e.length){for(var n=0;n<e.length;n++){var o=e[n],i=o.get("url"),s=o.get("time"),r=document.getElementById(i);$(r).find(t).text(s)}for(n=0;n<c.length;n++){i=c[n],r=document.getElementById(i);var l=$(r).find(t);""==l.text()&&l.text(0)}}else u.find(t).text(0)}).fail(function(e,t){console.log("Error: "+t.code+" "+t.message)})}function addCount(i){var e=$(".leancloud_visitors"),s=e.attr("id").trim(),r=e.attr("data-flag-title").trim(),t=new AV.Query(i);t.equalTo("url",s),t.find({success:function(e){if(0<e.length){var t=e[0];t.fetchWhenSave(!0),t.increment("time"),t.save(null,{success:function(e){$(document.getElementById(s)).find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to save Visitor num, with error message: "+t.message)}})}else{var n=new i,o=new AV.ACL;o.setPublicReadAccess(!0),o.setPublicWriteAccess(!0),n.setACL(o),n.set("title",r),n.set("url",s),n.set("time",1),n.save(null,{success:function(e){$(document.getElementById(s)).find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to create")}})}},error:function(e){console.log("Error:"+e.code+" "+e.message)}})}$(function(){var e=AV.Object.extend("Counter");1==$(".leancloud_visitors").length?addCount(e):1<$(".post-title-link").length&&showTime(e)})</script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(t,s)}()</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });</script><script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type="text/javascript" src="/js/src/love.js"></script></body></html>