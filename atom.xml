<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>那小子真帅</title>
  
  <subtitle>码农，程序猿，技术控</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://blog.dongfei.xin/"/>
  <updated>2018-04-13T11:54:49.620Z</updated>
  <id>http://blog.dongfei.xin/</id>
  
  <author>
    <name>那小子真帅</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Centos 安装 MongoDB </title>
    <link href="http://blog.dongfei.xin/2018-04-13/Centos-%E5%AE%89%E8%A3%85-MongoDB/"/>
    <id>http://blog.dongfei.xin/2018-04-13/Centos-安装-MongoDB/</id>
    <published>2018-04-13T07:39:20.000Z</published>
    <updated>2018-04-13T11:54:49.620Z</updated>
    
    <content type="html"><![CDATA[<h5 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h5><p>MongoDB 是一个介于关系数据库和非关系数据库之间的产品，是非关系数据库当中功能最丰富，最像关系数据库的。它支持的数据结构非常松散，是类似 json 的 bson 格式，因此可以存储比较复杂的数据类型。Mongo 最大的特点是它支持的查询语言非常强大，其语法有点类似于面向对象的查询语言，几乎可以实现类似关系数据库单表查询的绝大部分功能，而且还支持对数据建立索引。</p><p><img src="https://iamdongfei.oss-cn-shenzhen.aliyuncs.com/blog/MongoDB.png" alt="MongoDB"></p><a id="more"></a><p>Packages 包说明<br>MongoDB 官方源中包含以下几个依赖包：<br>mongodb-org: MongoDB 元数据包，安装时自动安装下面四个组件包：</p><ul><li>mongodb-org-server: 包含 MongoDB 守护进程和相关的配置和初始化脚本。</li><li>mongodb-org-mongos: 包含 mongos 的守护进程。</li><li>mongodb-org-shell: 包含 mongo shell。</li><li>mongodb-org-tools: 包含 MongoDB 的工具： mongoimport、 bsondump、 mongodump、 mongoexport、 mongofiles、 mongooplog、 mongoperf、 mongorestore、 mongostat、 and mongotop 。</li></ul><h5 id="安装步骤"><a href="#安装步骤" class="headerlink" title="安装步骤"></a>安装步骤</h5><h6 id="配置-MongoDB-的-yum-源"><a href="#配置-MongoDB-的-yum-源" class="headerlink" title="配置 MongoDB 的 yum 源"></a>配置 MongoDB 的 yum 源</h6><p>创建 yum 源文件：<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/yum<span class="selector-class">.repos</span><span class="selector-class">.d</span>/mongodb-org-<span class="number">3.4</span>.repo</span><br></pre></td></tr></table></figure></p><p>添加以下内容：<br><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">[mongodb-org-3.4]</span>  </span><br><span class="line"><span class="attr">name</span>=MongoDB Repository  </span><br><span class="line"><span class="attr">baseurl</span>=https://repo.mongodb.org/yum/redhat/<span class="variable">$releasever</span>/mongodb-org/<span class="number">3.4</span>/x<span class="number">86_64</span>/  </span><br><span class="line"><span class="attr">gpgcheck</span>=<span class="number">1</span>  </span><br><span class="line"><span class="attr">enabled</span>=<span class="number">1</span>  </span><br><span class="line"><span class="attr">gpgkey</span>=https://www.mongodb.org/static/pgp/server-<span class="number">3.4</span>.asc</span><br></pre></td></tr></table></figure></p><p>这里可以修改 <code>gpgcheck=0</code>、 省去 gpg 验证</p><p>安装之前先更新所有包 ：<code>yum update</code> （可选操作）</p><h6 id="安装-MongoDB-命令："><a href="#安装-MongoDB-命令：" class="headerlink" title="安装 MongoDB 命令："></a>安装 MongoDB 命令：</h6><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum -y <span class="keyword">install</span> mongodb-org</span><br></pre></td></tr></table></figure><p>安装完成后</p><p>查看 mongo 安装位置 <code>whereis mongod</code></p><p>查看修改配置文件 ：<code>vim /etc/mongod.conf</code></p><h5 id="启动-MongoDB-服务"><a href="#启动-MongoDB-服务" class="headerlink" title="启动 MongoDB 服务"></a>启动 MongoDB 服务</h5><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">启动 mongodb ：systemctl start mongod.service</span><br><span class="line">停止 mongodb ：systemctl stop mongod.service</span><br><span class="line">查看 mongodb 的状态：systemctl status mongod.service</span><br></pre></td></tr></table></figure><h5 id="设置开机启动"><a href="#设置开机启动" class="headerlink" title="设置开机启动"></a>设置开机启动</h5><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl <span class="builtin-name">enable</span> mongod.service</span><br></pre></td></tr></table></figure><h5 id="外网访问需要关闭防火墙："><a href="#外网访问需要关闭防火墙：" class="headerlink" title="外网访问需要关闭防火墙："></a>外网访问需要关闭防火墙：</h5><p>CentOS 7.0 默认使用的是 firewall 作为防火墙，这里改为 iptables 防火墙。</p><p>关闭 firewall：<br><figure class="highlight gauss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">systemctl <span class="keyword">stop</span> firewalld.service    <span class="meta"># 停止firewall</span></span><br><span class="line">systemctl <span class="keyword">disable</span> firewalld.service <span class="meta"># 禁止firewall开机启动</span></span><br></pre></td></tr></table></figure></p><h5 id="启动-Mongo-shell"><a href="#启动-Mongo-shell" class="headerlink" title="启动 Mongo shell"></a>启动 Mongo shell</h5><p>命令：<code>mongo</code> </p><p>查看数据库：<code>show dbs</code></p><h5 id="设置-mongodb-远程访问："><a href="#设置-mongodb-远程访问：" class="headerlink" title="设置 mongodb 远程访问："></a>设置 mongodb 远程访问：</h5><p>编辑 <code>mongod.conf</code> 注释 <code>bindIp</code>、并重启 mongodb.</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/mongod.conf</span><br><span class="line"></span><br><span class="line">------mongod.conf-----</span><br><span class="line">net:</span><br><span class="line">  port: 27017</span><br><span class="line">  # bindIp: 127.0.0.1  # Listen <span class="keyword">to</span> local<span class="built_in"> interface </span>only, comment <span class="keyword">to</span> listen on all interfaces.</span><br><span class="line"></span><br><span class="line">----------------</span><br><span class="line">重启 mongodb：systemctl restart mongod.service</span><br></pre></td></tr></table></figure><p>参考：</p><p><a href="https://www.cnblogs.com/web424/p/6928992.html" target="_blank" rel="noopener"> centos7 安装 MongoDB3.4 </a></p><p><a href="https://blog.csdn.net/dhfttkl123/article/details/53284238" target="_blank" rel="noopener"> Mongodb 3.2 开启密码认证 </a></p>]]></content>
    
    <summary type="html">
    
      &lt;h5 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h5&gt;&lt;p&gt;MongoDB 是一个介于关系数据库和非关系数据库之间的产品，是非关系数据库当中功能最丰富，最像关系数据库的。它支持的数据结构非常松散，是类似 json 的 bson 格式，因此可以存储比较复杂的数据类型。Mongo 最大的特点是它支持的查询语言非常强大，其语法有点类似于面向对象的查询语言，几乎可以实现类似关系数据库单表查询的绝大部分功能，而且还支持对数据建立索引。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://iamdongfei.oss-cn-shenzhen.aliyuncs.com/blog/MongoDB.png&quot; alt=&quot;MongoDB&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Linux" scheme="http://blog.dongfei.xin/categories/Linux/"/>
    
      <category term="CentOS" scheme="http://blog.dongfei.xin/categories/Linux/CentOS/"/>
    
    
      <category term="CentOS" scheme="http://blog.dongfei.xin/tags/CentOS/"/>
    
      <category term="MongoDB" scheme="http://blog.dongfei.xin/tags/MongoDB/"/>
    
  </entry>
  
  <entry>
    <title>Centos 配置 Shadowsocks 翻墙</title>
    <link href="http://blog.dongfei.xin/2018-04-13/Centos-%E9%85%8D%E7%BD%AE-Shadowsocks-%E7%BF%BB%E5%A2%99/"/>
    <id>http://blog.dongfei.xin/2018-04-13/Centos-配置-Shadowsocks-翻墙/</id>
    <published>2018-04-13T06:20:19.000Z</published>
    <updated>2018-04-13T15:45:59.560Z</updated>
    
    <content type="html"><![CDATA[<p><blockquote class="blockquote-center">再牛逼的梦想也抵不住傻逼似得坚持</blockquote><br><img src="https://iamdongfei.oss-cn-shenzhen.aliyuncs.com/blog/shadowsocks.png" alt="shadowsocks"></p><h5 id="Socks5-全局代理"><a href="#Socks5-全局代理" class="headerlink" title="Socks5 全局代理"></a>Socks5 全局代理</h5><h6 id="安装-sslocal"><a href="#安装-sslocal" class="headerlink" title="安装 sslocal"></a>安装 sslocal</h6><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pip <span class="keyword">install </span><span class="keyword">shadowsocks </span><span class="comment"># pip 安装 ss 客户端</span></span><br><span class="line">如果提示 -<span class="keyword">bash: </span>pip: command not found</span><br><span class="line">运行 yum -y <span class="keyword">install </span>python-pip</span><br></pre></td></tr></table></figure><a id="more"></a><h6 id="配置-shadowsocks-json"><a href="#配置-shadowsocks-json" class="headerlink" title="配置 shadowsocks.json"></a>配置 shadowsocks.json</h6><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">vim</span> <span class="string">/etc/shadowsocks.json</span></span><br><span class="line"><span class="bullet">-</span><span class="bullet">--</span> <span class="string">shadowsocks.json</span> <span class="meta">---</span></span><br><span class="line"><span class="string">&#123;</span></span><br><span class="line"><span class="attr">    "server":</span><span class="string">"SERVER-IP"</span><span class="string">,</span>   <span class="comment"># 你的服务器ip</span></span><br><span class="line"><span class="attr">    "server_port":</span><span class="string">PORT,</span>    <span class="comment"># 服务器端口</span></span><br><span class="line"><span class="attr">    "local_address":</span> <span class="string">"127.0.0.1"</span><span class="string">,</span></span><br><span class="line"><span class="attr">    "local_port":</span><span class="number">1080</span><span class="string">,</span></span><br><span class="line"><span class="attr">    "password":</span><span class="string">"PASSWORD"</span><span class="string">,</span>    <span class="comment"># 密码</span></span><br><span class="line"><span class="attr">    "timeout":</span><span class="number">300</span><span class="string">,</span></span><br><span class="line"><span class="attr">    "method":</span><span class="string">"aes-128-cfb"</span><span class="string">,</span> <span class="comment"># 加密方式</span></span><br><span class="line"><span class="attr">    "fast_open":</span> <span class="literal">false</span><span class="string">,</span></span><br><span class="line"><span class="attr">    "workers":</span> <span class="number">1</span></span><br><span class="line"><span class="string">&#125;</span></span><br><span class="line"><span class="bullet">-</span><span class="bullet">--</span> <span class="string">shadowsocks.json</span> <span class="meta">---</span></span><br></pre></td></tr></table></figure><h6 id="运行-sslocal"><a href="#运行-sslocal" class="headerlink" title="运行 sslocal"></a>运行 sslocal</h6><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup sslocal -c /etc/shadowsocks<span class="selector-class">.json</span> &amp;&gt;&gt; /var/log/sslocal<span class="selector-class">.log</span> &amp;</span><br></pre></td></tr></table></figure><h5 id="Privoxy-篇"><a href="#Privoxy-篇" class="headerlink" title="Privoxy 篇"></a>Privoxy 篇</h5><h6 id="安装-privoxy"><a href="#安装-privoxy" class="headerlink" title="安装 privoxy"></a>安装 privoxy</h6><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum -y <span class="keyword">install</span> privoxy</span><br></pre></td></tr></table></figure><h6 id="配置-socks5-全局代理"><a href="#配置-socks5-全局代理" class="headerlink" title="配置 socks5 全局代理"></a>配置 socks5 全局代理</h6><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">echo <span class="string">'forward-socks5 / 127.0.0.1:1080 .'</span> <span class="meta">&gt;&gt; </span>/etc/privoxy/config</span><br></pre></td></tr></table></figure><h6 id="设置-http-https-代理"><a href="#设置-http-https-代理" class="headerlink" title="设置 http/https 代理"></a>设置 http/https 代理</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export http_proxy=http://127.0.0.1:8118 # privoxy默认监听端口为8118</span><br><span class="line">export https_proxy=http://127.0.0.1:8118</span><br></pre></td></tr></table></figure><h6 id="运行-privoxy"><a href="#运行-privoxy" class="headerlink" title="运行 privoxy"></a>运行 privoxy</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">service privoxy start</span><br></pre></td></tr></table></figure><h6 id="测试-socks5-全局代理"><a href="#测试-socks5-全局代理" class="headerlink" title="测试 socks5 全局代理"></a>测试 socks5 全局代理</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">curl www.google.com</span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># 如果出现下面这段输出则代理成功！</span></span></span><br><span class="line">------------------------------------------------------------------------------</span><br><span class="line">&lt;HTML&gt;&lt;HEAD&gt;&lt;meta http-equiv="content-type" content="text/html;charset=utf-8"&gt;</span><br><span class="line">&lt;TITLE&gt;302 Moved&lt;/TITLE&gt;&lt;/HEAD&gt;&lt;BODY&gt;</span><br><span class="line">&lt;H1&gt;302 Moved&lt;/H1&gt;</span><br><span class="line">The document has moved</span><br><span class="line">&lt;A HREF="http://www.google.com.hk/url?sa=p&amp;amp;hl=zh-CN&amp;amp;pref=hkredirect&amp;amp;pval=yes&amp;amp;q=http://www.google.com.hk/%3Fgws_rd%3Dcr&amp;amp;ust=1480320257875871&amp;amp;usg=AFQjCNHg9F5zMg83aD2KKHHHf-yecq0nfQ"&gt;here&lt;/A&gt;.</span><br><span class="line">&lt;/BODY&gt;&lt;/HTML&gt;</span><br><span class="line">------------------------------------------------------------------------------</span><br></pre></td></tr></table></figure><h5 id="简化使用"><a href="#简化使用" class="headerlink" title="简化使用"></a>简化使用</h5><p>进过上面的步骤我们的确代理成功了。但是每次都要输入这么多命令太麻烦<br>这时我们可以利用 <strong>命令别名</strong> 来简化我们的操作<br><figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">alias</span> ssinit='nohup sslocal -c <span class="string">/etc/shadowsocks.json</span> &amp;&gt;&gt; <span class="string">/var/log/sslocal.log</span> &amp;'</span><br><span class="line"><span class="keyword">alias</span> sson='export http_proxy=http:<span class="string">//127.0.0.1</span><span class="function">:8118</span> &amp;&amp; export https_proxy=http:<span class="string">//127.0.0.1</span><span class="function">:8118</span> &amp;&amp; systemctl start privoxy'</span><br><span class="line"><span class="keyword">alias</span> ssoff='<span class="keyword">unset</span> http_proxy &amp;&amp; <span class="keyword">unset</span> https_proxy &amp;&amp; systemctl stop privoxy &amp;&amp; pkill sslocal'</span><br></pre></td></tr></table></figure></p><h6 id="使用方法"><a href="#使用方法" class="headerlink" title="使用方法"></a>使用方法</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">## 开启ss代理</span></span></span><br><span class="line">ssinit</span><br><span class="line">sson</span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># 关闭ss代理</span></span></span><br><span class="line">ssoff</span><br></pre></td></tr></table></figure><p>参考：</p><ol><li><p><a href="https://i.jakeyu.top/2017/03/16/centos%E4%BD%BF%E7%94%A8SS%E7%BF%BB%E5%A2%99/" target="_blank" rel="noopener">centos使用SS翻墙</a></p></li><li><p><a href="https://bodyno.com/tool/2017/09/03/centos-ss.html" target="_blank" rel="noopener">如何使用Shadowsocks让centos翻墙</a></p></li><li><p><a href="https://blog.csdn.net/xwydq/article/details/51274185" target="_blank" rel="noopener">linux下的ss+privoxy代理配置</a></p></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;blockquote class=&quot;blockquote-center&quot;&gt;再牛逼的梦想也抵不住傻逼似得坚持&lt;/blockquote&gt;&lt;br&gt;&lt;img src=&quot;https://iamdongfei.oss-cn-shenzhen.aliyuncs.com/blog/shadowsocks.png&quot; alt=&quot;shadowsocks&quot;&gt;&lt;/p&gt;
&lt;h5 id=&quot;Socks5-全局代理&quot;&gt;&lt;a href=&quot;#Socks5-全局代理&quot; class=&quot;headerlink&quot; title=&quot;Socks5 全局代理&quot;&gt;&lt;/a&gt;Socks5 全局代理&lt;/h5&gt;&lt;h6 id=&quot;安装-sslocal&quot;&gt;&lt;a href=&quot;#安装-sslocal&quot; class=&quot;headerlink&quot; title=&quot;安装 sslocal&quot;&gt;&lt;/a&gt;安装 sslocal&lt;/h6&gt;&lt;figure class=&quot;highlight mipsasm&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;pip &lt;span class=&quot;keyword&quot;&gt;install &lt;/span&gt;&lt;span class=&quot;keyword&quot;&gt;shadowsocks &lt;/span&gt;&lt;span class=&quot;comment&quot;&gt;# pip 安装 ss 客户端&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;如果提示 -&lt;span class=&quot;keyword&quot;&gt;bash: &lt;/span&gt;pip: command not found&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;运行 yum -y &lt;span class=&quot;keyword&quot;&gt;install &lt;/span&gt;python-pip&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="Linux" scheme="http://blog.dongfei.xin/categories/Linux/"/>
    
      <category term="CentOS" scheme="http://blog.dongfei.xin/categories/Linux/CentOS/"/>
    
    
      <category term="CentOS" scheme="http://blog.dongfei.xin/tags/CentOS/"/>
    
      <category term="翻墙" scheme="http://blog.dongfei.xin/tags/%E7%BF%BB%E5%A2%99/"/>
    
  </entry>
  
  <entry>
    <title>Scrapinghub 的 Scrapy 技巧系列（六）</title>
    <link href="http://blog.dongfei.xin/2018-04-12/Scrapinghub-%E7%9A%84-Scrapy-%E6%8A%80%E5%B7%A7%E7%B3%BB%E5%88%97%EF%BC%88%E5%85%AD%EF%BC%89/"/>
    <id>http://blog.dongfei.xin/2018-04-12/Scrapinghub-的-Scrapy-技巧系列（六）/</id>
    <published>2018-04-11T16:58:27.000Z</published>
    <updated>2018-04-13T07:20:37.845Z</updated>
    
    <content type="html"><![CDATA[<p>原文：<a href="https://blog.scrapinghub.com/2016/07/20/scrapy-tips-from-the-pros-july-2016/" target="_blank" rel="noopener"> Scrapy Tips from the Pros: July 2016 </a></p><hr><p><a href="http://scrapy.org" target="_blank" rel="noopener"> Scrapy </a>被设计成可扩展，并且组件之间松耦合。你可以轻松地使用自己的中间件或者 pipeline 扩展 Scrapy 的功能。</p><p>这使得 Scrapy 社区可以很容易地开发新的插件来改善现有功能，而不需改变 Scrapy 自身。在这篇文章中，我们将向你展示如何利用 DeltaFetch 插件来进行增量爬取。</p><p><img src="https://dongfei.oss-cn-shanghai.aliyuncs.com/blog/scrapy-tips.png" alt="Scrapy Tips"></p><a id="more"></a><h2 id="使用-Deltafetch-进行增量爬取"><a href="#使用-Deltafetch-进行增量爬取" class="headerlink" title="使用 Deltafetch 进行增量爬取"></a>使用 Deltafetch 进行增量爬取</h2><p>我们开发的一些爬虫设计成一次性爬取并抓取我们所需的数据。另一方面，许多爬虫需要周期性地爬取，以便让我们的数据集保持最新。</p><p>在这些周期爬虫中，我们只对最后一次爬取的最新页面感兴趣。例如，我们有一个从一堆网络媒体网点爬取文章的爬虫。该爬虫一天执行一次，并且它们首先从预定义的首页检索文章URL。然后，从每篇文章上提取标题、作者、日期和内容。这种方法通常会导致许多重复结果，并且使得每次我们运行爬虫时，爬取的数量越来越多。</p><p>幸运的是，我们并不是第一个有这个问题的人。社区已经有了解决方法：<a href="https://github.com/scrapy-plugins/scrapy-deltafetch" target="_blank" rel="noopener"> scrapy-deltafetch插件 </a>。你可以用这个插件进行增量爬取。 DeltaFetch 的主要目的是避免请求那些之前已经爬过的页面，即使它在之前的执行中已经出现了。它只会对那些之前没有提取任何项的页面、爬虫的 <code>start_urls</code> 属性中的URL、或者在爬虫的 <code>start_requests</code> 方法中生成的 Request 进行请求。</p><p>DeltaFetch 的工作原理是，对爬虫回调中生成的每一个 Item 和 Request 对象进行拦截。对于 Item ，它计算相关的 Request 标识符(又名，<a href="https://github.com/scrapy/scrapy/blob/master/scrapy/utils/request.py#L19" target="_blank" rel="noopener"> 指纹(fingerprint) </a>)，并将其存储到一个本地数据库中。对于 Request ，Deltafetch 计算 Request fingerprint ，并在在其已存在数据库的时候丢弃该 Request。</p><p>现在，看看如何为你的 Scrapy 爬虫设置 Deltafetch 。</p><h3 id="开始使用-DeltaFetch"><a href="#开始使用-DeltaFetch" class="headerlink" title="开始使用 DeltaFetch"></a>开始使用 DeltaFetch</h3><p>首先，用 pip 安装 DeltaFetch ：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ pip install scrapy-deltafetch</span><br></pre></td></tr></table></figure><p>然后，你必须在你的项目的 settings.py 文件中启用它：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">SPIDER_MIDDLEWARES = &#123;</span><br><span class="line">    <span class="string">'scrapy_deltafetch.DeltaFetch'</span>: <span class="number">100</span>,</span><br><span class="line">&#125;</span><br><span class="line">DELTAFETCH_ENABLED = <span class="keyword">True</span></span><br></pre></td></tr></table></figure><h3 id="使用-DeltaFetch"><a href="#使用-DeltaFetch" class="headerlink" title="使用 DeltaFetch"></a>使用 DeltaFetch</h3><p><a href="https://github.com/stummjr/books_crawler/" target="_blank" rel="noopener"> 这个爬虫 </a>有一个爬取<a href="http://books.toscrape.com" target="_blank" rel="noopener"> books.toscrape.com </a>的蜘蛛。它通过所有列出的页面进行导航，访问每本书的详细页面，获取一些数据，例如书标题、描述和目录。该爬虫每天执行一次，以捕获对应目录中包含的新书。无需访问那些已经爬过的书页面，因为由爬虫收集的数据通常不会改变。</p><p>想看看 Deltafetch 的使用，<a href="https://github.com/stummjr/books_crawler/" target="_blank" rel="noopener"> clone这个repo </a>，其中，已经在 settings.py 启用了 DeltaFetch ，然后运行：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ scrapy crawl toscrape</span><br></pre></td></tr></table></figure><p>等它结束，然后看看 Scrapy 在最后记录的统计数据：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="number">2016</span><span class="number">-07</span><span class="number">-19</span> <span class="number">10</span>:<span class="number">17</span>:<span class="number">53</span> [scrapy] INFO: Dumping Scrapy stats:</span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">'deltafetch/stored'</span>: <span class="number">1000</span>,</span><br><span class="line">    ...</span><br><span class="line">    <span class="string">'downloader/request_count'</span>: <span class="number">1051</span>,</span><br><span class="line">    ...</span><br><span class="line">    <span class="string">'item_scraped_count'</span>: <span class="number">1000</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>除此之外，你会看到爬虫进行了 1051 次请求来爬取 1000 个项，而 DeltaFetch 存储了 1000 个请求的 fingerprint 。这意味着，只有 51 个页面请求没有生成 item ，因此下次还会继续访问他们。</p><p>现在，再次运行该爬虫，你会看到许多像这样的日志信息：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="number">2016</span><span class="number">-07</span><span class="number">-19</span> <span class="number">10</span>:<span class="number">47</span>:<span class="number">10</span> [toscrape] INFO: Ignoring already visited: </span><br><span class="line">&lt;GET http://books.toscrape.com/....../index.html&gt;</span><br></pre></td></tr></table></figure><p>而在统计数据中，你会看到，跳过了 1000 个请求，因为在之前的爬取中，已经爬到了 item 。现在，该爬虫并未提取任何 item ，并且它只进行了 51 次请求，它们所有都是之前没有爬取到 item 的页面：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="number">2016</span><span class="number">-07</span><span class="number">-19</span> <span class="number">10</span>:<span class="number">47</span>:<span class="number">10</span> [scrapy] INFO: Dumping Scrapy stats:</span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">'deltafetch/skipped'</span>: <span class="number">1000</span>,</span><br><span class="line">    ...</span><br><span class="line">    <span class="string">'downloader/request_count'</span>: <span class="number">51</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="修改数据库键"><a href="#修改数据库键" class="headerlink" title="修改数据库键"></a>修改数据库键</h3><p>默认情况下，DeltaFetch 使用一个 Request fingerprint 来区分 Request 。该 fingerprint 是一个基于规范 URL、HTTP 方法和请求体计算的哈希值。</p><p>一些网站对于相同的数据会有多个 URL 。例如，一个电子商务网站可能有指向同一个产品的 URL ，如下所示：</p><ul><li><a href="http://www.example.com/product?id=123" target="_blank" rel="noopener">http://www.example.com/product?id=123</a></li><li><a href="http://www.example.com/deals?id=123" target="_blank" rel="noopener">http://www.example.com/deals?id=123</a></li><li><a href="http://www.example.com/category/keyboards?id=123" target="_blank" rel="noopener">http://www.example.com/category/keyboards?id=123</a></li><li><a href="http://www.example.com/category/gaming?id=123" target="_blank" rel="noopener">http://www.example.com/category/gaming?id=123</a></li></ul><p>在这些情况下，Request fingerprint 并不适用，因为规范的 URL 将会不同，即使 item 是相同的。在这个例子中，我们可以使用产品的 ID 作为 DeltaFetch 键。</p><p>DeltaFetch允许我们在初始化Request时，通过传递一个名为<code>deltafetch_key</code>的元参数来自定义键：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> w3lib.url <span class="keyword">import</span> url_query_parameter</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">for</span> product_url <span class="keyword">in</span> response.css(<span class="string">'a.product_listing'</span>):</span><br><span class="line">        <span class="keyword">yield</span> Request(</span><br><span class="line">            product_url,</span><br><span class="line">            meta=&#123;<span class="string">'deltafetch_key'</span>: url_query_parameter(product_url, <span class="string">'id'</span>)&#125;,</span><br><span class="line">            callback=self.parse_product_page</span><br><span class="line">        )</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure><p>通过这种方式， DeltaFetch 将会忽略对重复页面进行请求，即使它们有不同的 URL 。</p><h3 id="重置-DeltaFetch"><a href="#重置-DeltaFetch" class="headerlink" title="重置 DeltaFetch"></a>重置 DeltaFetch</h3><p>如果你想要重新爬取页面，可以通过传递一个 <code>deltafetch_reset</code> 参数给你的爬虫，来重置 DeltaFetch 缓存：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ scrapy crawl example -a deltafetch_reset=<span class="number">1</span></span><br></pre></td></tr></table></figure><h3 id="在-Scrapy-Cloud-上使用-DeltaFetch"><a href="#在-Scrapy-Cloud-上使用-DeltaFetch" class="headerlink" title="在 Scrapy Cloud 上使用 DeltaFetch"></a>在 Scrapy Cloud 上使用 DeltaFetch</h3><p>你也可以对运行在<a href="https://app.scrapinghub.com/account/signup/" target="_blank" rel="noopener"> Scrapy Cloud </a>之上的爬虫使用 DeltaFetch 。仅需在你项目的 Addons 页面启用 DeltaFetch 和 DotScrapy Persistence 插件。后者是用来允许你的爬虫访问 .scrapy 文件夹，该文件夹是 DeltaFetch 存储其数据库的地方。</p><p><img src="https://dongfei.oss-cn-shanghai.aliyuncs.com/blog/image00.png" alt="image00"></p><p>Deltafetch 在我们已经看到的那些情况下是非常方便的。<strong>请记住，Deltafetch 只是避免了发送请求到之前已经生成了 item 的页面，并且仅当这些请求尚未由爬虫的s tart_urls 或者 start_requests 生成。</strong>那些来自于没有直接爬取到 item 的页面，在每一次你运行你的爬虫的时候，将仍会抓取。</p><p>你可以看看 github 上该项目页以获取更多信息：<a href="http://github.com/scrapy-plugins/scrapy-deltafetch" target="_blank" rel="noopener">http://github.com/scrapy-plugins/scrapy-deltafetch</a></p><h2 id="总结一下"><a href="#总结一下" class="headerlink" title="总结一下"></a>总结一下</h2><p>你可以在 Github 的<a href="https://github.com/scrapy-plugins" target="_blank" rel="noopener"> scrapy-plugins </a>页面上找到许多有趣的 Scrapy 插件，你还可以在那里包含自己的插件来回馈社区。</p><p>如果你有问题，或者你想在这个每月专栏上看到某个主题，请在这里（ Ele 注，到原文留言哈）留下评论，让我们知道，或者通过在 Twitter 上<a href="http://twitter.com/scrapinghub" target="_blank" rel="noopener"> @scrapinghub </a>来找到我们。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;原文：&lt;a href=&quot;https://blog.scrapinghub.com/2016/07/20/scrapy-tips-from-the-pros-july-2016/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt; Scrapy Tips from the Pros: July 2016 &lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;a href=&quot;http://scrapy.org&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt; Scrapy &lt;/a&gt;被设计成可扩展，并且组件之间松耦合。你可以轻松地使用自己的中间件或者 pipeline 扩展 Scrapy 的功能。&lt;/p&gt;
&lt;p&gt;这使得 Scrapy 社区可以很容易地开发新的插件来改善现有功能，而不需改变 Scrapy 自身。在这篇文章中，我们将向你展示如何利用 DeltaFetch 插件来进行增量爬取。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://dongfei.oss-cn-shanghai.aliyuncs.com/blog/scrapy-tips.png&quot; alt=&quot;Scrapy Tips&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Python" scheme="http://blog.dongfei.xin/categories/Python/"/>
    
      <category term="Scrapy" scheme="http://blog.dongfei.xin/categories/Python/Scrapy/"/>
    
    
      <category term="Scrapy" scheme="http://blog.dongfei.xin/tags/Scrapy/"/>
    
  </entry>
  
  <entry>
    <title>Scrapinghub 的 Scrapy 技巧系列（五）</title>
    <link href="http://blog.dongfei.xin/2018-04-12/Scrapinghub-%E7%9A%84-Scrapy-%E6%8A%80%E5%B7%A7%E7%B3%BB%E5%88%97%EF%BC%88%E4%BA%94%EF%BC%89/"/>
    <id>http://blog.dongfei.xin/2018-04-12/Scrapinghub-的-Scrapy-技巧系列（五）/</id>
    <published>2018-04-11T16:57:39.000Z</published>
    <updated>2018-04-12T11:57:54.862Z</updated>
    
    <content type="html"><![CDATA[<p>原文：<a href="https://blog.scrapinghub.com/2016/06/22/scrapy-tips-from-the-pros-june-2016/" target="_blank" rel="noopener"> Scrapy Tips from the Pros June 2016 </a></p><hr><p>欢迎来到 Scrapy 技巧系列！每个月，我们会发布一些技巧和 hack ，来帮助你加快网页抓取和数据提取。作为牵头的 Scrapy 维护者，你可以想象的任何障碍我们都遇到过了，所以别担心，你能在这获益良多。随意到<a href="https://twitter.com/ScrapingHub" target="_blank" rel="noopener"> Twitter </a>或者<a href="https://www.facebook.com/ScrapingHub/" target="_blank" rel="noopener"> Facebook </a>访问我们，提出对未来主题的建议吧。</p><p><img src="https://dongfei.oss-cn-shanghai.aliyuncs.com/blog/scrapy-tips.png" alt="Scrapy Tips"></p><a id="more"></a><h2 id="抓取无限滚动页面"><a href="#抓取无限滚动页面" class="headerlink" title="抓取无限滚动页面"></a>抓取无限滚动页面</h2><p>在单页应用以及每页具有大量 AJAX 请求的时代，很多网站已经用花哨的无限滚动机制取代了 <strong>前一个/下一个</strong>分页按钮。使用这种技术的网站每当用户滚动到页面的底部的时候加载新项（想想微博，Facebook，谷歌图片）。虽然<a href="https://www.smashingmagazine.com/2013/05/infinite-scrolling-lets-get-to-the-bottom-of-this/" target="_blank" rel="noopener"> UX专家 </a>认为，无限滚动为用户提供了海量数据，但是我们看到越来越多的 web 页面诉诸于展示这种无休止的结果列表。</p><p>在开发 web 爬虫时，我们要做的第一件事就是找到能将我们引导到下一页结果的带有链接的 UI 组件。不幸的是，这些链接在无限滚动页面上不存在。</p><p>虽然这种场景可能看起来像诸如<a href="http://scrapinghub.com/splash/" target="_blank" rel="noopener"> Splash </a>或者<a href="http://www.seleniumhq.org/" target="_blank" rel="noopener"> Selenium </a>这样的 JavaScript 引擎的一个经典案例，但是它实际上是一个简单的修复。你所需要做的是在你滚动目标页面的时候检查浏览器的 AJAX 请求，然后在 Scrapy spider 中重新创建这些请求，而不是模拟用于与此类引擎的交互。</p><p>让我们以<a href="http://spidyquotes.herokuapp.com/scroll" target="_blank" rel="noopener"> Spidy Quotes </a>为例，构建一个爬虫来获取上面列出来的所有的项。</p><h2 id="审查页面"><a href="#审查页面" class="headerlink" title="审查页面"></a>审查页面</h2><p>先说重要的事，我们需要理解无限滚动是如何在这个页面工作的，我们可以通过<a href="https://developer.chrome.com/devtools#access" target="_blank" rel="noopener"> 浏览器的开发者工具 </a>中的Network面板来完成此项工作。打开该面板，然后滚动页面，看看浏览器发送了什么请求：</p><p><img src="https://scrapinghub.files.wordpress.com/2016/06/scrapy-tips-from-the-pros-june.png?w=648" alt="scrapy tips from the pros june"></p><p>点开一个请求仔细看看。浏览器发送了一个请求到<code>/api/quotes?page=x</code>，然后接收诸如以下的一个 JSON 对象作为响应：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">   <span class="string">"has_next"</span>:true,</span><br><span class="line">   <span class="string">"page"</span>:<span class="number">8</span>,</span><br><span class="line">   <span class="string">"quotes"</span>:[</span><br><span class="line">      &#123;</span><br><span class="line">         <span class="string">"author"</span>:&#123;</span><br><span class="line">            <span class="string">"goodreads_link"</span>:<span class="string">"/author/show/1244.Mark_Twain"</span>,</span><br><span class="line">            <span class="string">"name"</span>:<span class="string">"Mark Twain"</span></span><br><span class="line">         &#125;,</span><br><span class="line">         <span class="string">"tags"</span>:[<span class="string">"individuality"</span>, <span class="string">"majority"</span>, <span class="string">"minority"</span>, <span class="string">"wisdom"</span>],</span><br><span class="line">         <span class="string">"text"</span>:<span class="string">"Whenever you find yourself on the side of the ..."</span></span><br><span class="line">      &#125;,</span><br><span class="line">      &#123;</span><br><span class="line">         <span class="string">"author"</span>:&#123;</span><br><span class="line">            <span class="string">"goodreads_link"</span>:<span class="string">"/author/show/1244.Mark_Twain"</span>,</span><br><span class="line">            <span class="string">"name"</span>:<span class="string">"Mark Twain"</span></span><br><span class="line">         &#125;,</span><br><span class="line">         <span class="string">"tags"</span>:[<span class="string">"books"</span>, <span class="string">"contentment"</span>, <span class="string">"friends"</span>],</span><br><span class="line">         <span class="string">"text"</span>:<span class="string">"Good friends, good books, and a sleepy ..."</span></span><br><span class="line">      &#125;</span><br><span class="line">   ],</span><br><span class="line">   <span class="string">"tag"</span>:null,</span><br><span class="line">   <span class="string">"top_ten_tags"</span>:[[<span class="string">"love"</span>, <span class="number">49</span>], [<span class="string">"inspirational"</span>, <span class="number">43</span>], ...]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这就是我们的爬虫需要的信息了。它所需要做的仅是生成到 <code>/api/quotes?page=x</code>的请求，其中，<code>x</code> 的值不断增加，直到 <code>has_next</code> 字段为 false 。这样做最棒的是，我们甚至无需爬取HTML内容以获取所需数据。这些数据都在一个漂亮的机器可读的 JSON 中。</p><h2 id="构建Spider"><a href="#构建Spider" class="headerlink" title="构建Spider"></a>构建Spider</h2><p>下面是我们的 spider 。它从服务器返回的 JSON 内容提取目标数据。这种方法比挖掘页面的 HTML 树更容易并且更健壮，相信布局的改变不会搞挂我们的 spider 。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SpidyQuotesSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">'spidyquotes'</span></span><br><span class="line">    quotes_base_url = <span class="string">'http://spidyquotes.herokuapp.com/api/quotes?page=%s'</span></span><br><span class="line">    start_urls = [quotes_base_url % <span class="number">1</span>]</span><br><span class="line">    download_delay = <span class="number">1.5</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        data = json.loads(response.body)</span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> data.get(<span class="string">'quotes'</span>, []):</span><br><span class="line">            <span class="keyword">yield</span> &#123;</span><br><span class="line">                <span class="string">'text'</span>: item.get(<span class="string">'text'</span>),</span><br><span class="line">                <span class="string">'author'</span>: item.get(<span class="string">'author'</span>, &#123;&#125;).get(<span class="string">'name'</span>),</span><br><span class="line">                <span class="string">'tags'</span>: item.get(<span class="string">'tags'</span>),</span><br><span class="line">            &#125;</span><br><span class="line">        <span class="keyword">if</span> data[<span class="string">'has_next'</span>]:</span><br><span class="line">            next_page = data[<span class="string">'page'</span>] + <span class="number">1</span></span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(self.quotes_base_url % next_page)</span><br></pre></td></tr></table></figure><p>要进一步练习这个技巧，你可以做个实验，爬取我们的博客，因为它也是使用无限滚动来加载旧博文的。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>如果你被爬取无限滚动网站的前景吓到，那么希望现在你可以有点信心了。下一次你需要处理那种基于用户操作引发的 AJAX 调用的页面时，看一看你的浏览器发送的请求吧，然后将其重放到你的 spider 中。响应往往是 JSON 的格式，这使得你的 spider 甚至更简单了。</p><p>好啦，这就是六月份的！请在<a href="https://twitter.com/ScrapingHub" target="_blank" rel="noopener"> Twitter </a>上联系我们，让我们知道未来你希望看到什么技巧。最近，我们还发布了一个<a href="https://blog.scrapinghub.com/2016/06/09/introducing-the-new-open-data-catalog/" target="_blank" rel="noopener"> 数据集目录 </a>，所以，如果你还苦思要爬取什么，那么看看这个目录获取一些灵感吧。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;原文：&lt;a href=&quot;https://blog.scrapinghub.com/2016/06/22/scrapy-tips-from-the-pros-june-2016/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt; Scrapy Tips from the Pros June 2016 &lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;欢迎来到 Scrapy 技巧系列！每个月，我们会发布一些技巧和 hack ，来帮助你加快网页抓取和数据提取。作为牵头的 Scrapy 维护者，你可以想象的任何障碍我们都遇到过了，所以别担心，你能在这获益良多。随意到&lt;a href=&quot;https://twitter.com/ScrapingHub&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt; Twitter &lt;/a&gt;或者&lt;a href=&quot;https://www.facebook.com/ScrapingHub/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt; Facebook &lt;/a&gt;访问我们，提出对未来主题的建议吧。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://dongfei.oss-cn-shanghai.aliyuncs.com/blog/scrapy-tips.png&quot; alt=&quot;Scrapy Tips&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Python" scheme="http://blog.dongfei.xin/categories/Python/"/>
    
      <category term="Scrapy" scheme="http://blog.dongfei.xin/categories/Python/Scrapy/"/>
    
    
      <category term="Scrapy" scheme="http://blog.dongfei.xin/tags/Scrapy/"/>
    
  </entry>
  
  <entry>
    <title>Scrapinghub 的 Scrapy 技巧系列（四）</title>
    <link href="http://blog.dongfei.xin/2018-04-12/Scrapinghub-%E7%9A%84-Scrapy-%E6%8A%80%E5%B7%A7%E7%B3%BB%E5%88%97%EF%BC%88%E5%9B%9B%EF%BC%89/"/>
    <id>http://blog.dongfei.xin/2018-04-12/Scrapinghub-的-Scrapy-技巧系列（四）/</id>
    <published>2018-04-11T16:43:30.000Z</published>
    <updated>2018-04-12T11:58:09.763Z</updated>
    
    <content type="html"><![CDATA[<p>欢迎来到 Scrapy 技巧系列！每个月，我们会发布一些技巧和 hack ，来帮助你加快网页抓取和数据提取。作为牵头的 Scrapy 维护者，你可以想象的任何障碍我们都遇到过了，所以别担心，你能在这获益良多。随意到<a href="https://twitter.com/ScrapingHub" target="_blank" rel="noopener"> Twitter </a>或者<a href="https://www.facebook.com/ScrapingHub/" target="_blank" rel="noopener"> Facebook </a>访问我们，提出对未来主题的建议吧。</p><p><img src="https://dongfei.oss-cn-shanghai.aliyuncs.com/blog/scrapy-tips.png" alt="Scrapy Tips"></p><a id="more"></a><h1 id="如何调试你的爬虫"><a href="#如何调试你的爬虫" class="headerlink" title="如何调试你的爬虫"></a>如何调试你的爬虫</h1><p>你的爬虫不工作了，但是你想不明白为啥。一个快速识别潜在问题的方法是添加一些打印语句，以找出发生了什么。这通常是我的第一个步骤，而有时我所需要做的是发现那些妨碍我的爬虫正常运行的错误。如果这个方法对你有效，那就太棒了，但是如果这个方法还不够，那么读下去，学学如何处理那些需要更加彻底调查的令人讨厌的 bug 。在这篇文章中，我将向你介绍一些工具，当涉及到调试爬虫时，它们应该在每个 Scrapy 用户的工作区中。</p><h2 id="Scrapy-Shell-是你的好基友"><a href="#Scrapy-Shell-是你的好基友" class="headerlink" title="Scrapy Shell 是你的好基友"></a>Scrapy Shell 是你的好基友</h2><p>Scrapy shell 是一个全功能的 Python shell ，它加载了与你在你的爬虫的回调方法中得到的上下文相同的上下文。你只需要提供一个 URL ，Scrapy Shell 就会让你与那个你的爬虫在它的回调中处理的相同的对象进行交互，包括 response 对象。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ scrapy shell http://blog.scrapinghub.com</span><br><span class="line">[s] Available Scrapy objects:</span><br><span class="line">[s]   crawler    &lt;scrapy.crawler.Crawler object at <span class="number">0x7f0638a2cbd0</span>&gt;</span><br><span class="line">[s]   item       &#123;&#125;</span><br><span class="line">[s]   request    &lt;GET http://blog.scrapinghub.com&gt;</span><br><span class="line">[s]   response   &lt;<span class="number">200</span> https://blog.scrapinghub.com/&gt;</span><br><span class="line">[s]   settings   &lt;scrapy.settings.Settings object at <span class="number">0x7f0638a2cb50</span>&gt;</span><br><span class="line">[s]   spider     &lt;DefaultSpider <span class="string">'default'</span> at <span class="number">0x7f06371f3290</span>&gt;</span><br><span class="line">[s] Useful shortcuts:</span><br><span class="line">[s]   shelp()           Shell help (<span class="keyword">print</span> this help)</span><br><span class="line">[s]   fetch(req_or_url) Fetch request (<span class="keyword">or</span> URL) <span class="keyword">and</span> update local objects</span><br><span class="line">[s]   view(response)    View response <span class="keyword">in</span> a browser</span><br><span class="line">&gt;&gt;&gt;</span><br></pre></td></tr></table></figure><p>加载它之后，你可以开始玩玩 response ，以构建选择器来提取所需的数据。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>response.css(<span class="string">"div.post-header &gt; h2 ::text"</span>).extract()</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>如果你不熟悉 Scrapy Shell，那么不妨试一试。它与你的开发工作流程可以完美契合，它位于在浏览器中进行页面检查的动作之后。你可以创建并测试爬虫的抽取规则，而一旦你构建了所需的规则，就可以在爬虫代码中使用它们。</p><p>通过官方文档，了解更多关于<a href="http://doc.scrapy.org/en/latest/topics/shell.html" target="_blank" rel="noopener"> Scrapy Shell的细节 </a>。</p><h3 id="从你的-Spider-代码中启动-Scrapy-Shell"><a href="#从你的-Spider-代码中启动-Scrapy-Shell" class="headerlink" title="从你的 Spider 代码中启动 Scrapy Shell"></a>从你的 Spider 代码中启动 Scrapy Shell</h3><p>如果对于某些响应，你的爬虫表现异常，那么在爬虫代码中使用 <code>scrapy.shell.inspect_response</code> 方法，你可以很快地看到发生了什么事。这将打开一个 Scrapy shell 会话，以让你与当前的 response 对象进行交互。</p><p>例如，假设你的爬虫不从某些页面中提取所期望数量的项，而你想要看看网站返回的响应有啥问题：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> scrapy.shell <span class="keyword">import</span> inspect_response</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">BlogSpider</span><span class="params">(scrapy.Spider)</span></span></span><br><span class="line"><span class="function">    ...</span></span><br><span class="line"><span class="function">    <span class="title">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> len(response.css(<span class="string">'div.post-header &gt; h2 ::text'</span>)) &gt; EXPECTED:</span><br><span class="line">            <span class="comment"># generate the items</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            inspect_response(response, self)</span><br><span class="line">        ...</span><br></pre></td></tr></table></figure><p>一旦执行这个 inspect_response 调用，Scrapy Shell 就会被打开，而你就能与 response 进行交互，从而看看发生了啥事。</p><h2 id="快速绑定一个调试器到你的-Spider"><a href="#快速绑定一个调试器到你的-Spider" class="headerlink" title="快速绑定一个调试器到你的 Spider"></a>快速绑定一个调试器到你的 Spider</h2><p>另一个调试爬虫的方法是使用常规的 Python 调试器，例如 pdb 或者 PuDB 。我使用<a href="https://pypi.python.org/pypi/pudb" target="_blank" rel="noopener"> PuDB </a>，因为它是一个相当强大且易于使用的调试器，而要激活它，我所需要的只是将这行代码放在我想要断点的那一行：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> pudb;</span><br><span class="line">pudb.set_trace()</span><br></pre></td></tr></table></figure><p>当到达断点的时候，PuDB 在你的终端中打开一个很酷的文本模式的用户界面，它将带你回到使用 Turbo Pascal 调试器的那些美好的旧时光。</p><p>看一看：<img src="https://dongfei.oss-cn-shanghai.aliyuncs.com/blog/image002.png" alt="image00"></p><p>你可以使用 pip 安装 PuDB ：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ pip install pudb</span><br></pre></td></tr></table></figure><p>看看这个视频，其中，我们自己的<a href="https://twitter.com/eliasdorneles" target="_blank" rel="noopener"> @eliasdorneles </a>演示了使用 PuDB 的几个小技巧：<a href="https://vimeo.com/166584837" target="_blank" rel="noopener">https://vimeo.com/166584837</a></p><h2 id="Scrapy-解析-CLI-命令"><a href="#Scrapy-解析-CLI-命令" class="headerlink" title="Scrapy 解析 CLI 命令"></a>Scrapy 解析 CLI 命令</h2><p>有些情况下，你需要你的爬虫很长一段时间运行某些爬取项目。但是，在运行了几个小时后，你可能会悲催地在日志中看到，对于一些特​​定的 URL ，爬虫之一有爬取问题。你想要调试爬虫，但你肯定不希望再运行整个抓取过程，并且要等到为该特定的URL调用的具体的回调，这样你就可以启动你的调试器。</p><p>别担心，Scrapy CLI 的<a href="http://doc.scrapy.org/en/latest/topics/commands.html#std:command-parse" target="_blank" rel="noopener"> parse命令 </a>就是为了让你节约时间的！你只需要提供该爬虫的名字，应该使用的爬虫的回调，以及你想要解析的URL：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ scrapy parse https://blog.scrapinghub.com/comments/bla --spider blog -c parse_comments</span><br></pre></td></tr></table></figure><p>在这种情况下，Scrapy 将会调用 blog 爬虫的 parse_comments 方法来解析 <code>blog.scrapinghub.com/comments/bla</code> URL。如果你不指定爬虫，那么 Scrapy 将会在你的项目中，基于爬虫的 allowed_domains 设置，搜寻能够处理这个 URL 的爬虫。</p><p>然后，它将会向你显示回调的执行摘要：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>STATUS DEPTH LEVEL <span class="number">1</span> &lt;&lt;&lt;</span><br><span class="line"><span class="comment"># Scraped Items  ------------------------------------------------------------</span></span><br><span class="line">[&#123;<span class="string">'comments'</span>: [</span><br><span class="line">    &#123;<span class="string">'content'</span>: <span class="string">u"I've seen this language ..."</span>,</span><br><span class="line">     <span class="string">'username'</span>: <span class="string">u'forthemostpart'</span>&#125;,</span><br><span class="line">    &#123;<span class="string">'content'</span>: <span class="string">u"It's a ..."</span>,</span><br><span class="line">     <span class="string">'username'</span>: <span class="string">u'YellowAfterlife'</span>&#125;,</span><br><span class="line">    ...</span><br><span class="line">    &#123;<span class="string">'content'</span>: <span class="string">u"There is a macro for ..."</span>,</span><br><span class="line">    <span class="string">'username'</span>: <span class="string">u'mrcdk'</span>&#125;]&#125;]</span><br><span class="line"><span class="comment"># Requests  -----------------------------------------------------------------</span></span><br><span class="line">[]</span><br></pre></td></tr></table></figure><p>你也可以在方法里面附加一个调试器，以帮助你弄清楚发生了什么（见前面的提示）。</p><h2 id="Scrapy-fetch-和-view-命令"><a href="#Scrapy-fetch-和-view-命令" class="headerlink" title="Scrapy fetch 和 view 命令"></a>Scrapy fetch 和 view 命令</h2><p>在浏览器中检查页面内容可能会被欺骗，因为它们的 JavaScript 引擎可能渲染某些 Scrapy 下载器不会做的内容。如果你想快速检查当一个页面被 Scrapy 下载后，该页面会看起来是什么样的，那么你可以使用下面这些命令：</p><ul><li><strong>fetch</strong>: 使用 Scrapy 下载器下载 HTML ，然后打印到标准输出。</li><li><strong>view</strong>: 使用 Scrapy 下载器下载 HTML ，然后用你的默认浏览器打开它。</li></ul><p><strong>例如</strong>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ scrapy fetch http://blog.scrapinghub.com &gt; blog.html</span><br><span class="line">$ scrapy view http://scrapy.org</span><br></pre></td></tr></table></figure><h2 id="使用-–pdb-选项，对爬虫进行事后剖析侦错"><a href="#使用-–pdb-选项，对爬虫进行事后剖析侦错" class="headerlink" title="使用 –pdb 选项，对爬虫进行事后剖析侦错"></a>使用 –pdb 选项，对爬虫进行事后剖析侦错</h2><p>编写防故障软件几乎是不可能的。这种情况对于网络爬虫更加糟糕，因为它们处理的网页内容是经常变化的（和损坏的）。最好接受我们的爬虫最后将会失败，并确保我们有工具来快速了解为什么它挂了，并能尽快解决这个问题。</p><p>Python 的回溯是棒棒哒，但在某些情况下，它们不向我们提供关于在我们的代码中发生了什么的足够信息。这就是事后剖析侦错的用武之地。Scrapy 提供了–  <code>--pdb</code> 命令行选项，它在你的爬虫挂掉的地方打开一个 pdb 会话，这样你就可以检查它的上下文，从而明白发生了什么：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ scrapy crawl blog -o blog_items.jl --pdb</span><br></pre></td></tr></table></figure><p>如果你的爬虫由于致命异常而挂了，那么 pdb 调试器将会打开，这样你就可以仔细检查其死因。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>好啦，这就是Scrapy技巧的五月版。在<a href="http://scrapy.readthedocs.io/en/latest/topics/debug.html #debugging-spiders" target="_blank" rel="noopener"> Scrapy官方文档 </a>，你也可以看到其中一些调试技巧。</p><p>因为这里，我们是要帮助你更有效地爬取网页的，所以<a href="https://twitter.com/ScrapingHub" target="_blank" rel="noopener"> 请让我们知道 </a>你希望在将来看到什么。那就下个月再见啦！</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;欢迎来到 Scrapy 技巧系列！每个月，我们会发布一些技巧和 hack ，来帮助你加快网页抓取和数据提取。作为牵头的 Scrapy 维护者，你可以想象的任何障碍我们都遇到过了，所以别担心，你能在这获益良多。随意到&lt;a href=&quot;https://twitter.com/ScrapingHub&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt; Twitter &lt;/a&gt;或者&lt;a href=&quot;https://www.facebook.com/ScrapingHub/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt; Facebook &lt;/a&gt;访问我们，提出对未来主题的建议吧。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://dongfei.oss-cn-shanghai.aliyuncs.com/blog/scrapy-tips.png&quot; alt=&quot;Scrapy Tips&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Python" scheme="http://blog.dongfei.xin/categories/Python/"/>
    
      <category term="Scrapy" scheme="http://blog.dongfei.xin/categories/Python/Scrapy/"/>
    
    
      <category term="Scrapy" scheme="http://blog.dongfei.xin/tags/Scrapy/"/>
    
  </entry>
  
  <entry>
    <title>Scrapinghub 的 Scrapy 技巧系列（三）</title>
    <link href="http://blog.dongfei.xin/2018-04-12/Scrapinghub-%E7%9A%84-Scrapy-%E6%8A%80%E5%B7%A7%E7%B3%BB%E5%88%97%EF%BC%88%E4%B8%89%EF%BC%89/"/>
    <id>http://blog.dongfei.xin/2018-04-12/Scrapinghub-的-Scrapy-技巧系列（三）/</id>
    <published>2018-04-11T16:25:48.000Z</published>
    <updated>2018-04-12T11:58:01.437Z</updated>
    
    <content type="html"><![CDATA[<p>原文；<a href="https://blog.scrapinghub.com/2016/04/20/scrapy-tips-from-the-pros-april-2016-edition/" target="_blank" rel="noopener"> Scrapy Tips from the Pros: April 2016 Edition </a></p><hr><p><img src="https://dongfei.oss-cn-shanghai.aliyuncs.com/blog/scrapy-tips.png" alt="Scrapy Tips"></p><p>欢迎来到四月版本的<a href="https://blog.scrapinghub.com/category/scrapy-tips-from-the-pros/" target="_blank" rel="noopener"> Scrapy技巧 </a>。每个月我们都会发布一些我们发现的技巧和 hack ，以帮助你的 Scrapy 工作流更加顺利。</p><p>这个月，我们只给你带来了一个提示，但是，不是这样的哦！所以，如果你发现你在爬取一个需要通过表单提交数据的 ASP.Net 页面，那么，就回来看看这篇文章吧。</p><a id="more"></a><h1 id="处理-ASP-Net-页面，-PostBack-和视图状态"><a href="#处理-ASP-Net-页面，-PostBack-和视图状态" class="headerlink" title="处理 ASP.Net 页面， PostBack 和视图状态"></a>处理 ASP.Net 页面， PostBack 和视图状态</h1><p>使用 ASP.Net 技术构建的网站对于 web 爬虫开发者来说通常是一场噩梦，这主要是由于它们处理表单的方式。</p><p>这类网站通常在请求和响应中发送状态，以便跟踪客户端的 UI 状态。想想那些你浏览许多页面，在 HTML 表单中填写你的数据来注册的网站吧。一个 ASP.Net 网站通常存储那些在前一个页面填写的数据到一个名为 <code>__VIEWSTATE</code> 的隐藏字段中，这个字段包含了像下面显示的一个巨大的字符串：</p><p><a href="https://dongfei.oss-cn-shanghai.aliyuncs.com/blog/image032.png" target="_blank" rel="noopener"><img src="https://scrapinghub.files.wordpress.com/2016/04/image032.png?w=648&amp;h=408" alt="ViewState example"></a></p><p><em>我不是在开玩笑，它真的很大！ (有时是数十kB)</em></p><p>这是一个 Base64 编码字符串，它表示客户端 UI 状态，包括来自表单的值。这在表单中的用户动作触发 POST 请求返回给服务器以获取其他字段的数据的 web 应用中，这种设置尤为常见。</p><p>每次浏览器向服务器发起 POST 请求时，就会带着这个 <code>__VIEWSTATE</code> 字段。然后，服务器根据该数据解码并加载客户端的 UI 状态，执行一些处理，基于新值为新的视图状态计算值，然后将这个新的视图状态作为隐藏字段渲染结果页面。</p><p>如果 <code>__VIEWSTATE</code> 没有发回给服务器，那么你可能会看到一个空白表单，因为服务器完全失去了客户端 UI 状态。所以，为了爬取像这样的根据表单生成的页面，你必须确保你的爬虫在它发送的请求中带有这个状态，否则，页面将不会加载它应该加载的内容。</p><p>这里有一个具体的例子，你可以亲眼看到如何处理这类情况。</p><h1 id="抓取一个基于视图状态的网站"><a href="#抓取一个基于视图状态的网站" class="headerlink" title="抓取一个基于视图状态的网站"></a>抓取一个基于视图状态的网站</h1><p>今天抓取的小白鼠是<a href="http://spidyquotes.herokuapp.com/search.aspx" target="_blank" rel="noopener"> spidyquotes.herokuapp.com/search.aspx </a>。 SpidyQuotes 列出了来自名人的引言，而它的搜索页面允许你根据作者和标签过滤引言：</p><p><a href="https://scrapinghub.files.wordpress.com/2016/04/image052.png" target="_blank" rel="noopener"><img src="https://scrapinghub.files.wordpress.com/2016/04/image052.png?w=300&amp;h=246" alt="image05"></a></p><p><strong>Author</strong> 字段的改变触发了一个到服务器的 POST 请求，以使用与所选的用户相关的标签来填充 <strong>Tag</strong>选择框。点击 <strong>Search</strong>，显示与所选作者的标签相对应的引言：</p><p><a href="https://dongfei.oss-cn-shanghai.aliyuncs.com/blog/image041.png" target="_blank" rel="noopener"><img src="https://scrapinghub.files.wordpress.com/2016/04/image041.png?w=295&amp;h=300" alt="image04"></a></p><p>为了爬取这些引言，我们的爬虫必须模拟用户选择一个作者，一个标签并提交表单。通过使用<a href="https://developer.chrome.com/devtools" target="_blank" rel="noopener"> Network Panel </a>（你可以通过浏览器的开发者工具访问）来仔细看看这个流程的每一步。首先，访问<a href="http://spidyquotes.herokuapp.com/search.aspx" target="_blank" rel="noopener"> spidyquotes.herokuapp.com/search.aspx </a>，然后按下 F12 或 Ctrl+Shift+I (如果你使用的是 Chrome )来加载工具，接着点击 Network 选项卡。</p><p><a href="https://dongfei.oss-cn-shanghai.aliyuncs.com/blog/image001.png" target="_blank" rel="noopener"><img src="https://scrapinghub.files.wordpress.com/2016/04/image001.png?w=648&amp;h=430" alt="image00"></a></p><p>从列表中选择一个作者，然后你将看到生成了一个发往 <code>/filter.aspx</code>的请求。点击资源名 (filter.aspx) ，你就可以看到请求细节，其中包括你选择的作者，以及在来自于服务器的原始响应中的 <code>__VIEWSTATE</code> 数据。</p><p><a href="https://dongfei.oss-cn-shanghai.aliyuncs.com/blog/image022.png" target="_blank" rel="noopener"><img src="https://scrapinghub.files.wordpress.com/2016/04/image022.png?w=648&amp;h=209" alt="image02"></a></p><p>选择一个标签并点击 Search 。你会看到你的浏览器发送了在表单中选择的值，以及一个与前面不同的 <code>__VIEWSTATE</code> 值。这是因为，当你选择作者时，服务器包含了一些新的信息在视图状态中。</p><p><a href="https://dongfei.oss-cn-shanghai.aliyuncs.com/blog/image011.png" target="_blank" rel="noopener"><img src="https://scrapinghub.files.wordpress.com/2016/04/image011.png?w=648&amp;h=234" alt="image01"></a></p><p>现在，你只需要构建一个爬虫，这个爬虫完成与你的浏览器做的事情。</p><h1 id="构建爬虫"><a href="#构建爬虫" class="headerlink" title="构建爬虫"></a>构建爬虫</h1><p>这里是你的爬虫应该遵循的步骤：</p><ol><li>抽取 <code>spidyquotes.herokuapp.com/filter.aspx</code></li><li><p>对于每一个在表单作者列表中找到的 <strong>Author</strong>：</p><ul><li>创建一个到 <code>/filter.aspx</code>的 POST 请求，同时传递选择的 <strong>Author</strong> 和 <strong>__VIEWSTATE</strong> 值</li></ul></li><li><p>对于在结果页面中找到的每一个 <strong>Tag</strong>：</p><ul><li>发送一个到<code>/filter.aspx</code>的 POST 请求，同时传递选择的 <strong>Author</strong>，选择的 <strong>Tag</strong> 和视图状态</li></ul></li><li><p>抓取结果页面</p></li></ol><h2 id="爬虫编码"><a href="#爬虫编码" class="headerlink" title="爬虫编码"></a>爬虫编码</h2><p>这里是我开发的从该网站抓取引言的爬虫，遵循了刚刚描述的步骤：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SpidyQuotesViewStateSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">'spidyquotes-viewstate'</span></span><br><span class="line">    start_urls = [<span class="string">'http://spidyquotes.herokuapp.com/search.aspx'</span>]</span><br><span class="line">    download_delay = <span class="number">1.5</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> author <span class="keyword">in</span> response.css(<span class="string">'select#author &gt; option ::attr(value)'</span>).extract():</span><br><span class="line">            <span class="keyword">yield</span> scrapy.FormRequest(</span><br><span class="line">                <span class="string">'http://spidyquotes.herokuapp.com/filter.aspx'</span>,</span><br><span class="line">                formdata=&#123;</span><br><span class="line">                    <span class="string">'author'</span>: author,</span><br><span class="line">                    <span class="string">'__VIEWSTATE'</span>: response.css(<span class="string">'input#__VIEWSTATE::attr(value)'</span>).extract_first()</span><br><span class="line">                &#125;,</span><br><span class="line">                callback=self.parse_tags</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_tags</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> tag <span class="keyword">in</span> response.css(<span class="string">'select#tag &gt; option ::attr(value)'</span>).extract():</span><br><span class="line">            <span class="keyword">yield</span> scrapy.FormRequest(</span><br><span class="line">                <span class="string">'http://spidyquotes.herokuapp.com/filter.aspx'</span>,</span><br><span class="line">                formdata=&#123;</span><br><span class="line">                    <span class="string">'author'</span>: response.css(</span><br><span class="line">                        <span class="string">'select#author &gt; option[selected] ::attr(value)'</span></span><br><span class="line">                    ).extract_first(),</span><br><span class="line">                    <span class="string">'tag'</span>: tag,</span><br><span class="line">                    <span class="string">'__VIEWSTATE'</span>: response.css(<span class="string">'input#__VIEWSTATE::attr(value)'</span>).extract_first()</span><br><span class="line">                &#125;,</span><br><span class="line">                callback=self.parse_results,</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_results</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> quote <span class="keyword">in</span> response.css(<span class="string">"div.quote"</span>):</span><br><span class="line">            <span class="keyword">yield</span> &#123;</span><br><span class="line">                <span class="string">'quote'</span>: response.css(<span class="string">'span.content ::text'</span>).extract_first(),</span><br><span class="line">                <span class="string">'author'</span>: response.css(<span class="string">'span.author ::text'</span>).extract_first(),</span><br><span class="line">                <span class="string">'tag'</span>: response.css(<span class="string">'span.tag ::text'</span>).extract_first(),</span><br><span class="line">            &#125;</span><br></pre></td></tr></table></figure></p><p><strong>步骤1</strong>由 Scrapy 完成，它读取 start_urls ，然后生成一个到 <code>/search.aspx</code>的 GET 请求。</p><p>parse() 方法负责 <strong>步骤2</strong>。它遍历了在第一个选择框中找到的 <strong>Authors</strong>，然后为每一个 <strong>Author</strong>创建一个到 <code>/filter.aspx</code> 的<a href="http://doc.scrapy.org/en/latest/topics/request-response.html#formrequest-objects" target="_blank" rel="noopener"> FormRequest </a>，模拟用户点击了列表中的每一个元素。值得注意的是，parse() 方法从它所收到的表单中读取__VIEWSTATE字段，然后将其传回给服务器，所以服务器可以跟踪我们位于哪个页面流。</p><p><strong>步骤3</strong>由 parse_tags() 方法来处理。它与 parse() 方法非常类似，因为它提取了所列的 <strong>Tags</strong> ，然后创建 POST 请求来传递每一个 <strong>Tag</strong> ，在前一个步骤中选择的 <strong>Author</strong> 以及从服务器收到的 __VIEWSTATE。</p><p>最后，在 <strong>步骤4</strong>中，parse_results()方法解析页面展示的引言列表，然后从中生成项。</p><h2 id="使用-FormRequest-from-response-简化你的爬虫"><a href="#使用-FormRequest-from-response-简化你的爬虫" class="headerlink" title="使用 FormRequest.from_response() 简化你的爬虫"></a>使用 FormRequest.from_response() 简化你的爬虫</h2><p>你也许注意到，在发送 POST 请求到服务器之前，我们的爬虫抽取了那些它从服务器收到的表单中的预填值，并在它将创建的请求中包含了这些值。</p><p>我们不需要对其手工编码，因为<a href="http://scrapy.org/" target="_blank" rel="noopener"> Scrapy </a>提供了<a href="http://doc.scrapy.org/en/latest/topics/request-response.html#scrapy.http.FormRequest.from_response" target="_blank" rel="noopener"> FormRequest.from_response() </a>方法。该方法读取 response 对象，创建一个 <code>FormRequest</code>，它自动包含表单所有的预填值以及隐藏值。这是我们的爬虫的 parse_tags() 方法：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_tags</span><span class="params">(self, response)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> tag <span class="keyword">in</span> response.css(<span class="string">'select#tag &gt; option ::attr(value)'</span>).extract():</span><br><span class="line">        <span class="keyword">yield</span> scrapy.FormRequest.from_response(</span><br><span class="line">            response,</span><br><span class="line">            formdata=&#123;<span class="string">'tag'</span>: tag&#125;,</span><br><span class="line">            callback=self.parse_results,</span><br><span class="line">        )</span><br></pre></td></tr></table></figure></p><p>所以，无论何时你处理包含隐藏值和预填值的表单，使用 <code>from_response</code> 方法，因为这样你的代码会看起来干净得多。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>好了，这就是这个月的技巧了。你可以<a href="http://msdn.microsoft.com/en-us/library/ms972976.aspx" target="_blank" rel="noopener"> 在这里读取更多关于ViewStates </a>的信息。我们希望你觉得这个技巧有用，并且很高兴看到你用它来做点什么。我们一直在寻找新的hack，所以如果你在爬取web的时候遇到了什么困难，请告诉我们。</p><p>随意在<a href="https://twitter.com/scrapinghub" target="_blank" rel="noopener"> Twitter </a>或者<a href="https://www.facebook.com/ScrapingHub/" target="_blank" rel="noopener"> Facebook </a>上告诉我们，你未来想看到什么吧。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;原文；&lt;a href=&quot;https://blog.scrapinghub.com/2016/04/20/scrapy-tips-from-the-pros-april-2016-edition/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt; Scrapy Tips from the Pros: April 2016 Edition &lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;img src=&quot;https://dongfei.oss-cn-shanghai.aliyuncs.com/blog/scrapy-tips.png&quot; alt=&quot;Scrapy Tips&quot;&gt;&lt;/p&gt;
&lt;p&gt;欢迎来到四月版本的&lt;a href=&quot;https://blog.scrapinghub.com/category/scrapy-tips-from-the-pros/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt; Scrapy技巧 &lt;/a&gt;。每个月我们都会发布一些我们发现的技巧和 hack ，以帮助你的 Scrapy 工作流更加顺利。&lt;/p&gt;
&lt;p&gt;这个月，我们只给你带来了一个提示，但是，不是这样的哦！所以，如果你发现你在爬取一个需要通过表单提交数据的 ASP.Net 页面，那么，就回来看看这篇文章吧。&lt;/p&gt;
    
    </summary>
    
      <category term="Python" scheme="http://blog.dongfei.xin/categories/Python/"/>
    
      <category term="Scrapy" scheme="http://blog.dongfei.xin/categories/Python/Scrapy/"/>
    
    
      <category term="Scrapy" scheme="http://blog.dongfei.xin/tags/Scrapy/"/>
    
  </entry>
  
  <entry>
    <title>Scrapinghub 的 Scrapy 技巧系列（二）</title>
    <link href="http://blog.dongfei.xin/2018-04-12/Scrapinghub-%E7%9A%84-Scrapy-%E6%8A%80%E5%B7%A7%E7%B3%BB%E5%88%97%EF%BC%88%E4%BA%8C%EF%BC%89/"/>
    <id>http://blog.dongfei.xin/2018-04-12/Scrapinghub-的-Scrapy-技巧系列（二）/</id>
    <published>2018-04-11T16:02:45.000Z</published>
    <updated>2018-04-12T11:57:58.354Z</updated>
    
    <content type="html"><![CDATA[<p>原文：<a href="https://blog.scrapinghub.com/2016/03/23/scrapy-tips-from-the-pros-march-2016-edition/" target="_blank" rel="noopener">Scrapy Tips from the Pros: March 2016 Edition</a></p><hr><p><img src="https://dongfei.oss-cn-shanghai.aliyuncs.com/blog/scrapy-tips-march-2016.png" alt="Scrapy-Tips-March-2016"></p><p>欢迎来到三月份版本的<a href="https://blog.scrapinghub.com/category/scrapy-tips-from-the-pros/" target="_blank" rel="noopener"> Scrapy 技巧 </a>! 每个月，我们都会发布一些我们开发的技巧和 hack，来帮助你，使得你的 Scrapy 工作流更顺畅。</p><p>这个月，我们将涵盖如何和 CookiesMiddleware 一起使用<a href="http://doc.scrapy.org/en/latest/topics/downloader-middleware.html#std:reqmeta-cookiejar" target="_blank" rel="noopener"> cookiejar </a>来绕过那些不允许你使用相同的 cookie 同时爬取多个页面的网站。我们还将分享一个一个好用的技巧，这个技巧关于如何和<a href="http://doc.scrapy.org/en/latest/topics/loaders.html" target="_blank" rel="noopener"> item loader </a>一起使用多个备用的 XPath/CSS 表达式，来从网站上更可靠地获取数据。</p><a id="more"></a><p><strong>学生请阅读以下：我们正参与<a href="https://blog.scrapinghub.com/2016/03/14/join-scrapinghub-for-google-summer-of-code-2016/" target="_blank" rel="noopener"> 2016年Google编程之夏 </a>，而我们一部分的项目点子使用了Scrapy! 如果你感兴趣，那么看一看<a href="http://gsoc2016.scrapinghub.com/ideas/" target="_blank" rel="noopener"> 我们的点子 </a>，并记得<a href="https://wiki.python.org/moin/SummerOfCode/2016#How_do_I_Apply.3F" target="_blank" rel="noopener"> 在3.25，也就是周五之前申请 </a>!</strong></p><p><strong>如果你不是学生，那么请与你的学生朋友分享。他们会获得一份夏天津贴，甚至最后我们可能聘用他们。</strong></p><h1 id="使用-CookieJar-解决站点怪异会话行为"><a href="#使用-CookieJar-解决站点怪异会话行为" class="headerlink" title="使用 CookieJar 解决站点怪异会话行为"></a>使用 CookieJar 解决站点怪异会话行为</h1><p>那些将你的 UI 状态存储在自己的服务器的会话中的网站是难以导航的，更别说抓取。你有没有遇到过那些在同一个网站上打开的一个选项卡会影响其他选项卡的网站？那么，你可能会碰到这个问题。</p><p>虽然这是令人沮丧的，它甚至对于网络爬虫更糟糕。它会严重阻碍网络爬虫会话。不幸的是，这是 ASP.Net 和基于 J2EE 的网站的通用模式。而这正是<a href="http://doc.scrapy.org/en/latest/topics/downloader-middleware.html#std:reqmeta-cookiejar" target="_blank" rel="noopener"> cookiejars </a>的用处所在。虽然不是经常需要 cookiejar ，但是对于那些意想不到的情况，你会很高兴拥有它。</p><p>当你的爬取一个网站时，Scrapy 自动为你处理 cookie ，存储并在随后的请求到将其发送到同一站点。但是，正如你可能知道的， Scrapy 请求是异步的。这意味着，你可能有发到相同的网站上的多个请求被同时处理，同时共享相同的 cookie 。为避免在爬取这些类型的网站时，请求相互影响，你必须为不同的请求设置不同的 cookie 。</p><p>您可以通过使用一个<a href="http://doc.scrapy.org/en/latest/topics/downloader-middleware.html#std:reqmeta-cookiejar" target="_blank" rel="noopener"> cookiejar </a>为同一网站中的不同页面存储单独的 cookie 来做到这点。该 cookiejar 只是在 Scrapy 爬取会话期间保持的一个 cookie 键值集合。你只需要为每个你想要存储的 cookie 定义一个唯一标识符，然后当你想要使用特定的 cookie 时，使用它的标识符。</p><p>例如，假设你想抓取一个网站上的多个类别，但这个网站存储与你在服务器会话中爬行/浏览的类别相关的数据。要同时爬取这些类别，则需要通过将类别名称作为 cookiejar 元参数的标识符来为每个类别创建一个 cookie ：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ExampleSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    urls = [</span><br><span class="line">        <span class="string">'http://www.example.com/category/photo'</span>,</span><br><span class="line">        <span class="string">'http://www.example.com/category/videogames'</span>,</span><br><span class="line">        <span class="string">'http://www.example.com/category/tablets'</span></span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">start_requests</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> url <span class="keyword">in</span> urls:</span><br><span class="line">            category = url.split(<span class="string">'/'</span>)[<span class="number">-1</span>]</span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(url, meta=&#123;<span class="string">'cookiejar'</span>: category&#125;)</span><br></pre></td></tr></table></figure></p><p>在此情况下，将管理三种不同的 Cookie（<code>photo</code>、 <code>videogames</code> 和 <code>tablets</code>）。每当你传递一个不存在的键作为 cookiejar 元值（例如，当一个类别名称尚未访问）时，你可以创建一个新的 Cookie 。当我们传递的键已经存在时，<a href="http://scrapy.org/" target="_blank" rel="noopener"> Scrapy </a>使用该请求相应的 cookie 。</p><p>所以，例如，如果你想重新使用已被用来抓取 <code>videogames</code>页面的 cookie，那么你只需要将 <code>videogames</code> 作为唯一键传递给 cookiejar。它将使用先用的 cookie，而不是创建一个新的 cookie：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">yield</span> scrapy.Request(<span class="string">'http://www.example.com/atari2600'</span>, meta=&#123;<span class="string">'cookiejar'</span>: <span class="string">'videogames'</span>&#125;)</span><br></pre></td></tr></table></figure></p><h1 id="添加备用的-CSS-XPath-规则"><a href="#添加备用的-CSS-XPath-规则" class="headerlink" title="添加备用的 CSS/XPath 规则"></a>添加备用的 CSS/XPath 规则</h1><p>当你需要完成比简单地填充字典或带有你的 spider 收集的数据的 Item 对象更多的东西时，<a href="http://doc.scrapy.org/en/latest/topics/loaders.html" target="_blank" rel="noopener"> Item Loader </a>是有用的。例如，你可能需要将一些后处理逻辑添加到你刚刚收集的数据中。你可能对某些如将标题中的每个单词首字母大写一样简单的事，甚至是更复杂的操作有兴趣。使用 ItemLoader ，你可以从 spider 中解耦这种后处理逻辑，以便拥有一个更易于维护的设计。</p><p>这个技巧说明如何将额外的功能添加到一个 Item Loader 中。比方说，你正爬取 Amazon.com ，并且提取每个产品的价格。你可以使用 Item Loader 来为 ProductItem 对象填充产品数据：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ProductItem</span><span class="params">(scrapy.Item)</span>:</span></span><br><span class="line">    name = scrapy.Field()</span><br><span class="line">    url = scrapy.Field()</span><br><span class="line">    price = scrapy.Field()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AmazonSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">"amazon"</span></span><br><span class="line">    allowed_domains = [<span class="string">"amazon.com"</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">start_requests</span><span class="params">(self)</span>:</span></span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_product</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        loader = ItemLoader(item=ProductItem(), response=response)</span><br><span class="line">        loader.add_css(<span class="string">'price'</span>, <span class="string">'#priceblock_ourprice ::text'</span>)</span><br><span class="line">        loader.add_css(<span class="string">'name'</span>, <span class="string">'#productTitle ::text'</span>)</span><br><span class="line">        loader.add_value(<span class="string">'url'</span>, response.url)</span><br><span class="line">        <span class="keyword">yield</span> loader.load_item()</span><br></pre></td></tr></table></figure><p>这种方法工作得很好，除非被爬取的产品是一次交易。这是因为对比那些普通的价格， Amazon 以一种稍微不同的格式展示交易价格。而普通产品的价格是这样表示的：</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">span</span> <span class="attr">id</span>=<span class="string">"priceblock_ourprice"</span> <span class="attr">class</span>=<span class="string">"a-size-medium a-color-price"</span>&gt;</span></span><br><span class="line">    $699.99</span><br><span class="line"><span class="tag">&lt;/<span class="name">span</span>&gt;</span></span><br></pre></td></tr></table></figure><p>交易价格显示稍微有点不同：<br><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">span</span> <span class="attr">id</span>=<span class="string">"priceblock_dealprice"</span> <span class="attr">class</span>=<span class="string">"a-size-medium a-color-price"</span>&gt;</span></span><br><span class="line">    $649.99</span><br><span class="line"><span class="tag">&lt;/<span class="name">span</span>&gt;</span></span><br></pre></td></tr></table></figure></p><p>要处理这种情况的一个好方法是，为 Item loader 中的价格字段添加一个后备规则。这是一个只有当该字段的前一规则已经失败时才应用的规则。要用 Item Loader 做到这一点，你可以添加一个 <code>add_fallback_css</code> 方法：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AmazonItemLoader</span><span class="params">(ItemLoader)</span>:</span></span><br><span class="line">    default_output_processor = TakeFirst()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_collected_values</span><span class="params">(self, field_name)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> (self._values[field_name] <span class="keyword">if</span> field_name <span class="keyword">in</span> self._values <span class="keyword">else</span> self._values.default_factory())</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add_fallback_css</span><span class="params">(self, field_name, css, *processors, **kw)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> any(self.get_collected_values(field_name)):</span><br><span class="line">            self.add_css(field_name, css, *processors, **kw)</span><br></pre></td></tr></table></figure></p><p>正如你所看到的， 如果对于该字段，没有之前收集到的值，那么 <code>add_fallback_css</code> 方法将使用 CSS 规则。现在，我们可以改变我们的 spider 来使用 AmazonItemLoader ，然后添加后备 CSS 规则到我们的 loader 中：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_product</span><span class="params">(self, response)</span>:</span></span><br><span class="line">    loader = AmazonItemLoader(item=ProductItem(), response=response)</span><br><span class="line">    loader.add_css(<span class="string">'price'</span>, <span class="string">'#priceblock_ourprice ::text'</span>)</span><br><span class="line">    loader.add_fallback_css(<span class="string">'price'</span>, <span class="string">'#priceblock_dealprice ::text'</span>)</span><br><span class="line">    loader.add_css(<span class="string">'name'</span>, <span class="string">'#productTitle ::text'</span>)</span><br><span class="line">    loader.add_value(<span class="string">'url'</span>, response.url)</span><br><span class="line">    <span class="keyword">yield</span> loader.load_item()</span><br></pre></td></tr></table></figure></p><p>这个技巧可以节省你的时间，让你的 spider 更健壮。如果有一个 CSS 规则无法获取数据，那么可以应用其他跪在来提取所需的数据。</p><p>如果Item Loader对于你来说是新玩意，那么<a href="http://doc.scrapy.org/en/latest/topics/loaders.html" target="_blank" rel="noopener"> 看看这个文档 </a>。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>就是这样了！请跟我们分享你在网络抓取以及提取数据时碰到的任何问题。我们一直在寻找新的技巧和 hack ，并且在我们的每月专栏上分享我们的 Scrapy 技巧。在<a href="https://twitter.com/ScrapingHub" target="_blank" rel="noopener"> Twitter </a>或者<a href="https://www.facebook.com/ScrapingHub/" target="_blank" rel="noopener"> Facebook  </a>上联系我们，并且让我们知道我们是否帮到了你。</p><p>如果你还没有，试试<a href="http://doc.scrapinghub.com/portia.html" target="_blank" rel="noopener"> Portia </a>，我们的开源可视化 Web 抓取工具。我们知道你喜欢<a href="http://scrapy.org/" target="_blank" rel="noopener"> Scrapy </a>，但是体验从来就不是一种令人痛苦的事 ;)</p><p>请<a href="http://gsoc2016.scrapinghub.com/ideas/" target="_blank" rel="noopener"> 申请加入我们的2016年Google编程之夏 </a>，截止如期是 3.25，周五！</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;原文：&lt;a href=&quot;https://blog.scrapinghub.com/2016/03/23/scrapy-tips-from-the-pros-march-2016-edition/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Scrapy Tips from the Pros: March 2016 Edition&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;img src=&quot;https://dongfei.oss-cn-shanghai.aliyuncs.com/blog/scrapy-tips-march-2016.png&quot; alt=&quot;Scrapy-Tips-March-2016&quot;&gt;&lt;/p&gt;
&lt;p&gt;欢迎来到三月份版本的&lt;a href=&quot;https://blog.scrapinghub.com/category/scrapy-tips-from-the-pros/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt; Scrapy 技巧 &lt;/a&gt;! 每个月，我们都会发布一些我们开发的技巧和 hack，来帮助你，使得你的 Scrapy 工作流更顺畅。&lt;/p&gt;
&lt;p&gt;这个月，我们将涵盖如何和 CookiesMiddleware 一起使用&lt;a href=&quot;http://doc.scrapy.org/en/latest/topics/downloader-middleware.html#std:reqmeta-cookiejar&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt; cookiejar &lt;/a&gt;来绕过那些不允许你使用相同的 cookie 同时爬取多个页面的网站。我们还将分享一个一个好用的技巧，这个技巧关于如何和&lt;a href=&quot;http://doc.scrapy.org/en/latest/topics/loaders.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt; item loader &lt;/a&gt;一起使用多个备用的 XPath/CSS 表达式，来从网站上更可靠地获取数据。&lt;/p&gt;
    
    </summary>
    
      <category term="Python" scheme="http://blog.dongfei.xin/categories/Python/"/>
    
      <category term="Scrapy" scheme="http://blog.dongfei.xin/categories/Python/Scrapy/"/>
    
    
      <category term="Scrapy" scheme="http://blog.dongfei.xin/tags/Scrapy/"/>
    
  </entry>
  
  <entry>
    <title>Scrapinghub 的 Scrapy 技巧系列（一）</title>
    <link href="http://blog.dongfei.xin/2018-04-11/Scrapinghub-%E7%9A%84-Scrapy-%E6%8A%80%E5%B7%A7%E7%B3%BB%E5%88%97%EF%BC%88%E4%B8%80%EF%BC%89/"/>
    <id>http://blog.dongfei.xin/2018-04-11/Scrapinghub-的-Scrapy-技巧系列（一）/</id>
    <published>2018-04-11T15:22:14.000Z</published>
    <updated>2018-04-12T12:10:32.643Z</updated>
    
    <content type="html"><![CDATA[<p>原文：<a href="http://blog.scrapinghub.com/2016/01/19/scrapy-tips-from-the-pros-part-1/" target="_blank" rel="noopener"> 跟着高手学习Scrapy技巧：第一部分 </a></p><hr><p><a href="http://scrapy.org" target="_blank" rel="noopener">Scrapy </a> 是<a href="http://scrapinghub.com" target="_blank" rel="noopener"> Scrapinghub </a>  的关键部分。我们广泛地采用此框架，并已积累了许多各种不同的快捷方法来解决常见问题。我们推出了一个系列来与大家分享这些 Scrapy 的技巧，这样，你就可以在你的日常工作流程中最有效的使用它。每一个博文将给出两到三个提示，敬请关注。</p><p><img src="https://dongfei.oss-cn-shanghai.aliyuncs.com/blog/scrapylogo.png" alt="scrapylogo"></p><a id="more"></a><h2 id="使用-Extruct-从网站中提取微观数据-Microdata"><a href="#使用-Extruct-从网站中提取微观数据-Microdata" class="headerlink" title="使用 Extruct 从网站中提取微观数据 ( Microdata )"></a>使用 Extruct 从网站中提取微观数据 ( Microdata )</h2><p>我相信网络爬虫的每一个开发者都会有理由来咒骂那些对他们的网站使用凌乱的布局的 Web 开发者。没有语义标记的网站，特别是那些基于 HTML 表格的网站，绝对是槽糕透顶。这些类型的网站使得爬取更加困难，因为几乎没有关于每一个元素代表什么的提示。有时候，你甚至不得不相信每个页面上的元素顺序将保持不变，从而抓取你需要的数据。</p><p>这就是为什么我们如此感激<a href="https://schema.org/" target="_blank" rel="noopener"> Schema.org </a> ，共同努力来使得语义标记在网页上。该项目为 Web 开发者提供了在他们的网站上展示一定范围的不同对象（包括 Person、 Product 和 Review）的架构，并使用例如<a href="http://www.w3.org/TR/microdata/" target="_blank" rel="noopener"> Microdata </a>，<a href="https://rdfa.info/" target="_blank" rel="noopener"> RDFa </a> ，<a href="http://json-ld.org/" target="_blank" rel="noopener"> JSON-LD </a> 等的任何元数据格式。这使得搜索引擎工作更加容易，因为它们可以从网站上提取有用信息，而不必深入到他们所抓取网站的 HTML 结构中。</p><p>例如，<a href="https://schema.org/AggregateRating" target="_blank" rel="noopener"> AggregateRating </a> 是网上零售商用来展示他们产品的用户评级的架构。下面是描述一个使用<a href="http://www.w3.org/TR/microdata/" target="_blank" rel="noopener"> Microdata format </a> 的网上商店中的一个产品的用户评级的标记：</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">itemprop</span>=<span class="string">"aggregateRating"</span> <span class="attr">itemscope</span>=<span class="string">""</span> <span class="attr">itemtype</span>=<span class="string">"http://schema.org/AggregateRating"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">meta</span> <span class="attr">itemprop</span>=<span class="string">"worstRating"</span> <span class="attr">content</span>=<span class="string">"1"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">meta</span> <span class="attr">itemprop</span>=<span class="string">"bestRating"</span> <span class="attr">content</span>=<span class="string">"5"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"bbystars-small-yellow"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"fill"</span> <span class="attr">style</span>=<span class="string">"width: 88%"</span>&gt;</span><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">span</span> <span class="attr">itemprop</span>=<span class="string">"ratingValue"</span> <span class="attr">aria-label</span>=<span class="string">"4.4 out of 5 stars"</span>&gt;</span>4.4<span class="tag">&lt;/<span class="name">span</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">meta</span> <span class="attr">itemprop</span>=<span class="string">"reviewCount"</span> <span class="attr">content</span>=<span class="string">"305733"</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br></pre></td></tr></table></figure><p>通过这种方式，搜索引擎可以在搜索结果中同时展示一个产品的评级及其 URL ，而不需要为每一个网站编写特定的爬虫：</p><p><img src="https://dongfei.oss-cn-shanghai.aliyuncs.com/blog/selection_096.png" alt="Example of a google search showing ratings for a product "></p><p>你还可以受益于一些网站使用的语义标记。我们推荐使用<a href="https://github.com/scrapinghub/extruct" target="_blank" rel="noopener"> Extruct </a>，一个从 HTML 文档中中提取 <a href="http://blog.scrapinghub.com/2014/06/18/extracting-schema-org-microdata-using-scrapy-selectors-and-xpath/" target="_blank" rel="noopener"> 嵌入式元数据 </a>  的库。它分析整个 HTML 并返回一个包含微观数据（ microdata ）的 Python 字典。看看我们是如何用它来提取展示用户评级的微观数据的：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> extruct.w3cmicrodata <span class="keyword">import</span> MicrodataExtractor</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>mde = MicrodataExtractor()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>data = mde.extract(html_content)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>data</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">'items'</span>: [</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="string">'type'</span>: <span class="string">'http://schema.org/AggregateRating'</span>,</span><br><span class="line">      <span class="string">'properties'</span>: &#123;</span><br><span class="line">        <span class="string">'reviewCount'</span>: <span class="string">'305733'</span>,</span><br><span class="line">        <span class="string">'bestRating'</span>: <span class="string">'5'</span>,</span><br><span class="line">        <span class="string">'ratingValue'</span>: <span class="string">u'4.4'</span>,</span><br><span class="line">        <span class="string">'worstRating'</span>: <span class="string">'1'</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>data[<span class="string">'items'</span>][<span class="number">0</span>][<span class="string">'properties'</span>][<span class="string">'ratingValue'</span>]</span><br><span class="line"><span class="string">u'4.4'</span></span><br></pre></td></tr></table></figure></p><p>现在，让我们建立一个使用 Extruct 的爬虫，它从<a href="http://www.apple.com/shop/mac/mac-accessories" target="_blank" rel="noopener"> 苹果产品网站 </a> 上提取价格和评级。该网站使用了微观数据来存储所列出的产品信息。它为每个产品使用这个结构 T ：<br><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">itemtype</span>=<span class="string">"http://schema.org/Product"</span> <span class="attr">itemscope</span>=<span class="string">"itemscope"</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">img</span> <span class="attr">src</span>=<span class="string">"/images/MLA02.jpg"</span> <span class="attr">itemprop</span>=<span class="string">"image"</span> /&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"/shop/product/MLA02/magic-mouse-2?"</span> <span class="attr">itemprop</span>=<span class="string">"url"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">span</span> <span class="attr">itemprop</span>=<span class="string">"name"</span>&gt;</span>Magic Mouse 2<span class="tag">&lt;/<span class="name">span</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"as-pinwheel-info"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">div</span> <span class="attr">itemprop</span>=<span class="string">"offers"</span> <span class="attr">itemtype</span>=<span class="string">"http://schema.org/Offer"</span> <span class="attr">itemscope</span>=<span class="string">"itemscope"</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">meta</span> <span class="attr">itemprop</span>=<span class="string">"priceCurrency"</span> <span class="attr">content</span>=<span class="string">"USD"</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">span</span> <span class="attr">class</span>=<span class="string">"as-pinwheel-pricecurrent"</span> <span class="attr">itemprop</span>=<span class="string">"price"</span>&gt;</span></span><br><span class="line">        $79.00</span><br><span class="line">      <span class="tag">&lt;/<span class="name">span</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br></pre></td></tr></table></figure></p><p>有了这个设置，你不需要使用 XPath 或者 CSS 选择器来提取所需数据。你只需要在你的爬虫中使用 Extruct 的 MicrodataExtractor ：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> extruct.w3cmicrodata <span class="keyword">import</span> MicrodataExtractor</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AppleSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">"apple"</span></span><br><span class="line">    allowed_domains = [<span class="string">"apple.com"</span>]</span><br><span class="line">    start_urls = (</span><br><span class="line">        <span class="string">'http://www.apple.com/shop/mac/mac-accessories'</span>,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        extractor = MicrodataExtractor()</span><br><span class="line">        items = extractor.extract(response.body_as_unicode(), response.url)[<span class="string">'items'</span>]</span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> items:</span><br><span class="line">            <span class="keyword">if</span> item.get(<span class="string">'properties'</span>, &#123;&#125;).get(<span class="string">'name'</span>):</span><br><span class="line">                properties = item[<span class="string">'properties'</span>]</span><br><span class="line">                <span class="keyword">yield</span> &#123;</span><br><span class="line">                    <span class="string">'name'</span>: properties[<span class="string">'name'</span>],</span><br><span class="line">                    <span class="string">'price'</span>: properties[<span class="string">'offers'</span>][<span class="string">'properties'</span>][<span class="string">'price'</span>],</span><br><span class="line">                    <span class="string">'url'</span>: properties[<span class="string">'url'</span>]</span><br><span class="line">                &#125;</span><br></pre></td></tr></table></figure></p><p>此爬虫会生成这样的项：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="string">"url"</span>: <span class="string">"http://www.apple.com/shop/product/MJ2R2/magic-trackpad-2?fnode=4c"</span>,</span><br><span class="line">    <span class="string">"price"</span>: <span class="string">u"$129.00"</span>,</span><br><span class="line">    <span class="string">"name"</span>: <span class="string">u"Magic Trackpad 2"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>所以，当你爬取的网站使用微观数据来将语义信息添加到它的内容中时，使用<a href="https://github.com/scrapinghub/extruct" target="_blank" rel="noopener"> Extruct </a>。这是一个比依赖统一的页面布局或者浪费时间分析 HTML 源代码更健壮的解决方案。</p><h2 id="使用-js2xml-抓取嵌入在-JavaScript-代码段中的数据"><a href="#使用-js2xml-抓取嵌入在-JavaScript-代码段中的数据" class="headerlink" title="使用 js2xml 抓取嵌入在 JavaScript 代码段中的数据"></a>使用 js2xml 抓取嵌入在 JavaScript 代码段中的数据</h2><p>你是否曾经受挫于你的浏览器呈现的网页与Scrapy下载的网页之间的差距？这可能是因为该网页中的一些内容并不在服务器发送给你的响应中。相反，它们是由你的浏览器通过JavaScript代码生成的。</p><p>你可以通过将此请求传递给一个例如<a href="http://scrapinghub.com/splash/" target="_blank" rel="noopener"> Splash </a>的 JavaScript 渲染服务来解决此问题。Splash 运行页面上的 JavaScript ，然后返回最终的页面结构以供你的爬虫使用。</p><p>Splash 专门为此设计，并<a href="http://blog.scrapinghub.com/2015/03/02/handling-javascript-in-scrapy-with-splash/" target="_blank" rel="noopener"> 与Scrapy很好的整合在一起 </a>。然而，在某些情况下，你需要的是一些简单功能，例如从一个 JavaScript 段中获取一个变量的值，所以使用这种强大的工具将大材小用。而这恰恰是 <a href="https://github.com/redapple/js2xml" target="_blank" rel="noopener"> js2xml </a> 的用武之地。它是一个将 JavaScript 代码转换成 XML 数据的库。</p><p>例如，假设一个在线零售商网站通过 JavaScript 加载产品评级。混合在该 HTML 中有这样一段 JavaScript 代码：<br><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">script</span> <span class="attr">type</span>=<span class="string">"text/javascript"</span>&gt;</span><span class="undefined"></span></span><br><span class="line"><span class="actionscript">    <span class="keyword">var</span> totalReviewsValue = <span class="number">32</span>;</span></span><br><span class="line"><span class="actionscript">    <span class="keyword">var</span> averageRating = <span class="number">4.5</span>;</span></span><br><span class="line"><span class="actionscript">    <span class="keyword">if</span>(totalReviewsValue != <span class="number">0</span>)&#123;</span></span><br><span class="line"><span class="actionscript">        events = <span class="string">"..."</span>;</span></span><br><span class="line"><span class="undefined">    &#125;</span></span><br><span class="line"><span class="undefined">    ...</span></span><br><span class="line"><span class="undefined"></span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br></pre></td></tr></table></figure></p><p>要使用 js2xml 提取 <code>averageRating</code> 的值，我们首选需要提取 <code>&lt;script&gt;</code>块，然后使用 js2xml 将此代码转换成 XML ：<br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>js_code = response.xpath(<span class="string">"//script[contains(., 'averageRating')]/text()"</span>).extract_first()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> js2xml</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>parsed_js = js2xml.parse(js_code)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">print</span> js2xml.pretty_print(parsed_js)</span><br><span class="line">&lt;program&gt;</span><br><span class="line">  &lt;var name=<span class="string">"totalReviewsValue"</span>&gt;</span><br><span class="line">    &lt;number value=<span class="string">"32"</span>/&gt;</span><br><span class="line">  &lt;/var&gt;</span><br><span class="line">  &lt;var name=<span class="string">"averageRating"</span>&gt;</span><br><span class="line">    &lt;number value=<span class="string">"4.5"</span>/&gt;</span><br><span class="line">  &lt;/var&gt;</span><br><span class="line">  &lt;<span class="keyword">if</span>&gt;</span><br><span class="line">    &lt;predicate&gt;</span><br><span class="line">      &lt;binaryoperation operation=<span class="string">"!="</span>&gt;</span><br><span class="line">        &lt;left&gt;&lt;identifier name="totalReviewsValue"/&gt;&lt;/left&gt;</span><br><span class="line">        &lt;right&gt;&lt;number value="0"/&gt;&lt;/right&gt;</span><br><span class="line">      &lt;/binaryoperation&gt;</span><br><span class="line">    &lt;/predicate&gt;</span><br><span class="line">    &lt;then&gt;</span><br><span class="line">      &lt;block&gt;</span><br><span class="line">        &lt;assign operator=<span class="string">"="</span>&gt;</span><br><span class="line">          &lt;left&gt;&lt;identifier name="events"/&gt;&lt;/left&gt;</span><br><span class="line">          &lt;right&gt;&lt;string&gt;...&lt;/string&gt;&lt;/right&gt;</span><br><span class="line">        &lt;/assign&gt;</span><br><span class="line">      &lt;/block&gt;</span><br><span class="line">    &lt;/then&gt;</span><br><span class="line">  &lt;/if&gt;</span><br><span class="line">&lt;/program&gt;</span><br></pre></td></tr></table></figure></p><p>现在，只需要建立一个 Scrapy 的 Selector ，然后使用 XPath 获取我们想要的值：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>js_sel = scrapy.Selector(_root=parsed_js)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>js_sel.xpath(<span class="string">"//program/var[@name='averageRating']/number/@value"</span>).extract_first()</span><br><span class="line"><span class="string">u'4.5'</span></span><br></pre></td></tr></table></figure></p><p>虽然你可能用思考的速度就可以编写一个正则表达式来解决这个问题，但是，一个 JavaScript 解析器将会更可靠。我们这里使用的例子是非常简单的，但在一些更复杂的例子中，正则表达式可能更难以维护得多。</p><h2 id="使用-w3lib-url-中的函数来从-URL-中抓取数据"><a href="#使用-w3lib-url-中的函数来从-URL-中抓取数据" class="headerlink" title="使用 w3lib.url 中的函数来从 URL 中抓取数据"></a>使用 w3lib.url 中的函数来从 URL 中抓取数据</h2><p>有时候，你感兴趣的数据段并不单独在一个 HTML 标签内。通常，你需要从页面上列出的 URL 中获取一些参数的值。例如，你可能对获取在 HTML 中列出的 URL 中的 <code>username</code> 的值感兴趣：<br><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"users"</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">ul</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">li</span>&gt;</span><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"/users?username=johndoe23"</span>&gt;</span>John Doe<span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">li</span>&gt;</span><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"/users?active=0&amp;username=the_jan"</span>&gt;</span>Jan Roe<span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line">     …</span><br><span class="line">    <span class="tag">&lt;<span class="name">li</span>&gt;</span><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"/users?active=1&amp;username=janie&amp;ref=b1946ac9249&amp;gas=_ga=1.234.567"</span>&gt;</span>Janie Doe<span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">ul</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br></pre></td></tr></table></figure></p><p>也许你会试图使用<a href="https://xkcd.com/208/" target="_blank" rel="noopener"> 正则表达式的超能力 </a>, 但是，请淡定，这里的<a href="https://github.com/scrapy/w3lib" target="_blank" rel="noopener"> w3lib </a>有一个更可靠的解决方案可以挽救局面：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> w3lib.url <span class="keyword">import</span> url_query_parameter</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>url_query_parameter(<span class="string">'/users?active=0&amp;username=the_jan'</span>, <span class="string">'username'</span>)</span><br><span class="line">    <span class="string">'the_jan'</span></span><br></pre></td></tr></table></figure></p><p>假如你对<a href="https://github.com/scrapy/w3lib" target="_blank" rel="noopener"> w3lib </a>感到陌生，那么看一看<a href="http://w3lib.readthedocs.org/en/latest/w3lib.html" target="_blank" rel="noopener"> 文档 </a>。稍后，我们将在我们的 <strong>跟着高手学习Scrapy技巧</strong> 系列中覆盖此Python库的一些其他功能。</p><h2 id="用-Scrapy-SitemapSpider-抓取网站"><a href="#用-Scrapy-SitemapSpider-抓取网站" class="headerlink" title="用 Scrapy SitemapSpider 抓取网站"></a>用 Scrapy SitemapSpider 抓取网站</h2><p>网络抓取工具以 URL 为基础。他们拥有的越多，他们生活的时间越长。为任何特定网站找到一个好的网址来源非常重要，因为它为抓取工具提供了一个强有力的起点。</p><p>站点地图是种子网址的绝佳来源。网站开发人员使用它们来指示哪些URL可用于以机器可读格式进行爬网。站点地图也是发现网页的好方法，否则无法访问网页，因为有些网页可能未链接到站点地图以外的任何其他页面。</p><p>站点地图通常可在<code>/sitemap.xml</code>，<code>robots.txt</code>文件中指定的不同位置或位于不同位置。</p><p>使用 Scrapy ，您无需担心解析 XML 和发出请求。它包含一个 SitemapSpider 类，您可以继承它以处理所有这些问题。</p><p><strong>SitemapSpider in Action</strong>：假设您想抓取 Apple 的网站来检查不同的产品。您希望访问尽可能多的网页，以便尽可能多地抓取数据。幸运的是，Apple 的网站在 <a href="http://apple.com/sitemap.xml" target="_blank" rel="noopener"> apple.com/sitemap.xml </a> 上提供了一个网站地图，如下所示：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">urlset</span> <span class="attr">xmlns</span>=<span class="string">"http://www.sitemaps.org/schemas/sitemap/0.9"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">url</span>&gt;</span><span class="tag">&lt;<span class="name">loc</span>&gt;</span>http://www.apple.com/<span class="tag">&lt;/<span class="name">loc</span>&gt;</span><span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">url</span>&gt;</span><span class="tag">&lt;<span class="name">loc</span>&gt;</span>http://www.apple.com/about/<span class="tag">&lt;/<span class="name">loc</span>&gt;</span><span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="tag">&lt;<span class="name">url</span>&gt;</span><span class="tag">&lt;<span class="name">loc</span>&gt;</span>http://www.apple.com/about/workingwithapple.html<span class="tag">&lt;/<span class="name">loc</span>&gt;</span><span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">url</span>&gt;</span><span class="tag">&lt;<span class="name">loc</span>&gt;</span>http://www.apple.com/accessibility/<span class="tag">&lt;/<span class="name">loc</span>&gt;</span><span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line">    ...</span><br><span class="line"><span class="tag">&lt;/<span class="name">urlset</span>&gt;</span></span><br></pre></td></tr></table></figure><p>Scrapy 的通用 <a href="http://doc.scrapy.org/en/latest/topics/spiders.html?_ga=2.191979183.427721090.1523448002-1918622539.1516814412#sitemapspider" target="_blank" rel="noopener"> SitemapSpider </a> 类实现了解析和分派处理站点地图所需的所有请求的逻辑。它从站点地图中读取和提取 URL ，并会为它找到的每个 URL 分派一个请求。这是一个蜘蛛，它会使用网站地图作为种子来刮掉 Apple 的网站：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.spiders <span class="keyword">import</span> SitemapSpider</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AppleSpider</span><span class="params">(SitemapSpider)</span>:</span></span><br><span class="line">    name = <span class="string">'apple-spider'</span></span><br><span class="line">    sitemap_urls = [<span class="string">'http://www.apple.com/sitemap.xml'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="keyword">yield</span> &#123;</span><br><span class="line">            <span class="string">'title'</span>: response.css(<span class="string">"title ::text"</span>).extract_first(),</span><br><span class="line">            <span class="string">'url'</span>: response.url</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment"># ...</span></span><br></pre></td></tr></table></figure><p>正如您所看到的，您只需要 SiteSiteider 的子类并将该站点地图的 URL 添加到该 sitemap_urls 属性。</p><p>现在，运行 spider 并检查结果：<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> scrapy runspider apple_spider.py -o items.jl --nolog</span></span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> head -n 5 items.jl </span></span><br><span class="line">&#123;"url": "http://www.apple.com/", "title": "Apple"&#125;</span><br><span class="line">&#123;"url": "http://www.apple.com/ae/support/products/iphone.html", "title": "Support - AppleCare+ - iPhone - Apple (AE)"&#125;</span><br><span class="line">&#123;"url": "http://www.apple.com/ae/support/products/ipad.html", "title": "Support - AppleCare+ - iPad - Apple (AE)"&#125;</span><br><span class="line">&#123;"url": "http://www.apple.com/ae/support/products/", "title": "Support - AppleCare - Apple (AE)"&#125;</span><br><span class="line">&#123;"url": "http://www.apple.com/ae/support/ipod/", "title": "iPod - Apple Support"&#125;</span><br></pre></td></tr></table></figure></p><p>Scrapy 会在站点地图中为 SitemapSpider 找到的每个网址发送一个请求，然后它会调用 parse 处理它获取的每个响应的方法。但是，网站中的某些页面在结构上可能会有所不同，因此您可能希望对不同类型的页面使用多个回调。</p><p>例如，您可以定义一个特定的回调来处理 Mac 页面，另一个用于 iTunes 页面和 parse 所有其他页面的默认方法：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.spiders <span class="keyword">import</span> SitemapSpider</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AppleSpider</span><span class="params">(SitemapSpider)</span>:</span></span><br><span class="line">    name = <span class="string">'apple-spider'</span></span><br><span class="line">    sitemap_urls = [<span class="string">'http://www.apple.com/sitemap.xml'</span>]</span><br><span class="line">    sitemap_rules = [</span><br><span class="line">        (<span class="string">'/mac/'</span>, <span class="string">'parse_mac'</span>),</span><br><span class="line">        (<span class="string">'/itunes/'</span>, <span class="string">'parse_itunes'</span>),</span><br><span class="line">        (<span class="string">''</span>, <span class="string">'parse'</span>)</span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        self.log(<span class="string">"default parsing method for &#123;&#125;"</span>.format(response.url))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_mac</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        self.log(<span class="string">"parse_mac method for &#123;&#125;"</span>.format(response.url))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_itunes</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        self.log(<span class="string">"parse_itunes method for &#123;&#125;"</span>.format(response.url))</span><br></pre></td></tr></table></figure><p>要做到这一点，您必须为 <code>sitemap_rules</code> 您的类添加一个属性，将 URL 模式映射到回调。例如，与 <code>/mac/</code> 模式匹配的 URL 将通过该 <code>parse_mac</code> 方法处理其响应。</p><p>因此，下次您编写抓取工具时，如果您想全面抓取网站，请务必使用 SitemapSpider 。</p><p>有关更多功能，请查看<a href="http://doc.scrapy.org/en/latest/topics/spiders.html?_ga=2.229931289.427721090.1523448002-1918622539.1516814412#sitemapspider" target="_blank" rel="noopener"> SitemapSpider </a>的文档。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;原文：&lt;a href=&quot;http://blog.scrapinghub.com/2016/01/19/scrapy-tips-from-the-pros-part-1/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt; 跟着高手学习Scrapy技巧：第一部分 &lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;a href=&quot;http://scrapy.org&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Scrapy &lt;/a&gt; 是&lt;a href=&quot;http://scrapinghub.com&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt; Scrapinghub &lt;/a&gt;  的关键部分。我们广泛地采用此框架，并已积累了许多各种不同的快捷方法来解决常见问题。我们推出了一个系列来与大家分享这些 Scrapy 的技巧，这样，你就可以在你的日常工作流程中最有效的使用它。每一个博文将给出两到三个提示，敬请关注。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://dongfei.oss-cn-shanghai.aliyuncs.com/blog/scrapylogo.png&quot; alt=&quot;scrapylogo&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Python" scheme="http://blog.dongfei.xin/categories/Python/"/>
    
      <category term="Scrapy" scheme="http://blog.dongfei.xin/categories/Python/Scrapy/"/>
    
    
      <category term="Scrapy" scheme="http://blog.dongfei.xin/tags/Scrapy/"/>
    
  </entry>
  
  <entry>
    <title>精通 Scrapy 网络爬虫（代理篇）</title>
    <link href="http://blog.dongfei.xin/2018-04-08/%E7%B2%BE%E9%80%9A-Scrapy-%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%EF%BC%88%E4%BB%A3%E7%90%86%E7%AF%87%EF%BC%89/"/>
    <id>http://blog.dongfei.xin/2018-04-08/精通-Scrapy-网络爬虫（代理篇）/</id>
    <published>2018-04-08T06:44:23.000Z</published>
    <updated>2018-04-12T11:22:50.354Z</updated>
    
    <content type="html"><![CDATA[<h5 id="使用-HTTP-代理"><a href="#使用-HTTP-代理" class="headerlink" title="使用 HTTP 代理"></a>使用 HTTP 代理</h5><p>HTTP 代理服务器可以比作客户端与 Web 服务器（网站）之间的一个信息中转站，客户端发送的 HTTP 请求和 Web 服务器返回的 HTTP 响应通过代理服务器转发给对方，如图所示：</p><p><img src="https://dongfei.oss-cn-shanghai.aliyuncs.com/blog/proxy1.jpg" alt="proxy1"></p><a id="more"></a><p>爬虫程序在爬取某些网站时也需要使用代理。例如：</p><ul><li><p>由于网络环境园素，直接爬取速度太慢，使用代理提高爬取速度。</p></li><li><p>某些网站对用户的访问速度进行限制，爬取过快会被封禁 ip ，使用代理防止被封禁。</p></li><li><p>由于地方法律或政治原因，某些网站无法直接访问，使用代理绕过访问限制。</p></li></ul><h5 id="HttpProxyMiddleware"><a href="#HttpProxyMiddleware" class="headerlink" title="HttpProxyMiddleware"></a>HttpProxyMiddleware</h5><p>Scrapy 内部提供了一个下载中间件 HttpProxyMiddleware ，专门用于给 Scrapy 爬虫设置代理。</p><h5 id="使用简介"><a href="#使用简介" class="headerlink" title="使用简介"></a>使用简介</h5><p>HttpProxyMiddleware 默认便是启用的，它会在系统环境变量中搜索当前系统代理（名字格式为 XXX_proxy 的环境变量），作为 scrapy 爬虫使用的代理。 </p><p><a href="http://cn-proxy.com/" target="_blank" rel="noopener">最新中国 ip 地址代理服务器</a></p><p>为本机的 Scrapy 爬虫分别设置发送 HTTP 和 HTTPS 请求时所使用的代理，只需要 bash 中添加加环境变量：<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">Linux 系统设置环境变量：</span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">export</span>  http_proxy=<span class="string">"http://120.92.118.64:10010"</span>  <span class="comment"># 为 HTTP 请求设置代理</span></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">export</span>  https_proxy=<span class="string">"http://118.114.77.47:8080"</span>  <span class="comment"># 为 HTTPS 请求设置代理</span></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">export</span>  <span class="comment"># 查看变量</span></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">unset</span> http_proxy  <span class="comment"># 删除变量</span></span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Windows 系统设置环境变量：</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">set</span> http_proxy=<span class="string">"http://120.92.118.64:10010"</span> <span class="comment"># 为 HTTP 请求设置代理</span></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">set</span> https_proxy=<span class="string">"http://118.114.77.47:8080"</span> <span class="comment"># 为 HTTPS 请求设置代理</span></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">set</span> <span class="comment"># 查看变量</span></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">set</span> http_proxy= <span class="comment"># 删除变量</span></span></span><br></pre></td></tr></table></figure></p><p>配置完成后，Scrapy 爬虫将会使用上面指定的代理下载页面。</p><p>示例，利用网站 <a href="http://httpbin.org" target="_blank" rel="noopener">http://httpbin.org</a> 提供的服务可以窥视我们所发送的 HTTP(S) 请求，如请求源 IP 地址、请求头部、Cookie 信息等。如图展示了该网站各种服务的 API 地址：</p><p><img src="https://dongfei.oss-cn-shanghai.aliyuncs.com/blog/proxy2.jpg" alt="proxy2"></p><p>访问 <a href="http://httpbin.org/ip" target="_blank" rel="noopener">http://httpbin.org/ip</a> 将返回一个包含请求源 IP 地址信息的 json 串，在 scrapy shell 中访问该 url ，查看请求源 IP 地址：</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">(py3) [root@izuf6g6v8wminmgpw6vd89z py3]<span class="comment"># scrapy shell </span></span><br><span class="line"></span><br><span class="line">。。。。。。。。。。。。</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; import json</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; fetch(scrapy.Request(<span class="string">'http://httpbin.org/ip'</span>))</span><br><span class="line"><span class="number">2018</span>-<span class="number">04</span>-08 <span class="number">22</span><span class="symbol">:</span><span class="number">44</span><span class="symbol">:</span><span class="number">26</span> [scrapy.core.engine] <span class="symbol">INFO:</span> Spider opened</span><br><span class="line"><span class="number">2018</span>-<span class="number">04</span>-08 <span class="number">22</span><span class="symbol">:</span><span class="number">44</span><span class="symbol">:</span><span class="number">26</span> [scrapy.core.engine] <span class="symbol">DEBUG:</span> Crawled (<span class="number">200</span>) &lt;GET <span class="symbol">http:</span>/<span class="regexp">/httpbin.org/ip</span>&gt; (<span class="symbol">referer:</span> None)</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; json.loads(response.text)</span><br><span class="line">&#123;<span class="string">'origin'</span>: <span class="string">'120.92.118.64'</span>&#125;</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; fetch(scrapy.Request(<span class="string">'https://httpbin.org/ip'</span>))</span><br><span class="line"><span class="number">2018</span>-<span class="number">04</span>-08 <span class="number">22</span><span class="symbol">:</span><span class="number">44</span><span class="symbol">:</span><span class="number">52</span> [scrapy.core.engine] <span class="symbol">DEBUG:</span> Crawled (<span class="number">200</span>) &lt;GET <span class="symbol">https:</span>/<span class="regexp">/httpbin.org/ip</span>&gt; (<span class="symbol">referer:</span> None)</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; json.loads(response.text)</span><br><span class="line">&#123;<span class="string">'origin'</span>: <span class="string">'118.114.77.47'</span>&#125;</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt;</span><br></pre></td></tr></table></figure><p>在上述实验中，分别以 HTTP 和 HTTPS 发送请求，使用 json 模块对响应结果进行解析，读取请求源 IP 地址（ origin 字段），其值正是代理服务器的 IP。由此证明，Scrapy 爬虫使用了指定的代理。</p><p>上面使用的是无须身份验证的代理服务器，还有一些代理服务器需要用户提供账号、密码进行身份验证，验证成功后才提供代理服务，使用此类代理时，可按以下格式配置：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="builtin-name">export</span> <span class="attribute">http_proxy</span>=<span class="string">"http://dongfei:123456@39.134.10.98:8080"</span></span><br></pre></td></tr></table></figure><h5 id="源码分析"><a href="#源码分析" class="headerlink" title="源码分析"></a>源码分析</h5><p><strong>HttpProxyMiddleware</strong> ：<code>Lib/site-packages/scrapy/downloadermiddlewares/httpproxy.py</code></p><p>分析代码如下：</p><ul><li>__init__ 方法</li></ul><p>在 HttpProxyMiddleware 的构造器中，使用 Python 标准库 urllib 中的 getproxies 函数在系统环境变量中搜索系统代理的相关配置（变量名格式为 [协议]_proxy 的变量）<br>调用 self._get_proxy 方法解析代理配置信息，并将其返回结果保存到 self.proxies 字段中，如果没有找到任何代理配置，就拋出 NotConfigured 异常，HttpProxyMiddleware 被弃用。</p><ul><li>_get_proxy 方法</li></ul><p>解析代理配置信息，返回身份验证信息以及代理服务器 url。</p><ul><li>process_request 方法</li></ul><p>处理每一个待发送的请求，为没有设置过代理的请求（meta 属性不包含 proxy 的请求）调用 self._set_proxy 方法设置代理。</p><ul><li>_set_proxy 方法</li></ul><p>为一个请求设置代理，以请求的协议（HTTP 或 HTTPS ）作为键，从代理服务器信息字典 self.proxies 中选择代理，赋值给 request.meta 的 proxy 字段。对于需要身份验证的代理服务器，添加 HTTP 头部 Proxy-Authorization ，其值是在 _get_proxy 方法中计算得到的。</p><p>经分析得知，在 Scrapy 中为一个请求设置代理的本质就是将代理服务器的 url 时填写到 request.meta[‘proxy’]。</p><h5 id="使用多个代理"><a href="#使用多个代理" class="headerlink" title="使用多个代理"></a>使用多个代理</h5><p>利用 HttpProxyMiddleware 为爬虫设置代理时，对于一种协议（ HTTP 或 HTTPS ）的所有请求只能使用一个代理，如果想使用多个代理，可以在构造每一个 Request 对象时，通过 meta 参数的 proxy 字段手动设置代理 ：<br><figure class="highlight lisp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">request = Request('http<span class="symbol">://example</span>.com/1', meta=&#123;'proxy':'http<span class="symbol">://42</span>.<span class="number">178.202</span>.<span class="number">18</span>:<span class="number">8080</span>'&#125;)</span><br><span class="line">request = Request('http<span class="symbol">://example</span>.com/2', meta=&#123;'proxy':'http<span class="symbol">://182</span>.<span class="number">18.202</span>.<span class="number">18</span>:<span class="number">8080</span>'&#125;)</span><br><span class="line">request = Request('http<span class="symbol">://example</span>.com/3', meta=&#123;'proxy':'http<span class="symbol">://89</span>.<span class="number">190.202</span>.<span class="number">18</span>:<span class="number">8080</span>'&#125;)</span><br></pre></td></tr></table></figure></p><p>按照与之前相同的做法，在 scrapy shell 进行实验，验证代理是否被使用：</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">(py3) [root@izuf6g6v8wminmgpw6vd89z py3]<span class="comment"># scrapy shell </span></span><br><span class="line"></span><br><span class="line">。。。。。。。。。。。。</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; import json</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; from scrapy import Request</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; r = Request(<span class="string">'http://httpbin.org/ip'</span>, meta=&#123;<span class="string">'proxy'</span>: <span class="string">'http://39.134.10.18:8080'</span>&#125;)</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; fetch(r)</span><br><span class="line"><span class="number">2018</span>-<span class="number">04</span>-08 <span class="number">19</span><span class="symbol">:</span><span class="number">43</span><span class="symbol">:</span><span class="number">06</span> [scrapy.core.engine] <span class="symbol">DEBUG:</span> Crawled (<span class="number">200</span>) &lt;GET <span class="symbol">http:</span>/<span class="regexp">/httpbin.org/ip</span>&gt; (<span class="symbol">referer:</span> None)</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; json.loads(response.text)</span><br><span class="line">&#123;<span class="string">'origin'</span>: <span class="string">'39.134.10.18'</span>&#125;</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; r = Request(<span class="string">'https://httpbin.org/ip'</span>, meta=&#123;<span class="string">'proxy'</span>: <span class="string">'http://42.178.202.18:8080'</span>&#125;)</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; fetch(r)</span><br><span class="line"><span class="number">2018</span>-<span class="number">04</span>-08 <span class="number">19</span><span class="symbol">:</span><span class="number">44</span><span class="symbol">:</span><span class="number">06</span> [scrapy.core.engine] <span class="symbol">DEBUG:</span> Crawled (<span class="number">200</span>) &lt;GET <span class="symbol">https:</span>/<span class="regexp">/httpbin.org/ip</span>&gt; (<span class="symbol">referer:</span> None)</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; json.loads(response.text)</span><br><span class="line">&#123;<span class="string">'origin'</span>: <span class="string">'42.178.202.18'</span>&#125;</span><br></pre></td></tr></table></figure><p>结果表明，Scrapy 爬虫同样使用了指定的代理服务器。</p><p>使用手动方式设置代理时，如果使用的代理需要身份验证，还需要通过 HTTP 头部的 Proxy-Authorization 字段传递包含用户账号和密码的身份验证信息。可以参考 HttpProxyMiddleware._get_proxy 中的相关实现，按以下过程生成身份验证信息：</p><p>（1）将账号、密码拼接成形如 <code>user:passwd</code> 的字符串 s1。<br>（2）按代理服务器要求对 s1 进行编码（如 utf8 ），生成 s2 。<br>（3）再对 s2 进行 Base64 编码，生成 s3。<br>（4）将 s3 拼接到固定字节串 b<code>Basic</code> 后面，得到最终的身份验证信息。</p><p>示例代码如下：</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;</span>&gt; from scrapy import Request</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; import base64</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; req = Request(<span class="string">'http://httpbin.org/ip'</span>, meta=&#123;<span class="string">'proxy'</span>: <span class="string">'http://42.178.202.18:8080'</span>&#125;)</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; user = <span class="string">'dongfei'</span></span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; passwd = <span class="string">'12345678'</span></span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; user_passwd = (<span class="string">'%s:%s'</span><span class="string">%(user,passwd)</span>).encode(<span class="string">'utf8'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; user_passwd</span><br><span class="line">b<span class="string">'dongfei:12345678'</span></span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; req.headers[<span class="string">'Proxy-Authorization'</span>] = b<span class="string">'Basic'</span> + base64.b64encode(user_passwd)</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; fetch(req)</span><br><span class="line">.........................</span><br></pre></td></tr></table></figure><h5 id="获取免费代理"><a href="#获取免费代理" class="headerlink" title="获取免费代理"></a>获取免费代理</h5><p>可以通过 google 或 baidu 找到一些提供免费代理服务器信息的网站。例如：</p><ul><li><a href="http://proxy-list.org" target="_blank" rel="noopener">http://proxy-list.org</a>（国外）</li><li><a href="http://free-proxy-list.net" target="_blank" rel="noopener">http://free-proxy-list.net</a>（国外）</li><li><a href="http://www.xicidaili.com" target="_blank" rel="noopener">http://www.xicidaili.com</a></li><li><a href="http://www.proxy360.cn" target="_blank" rel="noopener">http://www.proxy360.cn</a></li><li><a href="http://www.kuaidaili.com" target="_blank" rel="noopener">http://www.kuaidaili.com</a></li><li><a href="http://cn-proxy.com/" target="_blank" rel="noopener">http://cn-proxy.com/</a></li></ul><p>以 <a href="http://www.xicidaili.com" target="_blank" rel="noopener">http://www.xicidaili.com</a> 为例，如图所示为该网站 <strong>国内高匿代理</strong> 分类下的页面：</p><p><img src="https://dongfei.oss-cn-shanghai.aliyuncs.com/blog/proxy3.jpg" alt="proxy3"></p><p>接下来爬取 <strong>国内高匿代理</strong> 分类中前 3 页的所有代理服务器信息。并验证每个代理是否可用。</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy genspider xici_proxy www<span class="selector-class">.xicidaili</span><span class="selector-class">.com</span></span><br></pre></td></tr></table></figure><p>该网站会监测用户发送的 HTTP 请求头部中的 User-Agent 字段，因此我们需要伪装成某种常规浏览器，在配置文件添加如下代码：<br><figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">custom_settings = &#123;</span><br><span class="line">    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.<span class="number">3112.11</span>3 Safari/537.36',</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>实现 XiciProxySpider 爬取代理服务器信息，并过滤不可用代理，代码如下：</p><figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding:utf<span class="number">-8</span> -*-</span><br><span class="line">__author__ = <span class="string">"東飛"</span></span><br><span class="line">__date__ = <span class="string">"2017-11-29"</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> Request, FormRequest</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> XiciProxySpider(scrapy.Spider):</span><br><span class="line">    name = <span class="string">'xici_proxy'</span></span><br><span class="line">    allowed_domains = [<span class="string">'www.xicidaili.com'</span>]</span><br><span class="line">    start_urls = [<span class="string">'http://www.xicidaili.com/nn/'</span>]</span><br><span class="line"></span><br><span class="line">    custom_settings = &#123;</span><br><span class="line">        <span class="string">"DOWNLOADER_MIDDLEWARES"</span>: &#123;</span><br><span class="line">            # 置于 HttpProxyMiddleware(<span class="number">750</span>) 之前</span><br><span class="line">            # <span class="string">'ArticleSpider.middlewares.RandomUserAgentMiddleware'</span>: <span class="number">745</span>,</span><br><span class="line">            # <span class="string">'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware'</span>: None,</span><br><span class="line">            <span class="string">'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware'</span>: None,</span><br><span class="line">            <span class="string">'scrapy_fake_useragent.middleware.RandomUserAgentMiddleware'</span>: <span class="number">400</span>,</span><br><span class="line">        &#125;,</span><br><span class="line">        # <span class="string">'DOWNLOAD_DELAY'</span>: <span class="number">2</span>,</span><br><span class="line">        <span class="string">'COOKIES_ENABLED'</span>: <span class="literal">False</span>,</span><br><span class="line">        # <span class="string">'DOWNLOAD_TIMEOUT'</span>: <span class="number">180</span>,</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    def start_requests(self):</span><br><span class="line">        # 爬取 http:<span class="comment">//www.xicidaili.com/nn/ 前 3 页</span></span><br><span class="line">        for i <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">3</span>):</span><br><span class="line">            yield Request(<span class="string">'http://www.xicidaili.com/nn/%s'</span> % i)</span><br><span class="line"></span><br><span class="line">    def parse(self, response):</span><br><span class="line"></span><br><span class="line">        for sel <span class="keyword">in</span> response.xpath(<span class="string">'//table[@id="ip_list"]/tr[position()&gt;1]'</span>):</span><br><span class="line">            # 提取代理 IP、port、scheme(http or https)</span><br><span class="line">            ip = sel.css(<span class="string">'td:nth-child(2)::text'</span>).extract_first()</span><br><span class="line">            port = sel.css(<span class="string">'td:nth-child(3)::text'</span>).extract_first()</span><br><span class="line">            scheme = sel.css(<span class="string">'td:nth-child(6)::text'</span>).extract_first().lower()</span><br><span class="line"></span><br><span class="line">            # 使用爬取到的代理再次发送请求到 http(s):<span class="comment">//httpbin.org/ip ，验证代理是否可用</span></span><br><span class="line"></span><br><span class="line">            url = <span class="string">'%s://httpbin.org/ip'</span> % scheme</span><br><span class="line">            # url = <span class="string">'%s://ip.taobao.com/service/getIpInfo.php'</span> % scheme</span><br><span class="line">            # url = <span class="string">'%s://fp.ip-api.com/json'</span> % scheme</span><br><span class="line">            # url = <span class="string">'%s://ip-api.com/json'</span> % scheme</span><br><span class="line"></span><br><span class="line">            proxy = <span class="string">'%s://%s:%s'</span> % (scheme, ip, port)</span><br><span class="line"></span><br><span class="line">            # formdata = &#123;</span><br><span class="line">            #     <span class="string">'ip'</span>: <span class="string">'myip'</span>,</span><br><span class="line">            # &#125;</span><br><span class="line"></span><br><span class="line">            meta = &#123;</span><br><span class="line">                <span class="string">'proxy'</span>: proxy,</span><br><span class="line">                <span class="string">'dont_retry'</span>: <span class="literal">True</span>,</span><br><span class="line">                <span class="string">'download_timeout'</span>: <span class="number">10</span>,</span><br><span class="line"></span><br><span class="line">                # 以下两个字段是传递给 check_available 方法的信息，方便检测</span><br><span class="line">                <span class="string">'_proxy_scheme'</span>: scheme,</span><br><span class="line">                <span class="string">'_proxy_ip'</span>: ip,</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            yield Request(url, callback=self.check_available, meta=meta, dont_filter=<span class="literal">True</span>)</span><br><span class="line">            # yield FormRequest(url, callback=self.check_available, formdata=formdata, meta=meta, dont_filter=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    def check_available(self, response):</span><br><span class="line">        proxy_ip = response.meta[<span class="string">'_proxy_ip'</span>]</span><br><span class="line"></span><br><span class="line">        # <span class="keyword">if</span> proxy_ip == json.loads(response.text)[<span class="string">'data'</span>][<span class="string">'ip'</span>]:</span><br><span class="line">            # <span class="keyword">from</span> pprint <span class="keyword">import</span> pprint</span><br><span class="line">            # pprint(&#123;</span><br><span class="line">            #         <span class="string">'ip'</span>: json.loads(response.text)[<span class="string">'query'</span>],</span><br><span class="line">            #         <span class="string">'user_agent'</span>: json.loads(response.text)[<span class="string">'user_agent'</span>],</span><br><span class="line">            #     &#125;)</span><br><span class="line">            # <span class="keyword">if</span> proxy_ip == json.loads(response.text)[<span class="string">'query'</span>]:</span><br><span class="line">        </span><br><span class="line">        # 判断代理是否具有隐藏 IP 功能</span><br><span class="line">        <span class="keyword">if</span> proxy_ip == json.loads(response.text)[<span class="string">'origin'</span>]:</span><br><span class="line">            yield &#123;</span><br><span class="line">                <span class="string">'proxy_scheme'</span>: response.meta[<span class="string">'_proxy_scheme'</span>],</span><br><span class="line">                <span class="string">'proxy'</span>: response.meta[<span class="string">'proxy'</span>],</span><br><span class="line">            &#125;</span><br></pre></td></tr></table></figure><p>解释上述代码如下：</p><ul><li><p>在 start_requests 方法中请求 <a href="http://www.xicidaili.com/nn/" target="_blank" rel="noopener">http://www.xicidaili.com/nn/</a> 下的前 3 页，以 parse 方法作为页面解析函数。</p></li><li><p>在 parse 方法中提取一个页面中所有的代理服务器信息，这些代理未必都是可用的，因此使用爬取到的代理发送请求到 <code>http(s)://httpbin.org/ip</code> 验证其是否可用。以 check_available 方法作为页面解析函数。</p></li><li><p>能执行到 check_available 方法，意味着 response 对应请求所使用的代理是可用的。在 check_available 方法中，通过响应 json 串中的 origin 字段可以判断代理是否是匿名的（隐藏 ip），返回匿名代理。</p></li></ul><p>运行爬虫，将可用的代理服务器保存到 json 文件中，供其他程序使用：</p><figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">F:\VirtualEnv\jobboleArticle\ArticleSpider\ArticleSpider</span><br><span class="line">(jobboleArticle) $ scrapy crawl xici_proxy -o .\data\proxy_list.json</span><br><span class="line"></span><br><span class="line">....................</span><br><span class="line"></span><br><span class="line">(jobboleArticle) $ cat -n .\data\proxy_list.json</span><br><span class="line">     <span class="number">1</span>  [</span><br><span class="line">     <span class="number">2</span>  &#123;<span class="string">"proxy_scheme"</span>: <span class="string">"http"</span>, <span class="string">"proxy"</span>: <span class="string">"http://183.159.87.96:18118"</span>&#125;,</span><br><span class="line">     <span class="number">3</span>  &#123;<span class="string">"proxy_scheme"</span>: <span class="string">"https"</span>, <span class="string">"proxy"</span>: <span class="string">"https://60.177.228.86:18118"</span>&#125;,</span><br><span class="line">     <span class="number">4</span>  &#123;<span class="string">"proxy_scheme"</span>: <span class="string">"http"</span>, <span class="string">"proxy"</span>: <span class="string">"http://60.177.229.113:18118"</span>&#125;,</span><br><span class="line">     <span class="number">5</span>  &#123;<span class="string">"proxy_scheme"</span>: <span class="string">"http"</span>, <span class="string">"proxy"</span>: <span class="string">"http://183.159.93.180:18118"</span>&#125;,</span><br><span class="line">     <span class="number">6</span>  &#123;<span class="string">"proxy_scheme"</span>: <span class="string">"https"</span>, <span class="string">"proxy"</span>: <span class="string">"https://183.159.83.153:18118"</span>&#125;,</span><br><span class="line">     <span class="number">7</span>  &#123;<span class="string">"proxy_scheme"</span>: <span class="string">"https"</span>, <span class="string">"proxy"</span>: <span class="string">"https://114.99.29.251:18118"</span>&#125;,</span><br><span class="line">     <span class="number">8</span>  &#123;<span class="string">"proxy_scheme"</span>: <span class="string">"http"</span>, <span class="string">"proxy"</span>: <span class="string">"http://183.159.93.180:18118"</span>&#125;,</span><br><span class="line">     <span class="number">9</span>  &#123;<span class="string">"proxy_scheme"</span>: <span class="string">"https"</span>, <span class="string">"proxy"</span>: <span class="string">"https://183.159.94.136:18118"</span>&#125;,</span><br><span class="line">    <span class="number">10</span>  &#123;<span class="string">"proxy_scheme"</span>: <span class="string">"https"</span>, <span class="string">"proxy"</span>: <span class="string">"https://183.159.93.27:18118"</span>&#125;</span><br><span class="line">    <span class="number">11</span>  ]</span><br></pre></td></tr></table></figure><h5 id="实现随机代理"><a href="#实现随机代理" class="headerlink" title="实现随机代理"></a>实现随机代理</h5><p>某些网站为防止爬虫爬取会对接收到的请求进行监测，如果短时间内接收到了来自同一 IP 的大量请求，就判定该 IP 的主机在使用爬虫程序爬取网站，因而将该 IP 封禁（拒绝请求）。爬虫程序可以使用多个代理对此类网站进行爬取，此时单位时间的访问量会被多个代理分摊，从而避免封禁 IP 。</p><p>下面基于 HttpProxyMiddleware 实现一个随机代理下载中同件。</p><p>在 <code>middlewares.py</code> 中实现 <code>RandomHttpProxyMiddleware</code> 代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.downloadermiddlewares.httpproxy <span class="keyword">import</span> HttpProxyMiddleware</span><br><span class="line"><span class="keyword">from</span> scrapy.exceptions <span class="keyword">import</span> NotConfigured</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RandomHttpProxyMiddleware</span><span class="params">(HttpProxyMiddleware)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, auth_encoding=<span class="string">'latin-1'</span>, proxy_list_file=None)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> proxy_list_file:</span><br><span class="line">            <span class="keyword">raise</span> NotConfigured</span><br><span class="line"></span><br><span class="line">        self.auth_encoding = auth_encoding</span><br><span class="line">        <span class="comment"># 分别用两个列表维护 HTTP 和 HTTPS 代理，&#123;'http':[...],'https':[....]&#125;</span></span><br><span class="line">        self.proxies = defaultdict(list)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 从 json 文件中读取代理服务器信息，填入 self.proxies</span></span><br><span class="line">        <span class="keyword">with</span> open(proxy_list_file) <span class="keyword">as</span> f:</span><br><span class="line">            proxy_list = json.load(f)</span><br><span class="line">            <span class="keyword">for</span> proxy <span class="keyword">in</span> proxy_list:</span><br><span class="line">                scheme = proxy[<span class="string">'proxy_scheme'</span>]</span><br><span class="line">                url = proxy[<span class="string">'proxy'</span>]</span><br><span class="line">                self.proxies[scheme].append(self._get_proxy(url, scheme))</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_crawler</span><span class="params">(cls, crawler)</span>:</span></span><br><span class="line">        <span class="comment"># 从配置文件中读取用户验证信息的编码</span></span><br><span class="line">        auth_encoding = crawler.settings.get(<span class="string">'HTTPPROXY_AUTH_ENCODING'</span>,<span class="string">'latin-1'</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 从配置文件中读取代理服务器列表文件（json）的路径</span></span><br><span class="line">        proxy_list_file = crawler.settings.get(<span class="string">'HTTPPROXY_PROXY_LIST_FILE'</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> cls(auth_encoding,proxy_list_file)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_set_proxy</span><span class="params">(self, request, scheme)</span>:</span></span><br><span class="line">        <span class="comment"># 随机选择一个代理</span></span><br><span class="line">        creds, proxy = random.choice(self.proxies[scheme])</span><br><span class="line">        request.meta[<span class="string">'proxy'</span>] = proxy</span><br><span class="line">        <span class="keyword">if</span> creds:</span><br><span class="line">            request.headers[<span class="string">'Proxy-Authorization'</span>] = <span class="string">b'Basic '</span> + creds</span><br></pre></td></tr></table></figure><p>解释上述代码如下：</p><ul><li><p>仿照 HttpProxyMiddleware 构造器实现 RandomHttpProxyMiddleware 构造器，首先从代理服务器列表文件（配置文件中指定）中读取代理服务器信息。然后将它们按照协议 （ HTTP 或 HTTPS ）分别存入不同列表，由 self.proxies 字典维护。</p></li><li><p>_set_proxy 方法负责为每一个 Request 清单设置代理，覆写 _set_proxy 方法（覆盖基类方法），对于每一个 request ，根据请求协议获取 sdifproxis 中的代理服务器列表，然后从中随机抽取一个代理，赋值给 request.meta[‘proxy’]。</p></li></ul><p>在配置文件 <code>settings.py</code> 或者 <code>custom_settings</code> 中启用 RandomHttpProxyMiddleware ，并指定所要使用的代理服务器列表文件（json 文件），添加代码如下：</p><figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">custom_settings = &#123;</span><br><span class="line">    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.<span class="number">3112.11</span>3 Safari/537.36',</span><br><span class="line">    <span class="string">"DOWNLOADER_MIDDLEWARES"</span>: &#123;</span><br><span class="line">        <span class="meta"># 置于 HttpProxyMiddleware(750) 之前</span></span><br><span class="line">        'ArticleSpider.middlewares.RandomHttpProxyMiddleware': <span class="number">745</span>,</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="meta"># 使用之前在 http:<span class="comment">//www.xicidaili.com/ 网站爬取到的代理</span></span></span><br><span class="line">    'HTTPPROXY_PROXY_LIST_FILE': './data/proxy_list.json',</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>最后编写一个 TestRandomProxySpider 测试该中间件，重复向 发送请求，根据响应的请求源 IP 地址信息判断代理使用情况：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> Request</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TestRandomProxySpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">'test_random_proxy'</span></span><br><span class="line">    allowed_domains = [<span class="string">'httpbin.org'</span>]</span><br><span class="line">    start_urls = [<span class="string">'http://httpbin.org/'</span>]</span><br><span class="line">    custom_settings = &#123;</span><br><span class="line">        <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36'</span>,</span><br><span class="line">        <span class="string">"DOWNLOADER_MIDDLEWARES"</span>: &#123;</span><br><span class="line">            <span class="comment"># 置于 HttpProxyMiddleware(750) 之前</span></span><br><span class="line">            <span class="string">'ArticleSpider.middlewares.RandomHttpProxyMiddleware'</span>: <span class="number">745</span>,</span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="comment"># 使用之前在 http://www.xicidaili.com/ 网站爬取到的代理</span></span><br><span class="line">        <span class="string">'HTTPPROXY_PROXY_LIST_FILE'</span>: <span class="string">'ArticleSpider/data/proxy_list.json'</span>,</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">start_requests</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">            <span class="keyword">yield</span> Request(<span class="string">'http://httpbin.org/ip'</span>, dont_filter=<span class="keyword">True</span>)</span><br><span class="line">            <span class="keyword">yield</span> Request(<span class="string">'https://httpbin.org/ip'</span>, dont_filter=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        print(json.loads(response.text))</span><br></pre></td></tr></table></figure><p>运行爬虫，观察输出：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">F:\VirtualEnv\jobboleArticle\ArticleSpider\ArticleSpider</span><br><span class="line">(jobboleArticle) $ scrapy crawl test_random_proxy</span><br><span class="line"><span class="number">2018</span>-<span class="number">04</span>-<span class="number">09</span> <span class="number">11</span>:<span class="number">20</span>:<span class="number">11</span> [scrapy<span class="selector-class">.core</span><span class="selector-class">.engine</span>] DEBUG: Crawled (<span class="number">200</span>) &lt;GET http:<span class="comment">//httpbin.org/ip&gt; (referer: None)</span></span><br><span class="line">&#123;<span class="string">'origin'</span>: <span class="string">'60.177.229.113'</span>&#125;</span><br><span class="line"><span class="number">2018</span>-<span class="number">04</span>-<span class="number">09</span> <span class="number">11</span>:<span class="number">20</span>:<span class="number">13</span> [scrapy<span class="selector-class">.core</span><span class="selector-class">.engine</span>] DEBUG: Crawled (<span class="number">200</span>) &lt;GET https:<span class="comment">//httpbin.org/ip&gt; (referer: None)</span></span><br><span class="line">&#123;<span class="string">'origin'</span>: <span class="string">'183.159.94.136'</span>&#125;</span><br><span class="line"><span class="number">2018</span>-<span class="number">04</span>-<span class="number">09</span> <span class="number">11</span>:<span class="number">20</span>:<span class="number">14</span> [scrapy<span class="selector-class">.core</span><span class="selector-class">.engine</span>] DEBUG: Crawled (<span class="number">200</span>) &lt;GET https:<span class="comment">//httpbin.org/ip&gt; (referer: None)</span></span><br><span class="line">&#123;<span class="string">'origin'</span>: <span class="string">'60.177.229.113'</span>&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h5 id=&quot;使用-HTTP-代理&quot;&gt;&lt;a href=&quot;#使用-HTTP-代理&quot; class=&quot;headerlink&quot; title=&quot;使用 HTTP 代理&quot;&gt;&lt;/a&gt;使用 HTTP 代理&lt;/h5&gt;&lt;p&gt;HTTP 代理服务器可以比作客户端与 Web 服务器（网站）之间的一个信息中转站，客户端发送的 HTTP 请求和 Web 服务器返回的 HTTP 响应通过代理服务器转发给对方，如图所示：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://dongfei.oss-cn-shanghai.aliyuncs.com/blog/proxy1.jpg&quot; alt=&quot;proxy1&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Python" scheme="http://blog.dongfei.xin/categories/Python/"/>
    
      <category term="Scrapy" scheme="http://blog.dongfei.xin/categories/Python/Scrapy/"/>
    
    
      <category term="Scrapy" scheme="http://blog.dongfei.xin/tags/Scrapy/"/>
    
  </entry>
  
  <entry>
    <title>精通 Scrapy 网络爬虫（数据库篇）</title>
    <link href="http://blog.dongfei.xin/2018-04-06/%E7%B2%BE%E9%80%9A-Scrapy-%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%EF%BC%88%E6%95%B0%E6%8D%AE%E5%BA%93%E7%AF%87%EF%BC%89/"/>
    <id>http://blog.dongfei.xin/2018-04-06/精通-Scrapy-网络爬虫（数据库篇）/</id>
    <published>2018-04-06T11:59:01.000Z</published>
    <updated>2018-04-12T11:22:53.840Z</updated>
    
    <content type="html"><![CDATA[<h5 id="数据保存至数据库"><a href="#数据保存至数据库" class="headerlink" title="数据保存至数据库"></a>数据保存至数据库</h5><p>以 <code>toscrape_book</code> 项目作为环境，使用 Item Pipeline 实现 Scrapy 爬虫，将爬取到的数据存储到数据库中。爬取网站 <a href="http://books.toscrape.com/" target="_blank" rel="noopener">http://books.toscrape.com/</a> 中的书籍信息，其中每一本书的信息包括：<br><code>书名</code>、<code>价格</code>、<code>评价等级</code>、<code>产品编码</code>、<code>库存量</code>、<code>评价数量</code>。<br><img src="https://dongfei.oss-cn-shanghai.aliyuncs.com/blog/toscrape1.jpg" alt="toscrape1"><br><a id="more"></a></p><p><code>books.py</code> 代码：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> scrapy.linkextractors <span class="keyword">import</span> LinkExtractor</span><br><span class="line"><span class="keyword">from</span> ArticleSpider.items <span class="keyword">import</span> BookItem</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BooksSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">'books'</span></span><br><span class="line">    allowed_domains = [<span class="string">'books.toscrape.com'</span>]</span><br><span class="line">    start_urls = [<span class="string">'http://books.toscrape.com/'</span>]</span><br><span class="line"></span><br><span class="line">    custom_settings = &#123;</span><br><span class="line">        <span class="comment"># 指定 csv 文件各列的次序</span></span><br><span class="line">        <span class="string">"FEED_EXPORT_FIELDS"</span>: [<span class="string">'upc'</span>, <span class="string">'name'</span>, <span class="string">'price'</span>, <span class="string">'stock'</span>, <span class="string">'review_rating'</span>, <span class="string">'review_num'</span>],</span><br><span class="line">        <span class="string">"ITEM_PIPELINES"</span>: &#123;</span><br><span class="line">            <span class="string">'ArticleSpider.pipelines.BookPipeline'</span>: <span class="number">300</span>,</span><br><span class="line">        &#125;,</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 书籍列表页面的解析函数</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        le = LinkExtractor(restrict_css=<span class="string">'article.product_pod h3'</span>)</span><br><span class="line">        <span class="keyword">for</span> link <span class="keyword">in</span> le.extract_links(response):</span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(link.url, callback=self.parse_book)</span><br><span class="line"></span><br><span class="line">        le = LinkExtractor(restrict_css=<span class="string">'ul.pager li.next'</span>)</span><br><span class="line">        links = le.extract_links(response)</span><br><span class="line">        <span class="keyword">if</span> links:</span><br><span class="line">            next_url = links[<span class="number">0</span>].url</span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(next_url, callback=self.parse)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 书籍页面的解析函数</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_book</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        book = BookItem()</span><br><span class="line">        sel = response.css(<span class="string">'div.product_main'</span>)</span><br><span class="line">        book[<span class="string">'name'</span>] = sel.xpath(<span class="string">'./h1/text()'</span>).extract_first()</span><br><span class="line">        book[<span class="string">'price'</span>] = sel.css(<span class="string">'p.price_color::text'</span>).extract_first()</span><br><span class="line">        book[<span class="string">'review_rating'</span>] = sel.css(<span class="string">'p.star-rating::attr(class)'</span>).re_first(<span class="string">'star-rating ([A-Za-z]+)'</span>)</span><br><span class="line"></span><br><span class="line">        sel = response.css(<span class="string">'table.table.table-striped'</span>)</span><br><span class="line">        book[<span class="string">'upc'</span>] = sel.xpath(<span class="string">'(.//tr)[1]/td/text()'</span>).extract_first()</span><br><span class="line">        book[<span class="string">'stock'</span>] = sel.xpath(<span class="string">'(.//tr)[last()-1]/td/text()'</span>).re_first(<span class="string">'\((\d+) available\)'</span>)</span><br><span class="line">        book[<span class="string">'review_num'</span>] = sel.xpath(<span class="string">'(.//tr)[last()]/td/text()'</span>).extract_first()</span><br><span class="line">        <span class="keyword">yield</span> book</span><br></pre></td></tr></table></figure></p><p><code>pipelines.py</code> 代码：<br><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 处理书籍评价等级</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BookPipeline</span>(<span class="title">object</span>):</span></span><br><span class="line">    review_rating_map = &#123;</span><br><span class="line">        <span class="string">'One'</span>: <span class="number">1</span>,</span><br><span class="line">        <span class="string">'Two'</span>: <span class="number">2</span>,</span><br><span class="line">        <span class="string">'Three'</span>: <span class="number">3</span>,</span><br><span class="line">        <span class="string">'Four'</span>: <span class="number">4</span>,</span><br><span class="line">        <span class="string">'Five'</span>: <span class="number">5</span>,</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(<span class="keyword">self</span>, item, spider)</span></span><span class="symbol">:</span></span><br><span class="line">        rating = item[<span class="string">'review_rating'</span>]</span><br><span class="line">        <span class="keyword">if</span> <span class="symbol">rating:</span></span><br><span class="line">            item[<span class="string">'review_rating'</span>] = <span class="keyword">self</span>.review_rating_map[rating]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure></p><p><code>items.py</code> 代码：<br><figure class="highlight monkey"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BookItem</span>(<span class="title">scrapy</span>.<span class="title">Item</span>):</span></span><br><span class="line">    name = scrapy.<span class="keyword">Field</span>()<span class="meta">   # 书名</span></span><br><span class="line">    price = scrapy.<span class="keyword">Field</span>()<span class="meta">  # 价格</span></span><br><span class="line">    review_rating = scrapy.<span class="keyword">Field</span>()<span class="meta">  # 评价等级，1~5星</span></span><br><span class="line">    review_num = scrapy.<span class="keyword">Field</span>()<span class="meta">     # 评价数量</span></span><br><span class="line">    upc = scrapy.<span class="keyword">Field</span>()<span class="meta">    # 产品编码</span></span><br><span class="line">    stock = scrapy.<span class="keyword">Field</span>()<span class="meta">  # 库存量</span></span><br></pre></td></tr></table></figure></p><p>运行命令，爬取数据：<br><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">F:\VirtualEnv\<span class="keyword">jobboleArticle\ArticleSpider\ArticleSpider</span></span><br><span class="line"><span class="keyword">(jobboleArticle) </span>$ <span class="keyword">scrapy </span>crawl <span class="keyword">books </span>-o .\data\<span class="keyword">books.toscrape.csv </span>--nolog</span><br><span class="line">(<span class="keyword">jobboleArticle) </span>$ cat -n .\data\<span class="keyword">books.toscrape.csv</span></span><br><span class="line"><span class="keyword">。。。。。。。。。。</span></span><br><span class="line"><span class="keyword">995 </span> e7469e<span class="number">22b</span><span class="number">5b</span>fb3e7,The Art of War,£<span class="number">33</span>.<span class="number">34</span>,<span class="number">15</span>,<span class="number">5</span>,<span class="number">0</span></span><br><span class="line"><span class="number">996</span>  dd047728de72ad62,The Artist<span class="string">'s Way: A Spiritual Path to Higher Creativity,£38.49,15,5,0</span></span><br><span class="line"><span class="string">997  efc3768127714ec3,The Bridge to Consciousness: I'</span>m Writing the <span class="keyword">Bridge </span><span class="keyword">Between </span><span class="keyword">Science </span><span class="keyword">and </span>Our Old <span class="keyword">and </span>New <span class="keyword">Beliefs.,£32.00,15,3,0</span></span><br><span class="line"><span class="keyword">998 </span> <span class="keyword">b12b89017878a60d,Private </span>Paris (Private <span class="comment">#10),£47.61,17,5,0</span></span><br><span class="line"><span class="number">999</span>  <span class="number">6</span>fd646a334e6e133,What<span class="string">'s It Like in Space?: Stories from Astronauts Who'</span>ve <span class="keyword">Been </span>There,£<span class="number">19</span>.<span class="number">60</span>,<span class="number">14</span>,<span class="number">2</span>,<span class="number">0</span></span><br><span class="line"><span class="number">1000</span>  c<span class="number">8f</span><span class="number">7f</span>0cb1abb9cac,Reasons to Stay Alive,£<span class="number">26</span>.<span class="number">41</span>,<span class="number">17</span>,<span class="number">2</span>,<span class="number">0</span></span><br><span class="line"><span class="number">1001</span>  <span class="keyword">b4fd5943413e089a,Slow </span>States of Collapse: Poems,£<span class="number">57</span>.<span class="number">31</span>,<span class="number">17</span>,<span class="number">3</span>,<span class="number">0</span></span><br></pre></td></tr></table></figure></p><h5 id="SQLite"><a href="#SQLite" class="headerlink" title="SQLite"></a>SQLite</h5><p>SQLite 是一个文件型轻量级数据库，它的处理速度很快，在数据量不是很大的情况<br>下，使用 SQLite 足够了。</p><p>首先，创建一个供 Scrapy 使用的 SQLite 数据库，取名为 scrapy.db，在客户端中创建数据表（Table）  ：</p><figure class="highlight lasso"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">F:\VirtualEnv\jobboleArticle\ArticleSpider\ArticleSpider\<span class="built_in">data</span></span><br><span class="line">(jobboleArticle) $ sqlite3 scrapy.db</span><br><span class="line">SQLite version <span class="number">3.20</span><span class="number">.1</span> <span class="number">2017</span><span class="number">-08</span><span class="number">-24</span> <span class="number">16</span>:<span class="number">21</span>:<span class="number">36</span></span><br><span class="line">Enter <span class="string">".help"</span> for usage hints.</span><br><span class="line">sqlite&gt; CREATE TABLE books(</span><br><span class="line">   <span class="params">...</span>&gt; upc             CHAR(<span class="number">16</span>) <span class="literal">NOT</span> <span class="built_in">NULL</span> PRIMARY KEY,</span><br><span class="line">   <span class="params">...</span>&gt; name            VARCHAR(<span class="number">256</span>) <span class="literal">NOT</span> <span class="built_in">NULL</span>,</span><br><span class="line">   <span class="params">...</span>&gt; price           VARCHAR(<span class="number">16</span>) <span class="literal">NOT</span> <span class="built_in">NULL</span>,</span><br><span class="line">   <span class="params">...</span>&gt; review_rating   INT,</span><br><span class="line">   <span class="params">...</span>&gt; review_num      INT,</span><br><span class="line">   <span class="params">...</span>&gt; stock           INT</span><br><span class="line">   <span class="params">...</span>&gt; );</span><br><span class="line">sqlite&gt;</span><br></pre></td></tr></table></figure><p>可以使用 Pycharm 查看创建的数据表：</p><p><img src="https://dongfei.oss-cn-shanghai.aliyuncs.com/blog/toscrape2.jpg" alt="toscrape2"></p><p>在 Python 中访问 SQLite 数据库可使用 Python 标准库中的 sqlite3 模块。下面是使用<br>sqlite3 模块将数据写入 SQLite 数据库的简单示例：</p><figure class="highlight vala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># -*- coding:utf-8 -*-</span></span><br><span class="line">__author__ = <span class="string">"東飛"</span></span><br><span class="line">__date__ = <span class="string">"2018-4-7"</span></span><br><span class="line"></span><br><span class="line">import sqlite3</span><br><span class="line"></span><br><span class="line"><span class="meta"># 连接数据库，得到 Connection 对象</span></span><br><span class="line">conn = sqlite3.connect(<span class="string">'../ArticleSpider/data/scrapy.db'</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta"># 创建 Curosr 对象，用来执行 SQL 语句</span></span><br><span class="line">cur = conn.cursor()</span><br><span class="line"></span><br><span class="line"><span class="meta"># 创建数据表</span></span><br><span class="line">cur.execute(<span class="string">'CREATE TABLE person (name VARCHAR(32),age INT,sex char(1))'</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta"># 插入一条数据</span></span><br><span class="line">cur.execute(<span class="string">'INSERT INTO person VALUES (?,?,?)'</span>, (<span class="string">'那小子真帅'</span>, <span class="number">22</span>, <span class="string">'M'</span>))</span><br><span class="line"></span><br><span class="line"><span class="meta"># 保存变更，commit 后数据才被实际写入数据库</span></span><br><span class="line">conn.commit()</span><br><span class="line"></span><br><span class="line"><span class="meta"># 关闭连接</span></span><br><span class="line">conn.close()</span><br></pre></td></tr></table></figure><p>运行上述代码后，查看数据库中数据是否以更新。</p><p><img src="https://dongfei.oss-cn-shanghai.aliyuncs.com/blog/toscrape3.jpg" alt="toscrape3"></p><p>了解了在 Python 中如何操作 SQLite 数据库后，按下来编写一个能将爬取到的数据写入 SQLite 数据库的 Item Pipeline 。在 <code>pipelines.py</code> 中实现 SQLitePipeline 的代码如下：<br><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将数据保存至 SQLite</span></span><br><span class="line">import sqlite3</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SQLitePipeline</span>(<span class="title">object</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open_spider</span><span class="params">(<span class="keyword">self</span>, spider)</span></span><span class="symbol">:</span></span><br><span class="line">        db_name = spider.custom_settings.get(<span class="string">'SQLITE_DB_NAME'</span>, <span class="string">'scrapy_default.db'</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">self</span>.db_conn = sqlite3.connect(db_name)</span><br><span class="line">        <span class="keyword">self</span>.db_cur = <span class="keyword">self</span>.db_conn.cursor()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span><span class="params">(<span class="keyword">self</span>, spider)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">self</span>.db_conn.commit()</span><br><span class="line">        <span class="keyword">self</span>.db_conn.close()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(<span class="keyword">self</span>, item, spider)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">self</span>.insert_db(item)</span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">insert_db</span><span class="params">(<span class="keyword">self</span>, item)</span></span><span class="symbol">:</span></span><br><span class="line">        values = (</span><br><span class="line">            item[<span class="string">'upc'</span>],</span><br><span class="line">            item[<span class="string">'name'</span>],</span><br><span class="line">            item[<span class="string">'price'</span>],</span><br><span class="line">            item[<span class="string">'review_rating'</span>],</span><br><span class="line">            item[<span class="string">'review_num'</span>],</span><br><span class="line">            item[<span class="string">'stock'</span>],</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        sql = <span class="string">'INSERT INTO books VALUES (?,?,?,?,?,?)'</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">self</span>.db_cur.execute(sql, values)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 每插入一条就 commit 一次会影响效率</span></span><br><span class="line">        <span class="comment"># self.db_conn.commit()</span></span><br></pre></td></tr></table></figure></p><p>解释上述代码如下：</p><ul><li>open_spider 方法</li></ul><p>在开始爬取数据之前被调用，在该方法中通过 spider.custom_settings 对象读取用户在配置中设定的数据库，然后建立与数据库的连接，将得到的 Connection 时象和 Cursor 对象分別赋值给 self.db_conn 和 self.db_cur，以便之后使用。</p><ul><li>process_item 方法</li></ul><p>处理爬取到的每一项数据，在该方法中调用 insert_db 方法，执行插入数据操作的 SQL 语句。但需要注意的是，在 insert_db 中并没有调用连接对象的 commit 方法，也就意味着此时数据并没有实际写入数据库。如果每插入一条数据都调用一次 commit 方法，会严重降低程序执行效率，并且我们对数据插入数据库的实时性并没有什么要求，因此可以在爬取完全部数据后再调用 commit 方法。</p><ul><li>close_spider 方法</li></ul><p>在爬取完全部数据后被调用，在该方法中，调用连接对象的 commit 方法将之前所有的插入数据操作一次性提交给数据库，然后关闭连接对象</p><p>在配置文件 <code>settings.py</code> 或者 <code>custom_settings</code> 中指定我们所要使用的 SQLite 数据库，并启用 SQLitePipeline ：</p><figure class="highlight sml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"ITEM_PIPELINES"</span>: &#123;</span><br><span class="line">            <span class="symbol">'ArticleSpider</span>.pipelines.<span class="type">SQLitePipeline'</span>: <span class="number">400</span>,</span><br><span class="line">        &#125;,</span><br><span class="line"><span class="string">"SQLITE_DB_NAME"</span>:<span class="symbol">'ArticleSpider</span>/data/scrapy.db',</span><br></pre></td></tr></table></figure><p>运行爬虫，并查看数据库:</p><figure class="highlight gherkin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">F:\VirtualEnv\jobboleArticle\ArticleSpider\ArticleSpider</span><br><span class="line">(jobboleArticle) $ sqlite3 .\data\scrapy.db</span><br><span class="line">SQLite version 3.20.1 2017-08-24 16:21:36</span><br><span class="line">Enter <span class="string">".help"</span> for usage hints.</span><br><span class="line">sqlite&gt; select count(<span class="symbol">*</span>) from books;</span><br><span class="line">1000</span><br><span class="line">sqlite&gt; select <span class="symbol">*</span> from books;</span><br><span class="line">9528d0948525bf5f|<span class="string">Birdsong: A Story in Pictures</span>|<span class="string">￡54.64</span>|<span class="string">3</span>|<span class="string">0</span>|19</span><br><span class="line">55f9da0c5eea2e10|<span class="string">You can't bury them all: Poems</span>|<span class="string">￡33.63</span>|<span class="string">2</span>|<span class="string">0</span>|17</span><br><span class="line">be5cc846f45496fb|<span class="string">Behind Closed Doors</span>|<span class="string">￡52.22</span>|<span class="string">4</span>|<span class="string">0</span>|18</span><br><span class="line">19ed25f4641d5efd|<span class="string">In a Dark, Dark Wood</span>|<span class="string">￡19.63</span>|<span class="string">1</span>|<span class="string">0</span>|18</span><br><span class="line">094b269567e1c300|<span class="string">Maude (1883-1993):She Grew Up with the country</span>|<span class="string">￡18.02</span>|<span class="string">2</span>|<span class="string">0</span>|18</span><br><span class="line">6be3beb0793a53e7|<span class="string">Sophie's World</span>|<span class="string">￡15.94</span>|<span class="string">5</span>|<span class="string">0</span>|18</span><br><span class="line">。。。。。。。。。</span><br></pre></td></tr></table></figure><p><img src="https://dongfei.oss-cn-shanghai.aliyuncs.com/blog/toscrape4.jpg" alt="toscrape4"></p><h5 id="MySQL"><a href="#MySQL" class="headerlink" title="MySQL"></a>MySQL</h5><p>MySQL 是一个应用极其广泛的关系型数据库，它是开源免费的，可以支持大型数据库。<br>使客户端登录 MySQL ，创建一个供 Scrapy 使用的数据库，取名为 scrapy.db ：<br><figure class="highlight oxygene"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">F:\VirtualEnv\jobboleArticle\ArticleSpider\ArticleSpider</span><br><span class="line">(jobboleArticle) $ mysql -hlocalhost -uroot -proot -P3308</span><br><span class="line">Welcome <span class="keyword">to</span> the MySQL monitor.  Commands <span class="keyword">end</span> <span class="keyword">with</span> ; <span class="keyword">or</span> \g.</span><br><span class="line">Your MySQL connection id <span class="keyword">is</span> <span class="number">3</span></span><br><span class="line">Server version: <span class="number">5.7</span>.<span class="number">19</span> MySQL Community Server (GPL)</span><br><span class="line"></span><br><span class="line">Copyright (c) <span class="number">2000</span>, <span class="number">2017</span>, Oracle <span class="keyword">and</span>/<span class="keyword">or</span> its affiliates. All rights reserved.</span><br><span class="line"></span><br><span class="line">Oracle <span class="keyword">is</span> a registered trademark <span class="keyword">of</span> Oracle Corporation <span class="keyword">and</span>/<span class="keyword">or</span> its</span><br><span class="line">affiliates. Other names may be trademarks <span class="keyword">of</span> their respective</span><br><span class="line">owners.</span><br><span class="line"></span><br><span class="line"><span class="keyword">Type</span> <span class="string">'help;'</span> <span class="keyword">or</span> <span class="string">'\h'</span> <span class="keyword">for</span> help. <span class="keyword">Type</span> <span class="string">'\c'</span> <span class="keyword">to</span> clear the current input statement.</span><br><span class="line">mysql&gt; <span class="keyword">CREATE</span> DATABASE scrapy_db CHARACTER <span class="keyword">SET</span> <span class="string">'utf8'</span> COLLATE <span class="string">'utf8_general_ci'</span>;</span><br><span class="line">Query OK, <span class="number">1</span> row affected (<span class="number">0.00</span> sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; USE scrapy_db;</span><br><span class="line">Database changed</span><br></pre></td></tr></table></figure></p><p>接下来，创建存储书籍数据的表：<br><figure class="highlight haml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; CREATE TABLE books(</span><br><span class="line">    -<span class="ruby">&gt; upc             CHAR(<span class="number">16</span>) NOT NULL PRIMARY KEY,</span></span><br><span class="line"><span class="ruby">    -&gt; name            VARCHAR(<span class="number">256</span>) NOT NULL,</span></span><br><span class="line"><span class="ruby">    -&gt; price           VARCHAR(<span class="number">16</span>) NOT NULL,</span></span><br><span class="line"><span class="ruby">    -&gt; review_rating   INT,</span></span><br><span class="line"><span class="ruby">    -&gt; review_num      INT,</span></span><br><span class="line"><span class="ruby">    -&gt; stock           INT</span></span><br><span class="line"><span class="ruby">    -&gt; );</span></span><br><span class="line"><span class="ruby">Query OK, <span class="number">0</span> rows affected (<span class="number">0</span>.<span class="number">01</span> sec)</span></span><br><span class="line"><span class="ruby"></span></span><br><span class="line"><span class="ruby">mysql&gt;</span></span><br></pre></td></tr></table></figure></p><p>在 <strong>Python 2</strong> 中访问 MySQL 数据库可以使用第三方库 MySQL-Python （即 MySQLdb ），但是 MySQLdb 不支持 <strong>Python 3</strong> 。在 <strong>Python 3</strong> 中，可以使用另一个第三方库 mysqlclient 作为替代，它是基于 MySQL-Python 开发的，提供了几乎完全相同的接口。在两个 Python 版本下，可以使用相同的代码访问 MySQL 。</p><p><strong>Python 2</strong> 使用 pip 安装 MySQL-python ：<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip <span class="keyword">install</span> My <span class="keyword">SQL</span>-python</span><br></pre></td></tr></table></figure></p><p><strong>Python 3</strong> 使用 pip 安装 mysqlclient ：<br><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip <span class="keyword">install</span> mysqlclient</span><br></pre></td></tr></table></figure></p><p>下面面是使用 MySQLdb 将数据写入 MySQL 数据库的简单示例，与 sqlite3的使用<br>乎完全相同 ：</p><figure class="highlight vala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># -*- coding:utf-8 -*-</span></span><br><span class="line">__author__ = <span class="string">"東飛"</span></span><br><span class="line">__date__ = <span class="string">"2018-4-7"</span></span><br><span class="line"></span><br><span class="line">import MySQLdb</span><br><span class="line"></span><br><span class="line"><span class="meta"># 连接数据库，得到 Connection 对象</span></span><br><span class="line">conn = MySQLdb.connect(host=<span class="string">'localhost'</span>, db=<span class="string">'scrapy_db'</span>, user=<span class="string">'root'</span>, passwd=<span class="string">'root'</span>, port=<span class="number">3308</span>, charset=<span class="string">'utf8'</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta"># 创建 Curosr 对象，用来执行 SQL 语句</span></span><br><span class="line">cur = conn.cursor()</span><br><span class="line"></span><br><span class="line"><span class="meta"># 创建数据表</span></span><br><span class="line">cur.execute(<span class="string">'CREATE TABLE person (name VARCHAR(32),age INT,sex char(1)) ENGINE=InnoDB DEFAULT CHARSET=utf8'</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta"># 插入一条数据</span></span><br><span class="line">cur.execute(<span class="string">'INSERT INTO person VALUES (%s,%s,%s)'</span>, (<span class="string">'那小子真帅'</span>, <span class="number">21</span>, <span class="string">'M'</span>))</span><br><span class="line"></span><br><span class="line"><span class="meta"># 保存变更，commit 后数据才被实际写入数据库</span></span><br><span class="line">conn.commit()</span><br><span class="line"></span><br><span class="line"><span class="meta"># 关闭连接</span></span><br><span class="line">conn.close()</span><br></pre></td></tr></table></figure><p>使用 Pycharm 查看创建的数据表：</p><p><img src="https://dongfei.oss-cn-shanghai.aliyuncs.com/blog/toscrape5.jpg" alt="toscrape5"></p><p>仿照 SQLitePipeline 实现 MysqlBookPipeline ，代码如下：</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数据保存至 Mysql 数据库</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MysqlBookPipeline</span>(<span class="title">object</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open_spider</span><span class="params">(<span class="keyword">self</span>, spider)</span></span><span class="symbol">:</span></span><br><span class="line">        db = spider.custom_settings.get(<span class="string">'SQLITE_DB_NAME'</span>, <span class="string">'scrapy_default'</span>)</span><br><span class="line">        host = spider.custom_settings.get(<span class="string">"MYSQL_HOST"</span>, <span class="string">"localhost"</span>)</span><br><span class="line">        port = spider.custom_settings.get(<span class="string">"MYSQL_PORT"</span>, <span class="number">3306</span>)</span><br><span class="line">        user = spider.custom_settings.get(<span class="string">"MYSQL_USER"</span>, <span class="string">"root"</span>)</span><br><span class="line">        passwd = spider.custom_settings.get(<span class="string">"MYSQL_PASSWORD"</span>, <span class="string">"root"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">self</span>.db_conn = MySQLdb.connect(host=host, port=port, db=db,</span><br><span class="line">                                       user=user, passwd=passwd, charset=<span class="string">'utf8'</span>)</span><br><span class="line">        <span class="keyword">self</span>.db_cur = <span class="keyword">self</span>.db_conn.cursor()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span><span class="params">(<span class="keyword">self</span>, spider)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">self</span>.db_conn.commit()</span><br><span class="line">        <span class="keyword">self</span>.db_conn.close()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(<span class="keyword">self</span>, item, spider)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">self</span>.insert_db(item)</span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">insert_db</span><span class="params">(<span class="keyword">self</span>, item)</span></span><span class="symbol">:</span></span><br><span class="line">        values = (</span><br><span class="line">            item[<span class="string">'upc'</span>],</span><br><span class="line">            item[<span class="string">'name'</span>],</span><br><span class="line">            item[<span class="string">'price'</span>],</span><br><span class="line">            item[<span class="string">'review_rating'</span>],</span><br><span class="line">            item[<span class="string">'review_num'</span>],</span><br><span class="line">            item[<span class="string">'stock'</span>],</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        sql = <span class="string">'INSERT INTO books VALUES (%s,%s,%s,%s,%s,%s)'</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">self</span>.db_cur.execute(sql, values)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 每插入一条就 commit 一次会影响效率</span></span><br><span class="line">        <span class="comment"># self.db_conn.commit()</span></span><br></pre></td></tr></table></figure><p>上述代码结构与 SQLitePipeline 完全相同，不再赘述。<br>在配置文件 <code>settings.py</code> 或者 <code>custom_settings</code> 中指定我们所要使用的 MySQL 数据库，并启用 MysqlBookPipeline ：</p><figure class="highlight xquery"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">custom_settings = &#123;</span><br><span class="line">        <span class="string">"ITEM_PIPELINES"</span>: &#123;</span><br><span class="line">            <span class="string">'ArticleSpider.pipelines.BookPipeline'</span>: <span class="number">300</span>,</span><br><span class="line">            <span class="string">'ArticleSpider.pipelines.MysqlBookPipeline'</span>: <span class="number">400</span>,</span><br><span class="line">        &#125;, </span><br><span class="line">        <span class="string">"MYSQL_DB_NAME"</span>: <span class="string">'scrapy_db'</span>,</span><br><span class="line">        <span class="string">"MYSQL_HOST"</span>: <span class="string">"localhost"</span>,</span><br><span class="line">        <span class="string">"MYSQL_PORT"</span>: <span class="number">3308</span>,</span><br><span class="line">        <span class="string">"MYSQL_USER"</span>: <span class="string">"root"</span>,</span><br><span class="line">        <span class="string">"MYSQL_PASSWORD"</span>: <span class="string">"root"</span>,</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>运行爬虫，并查看数据库：<br><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">F:\VirtualEnv\jobboleArticle\ArticleSpider\ArticleSpider</span><br><span class="line">(jobboleArticle) $ scrapy crawl books</span><br><span class="line">。。。。。。。。。。 </span><br><span class="line">(jobboleArticle) $ mysql -hlocalhost -uroot -proot -P3308 scrapy<span class="emphasis">_db</span></span><br><span class="line"><span class="emphasis">Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.</span></span><br><span class="line"><span class="emphasis"></span></span><br><span class="line"><span class="emphasis">mysql&gt; select count(*) from books;</span></span><br><span class="line"><span class="emphasis">+----------+</span></span><br><span class="line"><span class="emphasis">| count(*) |</span></span><br><span class="line"><span class="emphasis">+----------+</span></span><br><span class="line"><span class="emphasis">|     1000 |</span></span><br><span class="line"><span class="emphasis">+----------+</span></span><br><span class="line"><span class="emphasis">1 row in set (0.00 sec)</span></span><br><span class="line"><span class="emphasis"></span></span><br><span class="line"><span class="emphasis">mysql&gt; select name from books;</span></span><br><span class="line"><span class="emphasis">| name</span></span><br><span class="line"><span class="emphasis">                                                                        |</span></span><br><span class="line"><span class="emphasis">+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+</span></span><br><span class="line"><span class="emphasis">| A Light in the Attic</span></span><br><span class="line"><span class="emphasis">                                                                        |</span></span><br><span class="line"><span class="emphasis">| Our Band Could Be Your Life: Scenes from the American Indie Underground, 1981-1991</span></span><br><span class="line"><span class="emphasis">                                                                        |</span></span><br><span class="line"><span class="emphasis">| It's Only the Himalayas</span></span><br><span class="line"><span class="emphasis">                                                                        |</span></span><br><span class="line"><span class="emphasis">| Scott Pilgrim's Precious Little Life (Scott Pilgrim #1)</span></span><br><span class="line"><span class="emphasis">                                                                        |</span></span><br><span class="line"><span class="emphasis">| Mesaerion: The Best Science Fiction Stories 1800-1849</span></span><br><span class="line"><span class="emphasis">                                                                        |</span></span><br><span class="line"><span class="emphasis">| Rip it Up and Start Again</span></span><br><span class="line"><span class="emphasis">                                                                        |</span></span><br><span class="line"><span class="emphasis">| Libertarianism for Beginners</span></span><br><span class="line"><span class="emphasis">                                                                        |</span></span><br><span class="line"><span class="emphasis">| Olio</span></span><br></pre></td></tr></table></figure></p><p>上述代码中，同样是先执行完全部的插入语句 <code>INSERT INTO</code>，最后一次性调用 commit 方法提交给数据库。或许在某些情况下，我们的确需要每执行一条插入语句，就立即调用 commit 方法更新数据库，如爬取过程很长，中途可能被迫中断，这样程序就不能执行到最后的 commit 。如果在上述代码的 <code>insert_db</code> 方法中直接添加 <code>self.db_conn.commit()</code> 又会使程序执行变慢。为解决以上难题，下面讲解另一种实现方法。</p><p>Scrapy 框架自身是使用另一个 Python 框架 Twisted 编写的程序，Twisted 是一个事件驱动型的异步网络框架，鼓励用户编写异步代码，Twisted 中提供了以异步方式多线程访问数据库的模块 adbapi，使用该模块可以显著提高程序访问数据库的效率，下面是使用 adbpi 中的连接池访问 MySQL 数据库的简单示例 ：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line">__author__ = <span class="string">"東飛"</span></span><br><span class="line">__date__ = <span class="string">"2018-4-7"</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> twisted.internet <span class="keyword">import</span> reactor, defer</span><br><span class="line"><span class="keyword">from</span> twisted.enterprise <span class="keyword">import</span> adbapi</span><br><span class="line"><span class="keyword">import</span> threading</span><br><span class="line"></span><br><span class="line">dbpool = adbapi.ConnectionPool(<span class="string">'MySQLdb'</span>, host=<span class="string">'localhost'</span>, database=<span class="string">'scrapy_db'</span>, user=<span class="string">'root'</span>, passwd=<span class="string">'root'</span>, port=<span class="number">3308</span>,</span><br><span class="line">                               charset=<span class="string">'utf8'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">insert_db</span><span class="params">(tx, item)</span>:</span></span><br><span class="line">    print(<span class="string">'In Thread: '</span>, threading.get_ident())</span><br><span class="line">    sql = <span class="string">'INSERT INTO person VALUES (%s,%s,%s)'</span></span><br><span class="line">    tx.execute(sql, item)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">    item = (<span class="string">'person%s'</span> % i, <span class="number">25</span>, <span class="string">'M'</span>)</span><br><span class="line">    dbpool.runInteraction(insert_db, item)</span><br><span class="line"></span><br><span class="line">reactor.run()</span><br></pre></td></tr></table></figure><p>上述代码解释如下：</p><ul><li>adbapi.ConnectionPool 方法</li></ul><p>可以创建一个数据库连接池对象，其中包含多个连接对象，每个连接对象在独立的线程中工作。adbapi 只是提供了异步访问数据库的编程框架，在其内部依然使用 MySQLdb、sqlite3 这样的库访问数据库，ConnectionPool 方法的第一个参数就是用来指定使用哪个库访问数据库，其他参数在创建连接对象时使用。</p><ul><li>dbpool.runInteraction(insert_db, item)</li></ul><p>以异步方式调用 instert_db 函數，dbpool 会选择连接池中的一个连接对象在独立线程中调用 insert_db ，其中参数 item 会被传给 insert_db 的第二个参数，传给 insert_db 的第一个参数是一个 Transaction 对象，其接口与 Cursor 对象类似，可以调用 execute 方法执行 SQL 语句，insert_db 执行完后，连接对象会自动调用 commit 方法。 </p><p>了解了 adbapi 的使用后，实现 MysqlBookAsyncPipeline，代码如下:<br><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数据异步存储至 Mysql 数据库</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MysqlBookAsyncPipeline</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open_spider</span><span class="params">(<span class="keyword">self</span>, spider)</span></span><span class="symbol">:</span></span><br><span class="line">        db = spider.custom_settings.get(<span class="string">'MYSQL_DB_NAME'</span>, <span class="string">'scrapy_default'</span>)</span><br><span class="line">        host = spider.custom_settings.get(<span class="string">"MYSQL_HOST"</span>, <span class="string">"localhost"</span>)</span><br><span class="line">        port = spider.custom_settings.get(<span class="string">"MYSQL_PORT"</span>, <span class="number">3306</span>)</span><br><span class="line">        user = spider.custom_settings.get(<span class="string">"MYSQL_USER"</span>, <span class="string">"root"</span>)</span><br><span class="line">        passwd = spider.custom_settings.get(<span class="string">"MYSQL_PASSWORD"</span>, <span class="string">"root"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">self</span>.dbpool = adbapi.ConnectionPool(<span class="string">'MySQLdb'</span>, host=host, port=port, db=db,</span><br><span class="line">                                            user=user, passwd=passwd, charset=<span class="string">'utf8'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span><span class="params">(<span class="keyword">self</span>, spider)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">self</span>.dbpool.close()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(<span class="keyword">self</span>, item, spider)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">self</span>.dbpool.runInteraction(<span class="keyword">self</span>.insert_db, item)</span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">insert_db</span><span class="params">(<span class="keyword">self</span>, tx,item)</span></span><span class="symbol">:</span></span><br><span class="line">        values = (</span><br><span class="line">            item[<span class="string">'upc'</span>],</span><br><span class="line">            item[<span class="string">'name'</span>],</span><br><span class="line">            item[<span class="string">'price'</span>],</span><br><span class="line">            item[<span class="string">'review_rating'</span>],</span><br><span class="line">            item[<span class="string">'review_num'</span>],</span><br><span class="line">            item[<span class="string">'stock'</span>],</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        sql = <span class="string">'INSERT INTO books VALUES (%s,%s,%s,%s,%s,%s)'</span></span><br><span class="line"></span><br><span class="line">        tx.execute(sql, values)</span><br></pre></td></tr></table></figure></p><h5 id="MongoDB"><a href="#MongoDB" class="headerlink" title="MongoDB"></a>MongoDB</h5><p>MongoDB 是一个面向文档的非关系型数据库（ NoSQL ），功能强大、灵活、易于拓展。<br><a href="https://blog.wuwii.com/install-mongodb.html" target="_blank" rel="noopener">安装 MongoDB 数据库</a> </p><p>启动 MongoDB 服务 ：<code>net start mongodb</code></p><p>在 Python 中可以使用第三方库 pymongo 访问 MongoDB 数据库，使用 pip 安装 pymongo ： <code>pip install pymongo</code> </p><p>下面是使用 pymongo 将数据写入 MongoDB 数据库的简单示例:</p><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line">__author__ = <span class="string">"東飛"</span></span><br><span class="line">__date__ = <span class="string">"2018-4-7"</span></span><br><span class="line"></span><br><span class="line">from pymongo import MongoClient</span><br><span class="line"></span><br><span class="line"><span class="comment"># 连接 MongoDB，得到一个客户端对象</span></span><br><span class="line"></span><br><span class="line">client = MongoClient('mongodb://localhost:27017')</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取名为 scrapy_db 的数据库的对象</span></span><br><span class="line">db = client.scrapy_db</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取名为 person 的集合的对象</span></span><br><span class="line">collection = db.person</span><br><span class="line"></span><br><span class="line">doc = &#123;</span><br><span class="line">    'name': '那小子真帅',</span><br><span class="line">    'age': 21,</span><br><span class="line">    'sex': 'M',</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将文档插入集合</span></span><br><span class="line">collection.insert_one(doc)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 关闭客户端</span></span><br><span class="line">client.close()</span><br></pre></td></tr></table></figure><p>Pycharm 安装 MongoDB 插件 ：</p><p>参照：<a href="https://blog.csdn.net/qq_24189933/article/details/75664743" target="_blank" rel="noopener">Pycharm 配置可视化 Mongodb 工具</a></p><p><img src="https://dongfei.oss-cn-shanghai.aliyuncs.com/blog/toscrape6.jpg" alt="toscrape6"></p><p>连接 MongoDB 数据库 ：</p><p><img src="https://dongfei.oss-cn-shanghai.aliyuncs.com/blog/toscrape7.jpg" alt="toscrape7"></p><p>查看数据 ：</p><p><img src="https://dongfei.oss-cn-shanghai.aliyuncs.com/blog/toscrape8.jpg" alt="toscrape8"></p><p>实现 MongoDBPipeline 代码如下：</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数据存储至 MongoDB 数据库</span></span><br><span class="line">from pymongo import MongoClient</span><br><span class="line">from scrapy import Item</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MongoDBPipeline</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open_spider</span><span class="params">(<span class="keyword">self</span>, spider)</span></span><span class="symbol">:</span></span><br><span class="line">        db_url = spider.custom_settings.get(<span class="string">'MONGODB_URL'</span>, <span class="string">'mongodb://localhost:27017'</span>)</span><br><span class="line">        db_name = spider.custom_settings.get(<span class="string">"MONGODB_DB_NAME"</span>, <span class="string">"scrapy_default"</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">self</span>.db_client = MongoClient(db_url)</span><br><span class="line">        <span class="keyword">self</span>.db = <span class="keyword">self</span>.db_client[db_name]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span><span class="params">(<span class="keyword">self</span>, spider)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">self</span>.db_client.close()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(<span class="keyword">self</span>, item, spider)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">self</span>.insert_db(item)</span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">insert_db</span><span class="params">(<span class="keyword">self</span>, item)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">if</span> isinstance(item,Item)<span class="symbol">:</span></span><br><span class="line">            item = dict(item)</span><br><span class="line">            </span><br><span class="line">        <span class="keyword">self</span>.db.books.insert_one(item)</span><br></pre></td></tr></table></figure><p>解释上述代码如下：</p><ul><li>open_spider 方法</li></ul><p>在开始爬取数据之前被调用，在该方法中通过 spider.custom_settings 或者 spider.settings 对象读取用户在配置文件中指定的数据库，然后建立与数据库的连接，将得到的 MongoClient 对象和 Database 对象分别赋值给 self.db_client 和 self.db，以便之后使用。</p><ul><li>process_item 方法</li></ul><p>处理爬取到的每一项数据，在该方法中调用 insert_db 方法，执行数据库的插入操作，在 insert_db 方法中，先将一项数据转换成字典，然后调用 insert_one 方法将其插入集合 books 。</p><ul><li>close spider 方法</li></ul><p>在爬取完全部数据后被调用，在该方法中关闭与数据库的连接。</p><p>在配置文件 <code>settings.py</code> 或者 <code>custom_settings</code> 中指定所要使用的 MongoDB 数据库，并启用 MongoDBPipeline ：</p><figure class="highlight xquery"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">custom_settings = &#123;</span><br><span class="line">        <span class="string">"ITEM_PIPELINES"</span>: &#123;</span><br><span class="line">            <span class="string">'ArticleSpider.pipelines.BookPipeline'</span>: <span class="number">300</span>,</span><br><span class="line">            <span class="string">'ArticleSpider.pipelines.MongoDBPipeline'</span>: <span class="number">403</span>,</span><br><span class="line">        &#125;, </span><br><span class="line">        <span class="string">'MONGODB_URL'</span>: <span class="string">'mongodb://localhost:27017'</span>,</span><br><span class="line">        <span class="string">'MONGODB_DB_NAME'</span>: <span class="string">'scrapy_db'</span>,</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>运行爬虫，井查看数据库 ：<br><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">F:\VirtualEnv\jobboleArticle\ArticleSpider\ArticleSpider</span><br><span class="line">(jobboleArticle) $ scrapy crawl books</span><br><span class="line"></span><br><span class="line">。。。。。。。。。。</span><br><span class="line"></span><br><span class="line">(jobboleArticle) $ mongo scrapy_db</span><br><span class="line">MongoDB <span class="keyword">shell</span> <span class="keyword">version</span> v3.<span class="number">4.9</span></span><br><span class="line">connecting <span class="keyword">to</span>: mongod<span class="variable">b:</span>//<span class="number">127.0</span>.<span class="number">0.1</span>:<span class="number">27017</span>/scrapy_db</span><br><span class="line">MongoDB server <span class="keyword">version</span>: <span class="number">3.4</span>.<span class="number">9</span></span><br><span class="line">Server <span class="built_in">has</span> startup warning<span class="variable">s:</span></span><br><span class="line"><span class="number">2018</span>-<span class="number">04</span>-<span class="number">07</span>T19:<span class="number">34</span>:<span class="number">13.576</span>+<span class="number">0800</span> I CONTROL  [initandlisten]</span><br><span class="line"><span class="number">2018</span>-<span class="number">04</span>-<span class="number">07</span>T19:<span class="number">34</span>:<span class="number">13.576</span>+<span class="number">0800</span> I CONTROL  [initandlisten] ** WARNING: Access control <span class="keyword">is</span> not enabled <span class="keyword">for</span> the database.</span><br><span class="line"><span class="number">2018</span>-<span class="number">04</span>-<span class="number">07</span>T19:<span class="number">34</span>:<span class="number">13.576</span>+<span class="number">0800</span> I CONTROL  [initandlisten] **          Read <span class="built_in">and</span> <span class="keyword">write</span> access <span class="keyword">to</span> data <span class="built_in">and</span> configuration <span class="keyword">is</span> unrestricted.</span><br><span class="line"><span class="number">2018</span>-<span class="number">04</span>-<span class="number">07</span>T19:<span class="number">34</span>:<span class="number">13.576</span>+<span class="number">0800</span> I CONTROL  [initandlisten]</span><br><span class="line">&gt; db.books.<span class="built_in">count</span>()</span><br><span class="line"><span class="number">1000</span></span><br><span class="line">&gt; db.books.<span class="keyword">find</span>()</span><br><span class="line">&#123; <span class="string">"_id"</span> : ObjectId(<span class="string">"5ac8ba021e885105bc184e6e"</span>), <span class="string">"name"</span> : <span class="string">"Tipping the Velvet"</span>, <span class="string">"price"</span> : <span class="string">"£53.74"</span>, <span class="string">"review_rating"</span> : <span class="number">1</span>, <span class="string">"upc"</span> : <span class="string">"90fa61229261140a"</span>, <span class="string">"stock"</span> : <span class="string">"20"</span>, <span class="string">"review_num"</span> : <span class="string">"0"</span> &#125;</span><br><span class="line">&#123; <span class="string">"_id"</span> : ObjectId(<span class="string">"5ac8ba021e885105bc184e6f"</span>), <span class="string">"name"</span> : <span class="string">"Sapiens: A Brief History of Humankind"</span>, <span class="string">"price"</span> : <span class="string">"£54.23"</span>, <span class="string">"review_rating"</span> : <span class="number">5</span>, <span class="string">"upc"</span> : <span class="string">"4165285e1663650f"</span>, <span class="string">"stock"</span> : <span class="string">"20"</span>, <span class="string">"review_num"</span> : <span class="string">"0"</span> &#125;</span><br><span class="line">&#123; <span class="string">"_id"</span> : ObjectId(<span class="string">"5ac8ba021e885105bc184e70"</span>), <span class="string">"name"</span> : <span class="string">"The Dirty Little Secrets of Getting Your Dream Job"</span>, <span class="string">"price"</span> : <span class="string">"£33.34"</span>, <span class="string">"review_rating"</span> : <span class="number">4</span>, <span class="string">"upc"</span> : <span class="string">"2597b5a345f45e1b"</span>, <span class="string">"stock"</span> : <span class="string">"19"</span>, <span class="string">"review_num"</span> : <span class="string">"0"</span> &#125;</span><br><span class="line">&#123; <span class="string">"_id"</span> : ObjectId(<span class="string">"5ac8ba021e885105bc184e71"</span>), <span class="string">"name"</span> : <span class="string">"The Requiem Red"</span>, <span class="string">"price"</span> : <span class="string">"£22.65"</span>, <span class="string">"review_rating"</span> : <span class="number">1</span>, <span class="string">"upc"</span> : <span class="string">"f77dbf2323deb740"</span>, <span class="string">"stock"</span> : <span class="string">"19"</span>, <span class="string">"review_num"</span> : <span class="string">"0"</span> &#125;</span><br><span class="line">&#123; <span class="string">"_id"</span> : ObjectId(<span class="string">"5ac8ba021e885105bc184e72"</span>), <span class="string">"name"</span> : <span class="string">"Soumission"</span>, <span class="string">"price"</span> : <span class="string">"£50.10"</span>, <span class="string">"review_rating"</span> : <span class="number">1</span>, <span class="string">"upc"</span> : <span class="string">"6957f44c3847a760"</span>, <span class="string">"stock"</span> : <span class="string">"20"</span>, <span class="string">"review_num"</span> : <span class="string">"0"</span> &#125;</span><br><span class="line">。。。。。。。。。。</span><br></pre></td></tr></table></figure></p><p><img src="https://dongfei.oss-cn-shanghai.aliyuncs.com/blog/toscrape9.jpg" alt="toscrape9"></p><h5 id="Redis"><a href="#Redis" class="headerlink" title="Redis"></a>Redis</h5><p>Redis 是一个使用 ANSI C 编写的高性能 Key-Value 数据库，使用内存作为主存储，内存中的数据也可以被持久化到硬盘。</p><p>在 Python 中可以使用第三方库 redis-py 访问 Redis 数据库，使用 pip 安装 redis-py ：<code>pip install redis</code></p><p>下面是使用 redis-py 将数据写入 Redis 数据库的简单示例：</p><figure class="highlight vala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># -*- coding:utf-8 -*-</span></span><br><span class="line">__author__ = <span class="string">"東飛"</span></span><br><span class="line">__date__ = <span class="string">"2018-4-7"</span></span><br><span class="line"></span><br><span class="line">import redis</span><br><span class="line"></span><br><span class="line"><span class="meta"># 连接数据库</span></span><br><span class="line">r = redis.StrictRedis(host=<span class="string">'localhost'</span>, port=<span class="number">6379</span>, db=<span class="number">0</span>, password=<span class="string">'root'</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta"># 创建 3 条数据</span></span><br><span class="line">person1 = &#123;</span><br><span class="line">    <span class="string">'name'</span>: <span class="string">'那小子真帅'</span>,</span><br><span class="line">    <span class="string">'age'</span>: <span class="number">21</span>,</span><br><span class="line">    <span class="string">'sex'</span>: <span class="string">'M'</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">person2 = &#123;</span><br><span class="line">    <span class="string">'name'</span>: <span class="string">'卖姑娘的小火柴'</span>,</span><br><span class="line">    <span class="string">'age'</span>: <span class="number">22</span>,</span><br><span class="line">    <span class="string">'sex'</span>: <span class="string">'M'</span>,</span><br><span class="line">&#125;</span><br><span class="line">person3 = &#123;</span><br><span class="line">    <span class="string">'name'</span>: <span class="string">'Hello'</span>,</span><br><span class="line">    <span class="string">'age'</span>: <span class="number">23</span>,</span><br><span class="line">    <span class="string">'sex'</span>: <span class="string">'M'</span>,</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta"># 将 3 条数据以 Hash 类型(哈希) 保存到 Redis 中</span></span><br><span class="line">r.hmset(<span class="string">'person:1'</span>, person1)</span><br><span class="line">r.hmset(<span class="string">'person:2'</span>, person2)</span><br><span class="line">r.hmset(<span class="string">'person:3'</span>, person3)</span><br><span class="line"></span><br><span class="line"><span class="meta"># 关闭连接</span></span><br><span class="line">r.connection_pool.disconnect()</span><br></pre></td></tr></table></figure><p>Redis 是 Key-Value 数据库，一项数据在数据库中就是一个键值对，存储多项同类的数据时（如 Book ），通常以 <code>item:id</code> 这样的形式作为每项数据的键，其中的  <strong>:</strong> 并没有什么特殊，也可以换成 <strong>.</strong> 或 <strong>1</strong> 等，只是大家习惯这样使用。</p><p>查看 Redis 中的数据：</p><figure class="highlight taggerscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">D:<span class="symbol">\P</span>rogram Files<span class="symbol">\R</span>edisWindows</span><br><span class="line">$ redis-cli -a root</span><br><span class="line">127.0.0.1:6379&gt; KEYS person:*</span><br><span class="line">1) "person:2"</span><br><span class="line">2) "person:3"</span><br><span class="line">3) "person:1"</span><br><span class="line">127.0.0.1:6379&gt; HGETALL person:1</span><br><span class="line">1) "name"</span><br><span class="line">2) "<span class="symbol">\x</span>e9<span class="symbol">\x</span>82<span class="symbol">\x</span>a3<span class="symbol">\x</span>e5<span class="symbol">\x</span>b0<span class="symbol">\x</span>8f<span class="symbol">\x</span>e5<span class="symbol">\x</span>ad<span class="symbol">\x</span>90<span class="symbol">\x</span>e7<span class="symbol">\x</span>9c<span class="symbol">\x</span>9f<span class="symbol">\x</span>e5<span class="symbol">\x</span>b8<span class="symbol">\x</span>85"</span><br><span class="line">3) "age"</span><br><span class="line">4) "21"</span><br><span class="line">5) "sex"</span><br><span class="line">6) "M"</span><br><span class="line">127.0.0.1:6379&gt;</span><br></pre></td></tr></table></figure><p>实现 RedisPipeline 代码如下：</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数据存储至 Redis 数据库</span></span><br><span class="line">import redis</span><br><span class="line">from scrapy import Item</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RedisPipeline</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open_spider</span><span class="params">(<span class="keyword">self</span>, spider)</span></span><span class="symbol">:</span></span><br><span class="line">        db_host = spider.custom_settings.get(<span class="string">'REDIS_HOST'</span>, <span class="string">'localhost'</span>)</span><br><span class="line">        db_port = spider.custom_settings.get(<span class="string">"REDIS_PORT"</span>, <span class="number">6379</span>)</span><br><span class="line">        db_passwd = spider.custom_settings.get(<span class="string">"REDIS_PASSWORD"</span>, <span class="string">'root'</span>)</span><br><span class="line">        db_index = spider.custom_settings.get(<span class="string">"REDIS_DB_INDEX"</span>, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">self</span>.db_conn = redis.StrictRedis(host=db_host, port=db_port, db=db_index, password=db_passwd)</span><br><span class="line">        <span class="keyword">self</span>.item_i = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span><span class="params">(<span class="keyword">self</span>, spider)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">self</span>.db_conn.connection_pool.disconnect()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(<span class="keyword">self</span>, item, spider)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">self</span>.insert_db(item)</span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">insert_db</span><span class="params">(<span class="keyword">self</span>, item)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">if</span> isinstance(item, Item)<span class="symbol">:</span></span><br><span class="line">            item = dict(item)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">self</span>.item_i += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">self</span>.db_conn.hmset(<span class="string">'book:%s'</span> % <span class="keyword">self</span>.item_i, item)</span><br></pre></td></tr></table></figure><p>解释上述代码如下:</p><ul><li>open_spider 方法</li></ul><p>在开始爬取数据之前被调用。在该方法中通过 spider.custom_settings 或者 spider.settings 对象读取用户在配置文件中指定的数据库，然后建立与数据库的连接，将得到的连接对象<br>賦值给 self.db_conn ，以便之后使用，并初始化一个 self.item_i 作为每项数据的 id 。在插入一项数据时，使用 self.item_i 自加的结果构造数据在数据库中的键。</p><ul><li>process_item 方法</li></ul><p>处理爬取到的每一项数据，在该方法中调用 insert_db 方法，执行数据库的插入操作，在 insert_db 方法中，先将每一项数据转换成字典，然后调用 hmset 方法将数据以 Hash 类型存入 Redis 数据库。 </p><ul><li>close_spider 方法</li></ul><p>在爬取完全部数据后被调用，在该方法中关闭与数据库的连接。</p><p>在配置文件 <code>settings.py</code> 或者 <code>custom_settings</code> 中指定所要使用的 MongoDB 数据库，并启用 RedisPipeline ：</p><figure class="highlight xquery"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">custom_settings = &#123;</span><br><span class="line">        <span class="string">"ITEM_PIPELINES"</span>: &#123;</span><br><span class="line">            <span class="string">'ArticleSpider.pipelines.BookPipeline'</span>: <span class="number">300</span>,</span><br><span class="line">            <span class="string">'ArticleSpider.pipelines.RedisPipeline'</span>: <span class="number">404</span>,</span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="string">'REDIS_HOST'</span>: <span class="string">'localhost'</span>,</span><br><span class="line">        <span class="string">"REDIS_PORT"</span>: <span class="number">6379</span>,</span><br><span class="line">        <span class="string">"REDIS_PASSWORD"</span>: <span class="string">'root'</span>,</span><br><span class="line">        <span class="string">"REDIS_DB_INDEX"</span>: <span class="number">0</span>,</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p><a href="https://blog.csdn.net/babados/article/details/78575145" target="_blank" rel="noopener">Pycharm 集成 Redis 可视化插件 Iedis</a></p><p>运行爬虫，井查看数据库 ：<br><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="symbol">F:</span>\VirtualEnv\jobboleArticle\ArticleSpider\ArticleSpider</span><br><span class="line">(jobboleArticle) $ scrapy crawl books</span><br><span class="line"></span><br><span class="line">。。。。。。。。。。</span><br><span class="line"></span><br><span class="line"><span class="symbol">D:</span>\Program Files\RedisWindows</span><br><span class="line">$ redis-cli -a root</span><br><span class="line"><span class="meta">127.0.0.1:6379&gt;</span> KEYS <span class="symbol">book:</span>*</span><br><span class="line">。。。。。。。。。。</span><br><span class="line"><span class="number">994</span>) <span class="string">"book:235"</span></span><br><span class="line"><span class="number">995</span>) <span class="string">"book:903"</span></span><br><span class="line"><span class="number">996</span>) <span class="string">"book:244"</span></span><br><span class="line"><span class="number">997</span>) <span class="string">"book:543"</span></span><br><span class="line"><span class="number">998</span>) <span class="string">"book:293"</span></span><br><span class="line"><span class="number">999</span>) <span class="string">"book:440"</span></span><br><span class="line"><span class="number">1000</span>) <span class="string">"book:729"</span></span><br><span class="line"><span class="meta">127.0.0.1:6379&gt;</span> HGETALL <span class="symbol">book:</span><span class="number">1</span></span><br><span class="line"> <span class="number">1</span>) <span class="string">"name"</span></span><br><span class="line"> <span class="number">2</span>) <span class="string">"Starving Hearts (Triangular Trade Trilogy, #1)"</span></span><br><span class="line"> <span class="number">3</span>) <span class="string">"price"</span></span><br><span class="line"> <span class="number">4</span>) <span class="string">"\xc2\xa313.99"</span></span><br><span class="line"> <span class="number">5</span>) <span class="string">"review_rating"</span></span><br><span class="line"> <span class="number">6</span>) <span class="string">"2"</span></span><br><span class="line"> <span class="number">7</span>) <span class="string">"upc"</span></span><br><span class="line"> <span class="number">8</span>) <span class="string">"0312262ecafa5a40"</span></span><br><span class="line"> <span class="number">9</span>) <span class="string">"stock"</span></span><br><span class="line"><span class="number">10</span>) <span class="string">"19"</span></span><br><span class="line"><span class="number">11</span>) <span class="string">"review_num"</span></span><br><span class="line"><span class="number">12</span>) <span class="string">"0"</span></span><br><span class="line"><span class="meta">127.0.0.1:6379&gt;</span> HGETALL <span class="symbol">book:</span><span class="number">2</span></span><br><span class="line"> <span class="number">1</span>) <span class="string">"price"</span></span><br><span class="line"> <span class="number">2</span>) <span class="string">"\xc2\xa317.93"</span></span><br><span class="line"> <span class="number">3</span>) <span class="string">"upc"</span></span><br><span class="line"> <span class="number">4</span>) <span class="string">"e72a5dfc7e9267b2"</span></span><br><span class="line"> <span class="number">5</span>) <span class="string">"review_rating"</span></span><br><span class="line"> <span class="number">6</span>) <span class="string">"3"</span></span><br><span class="line"> <span class="number">7</span>) <span class="string">"review_num"</span></span><br><span class="line"> <span class="number">8</span>) <span class="string">"0"</span></span><br><span class="line"> <span class="number">9</span>) <span class="string">"name"</span></span><br><span class="line"><span class="number">10</span>) <span class="string">"The Coming Woman: A Novel Based on the Life of the Infamous Feminist, Victoria Woodhull"</span></span><br><span class="line"><span class="number">11</span>) <span class="string">"stock"</span></span><br><span class="line"><span class="number">12</span>) <span class="string">"19"</span></span><br><span class="line"><span class="meta">127.0.0.1:6379&gt;</span></span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;h5 id=&quot;数据保存至数据库&quot;&gt;&lt;a href=&quot;#数据保存至数据库&quot; class=&quot;headerlink&quot; title=&quot;数据保存至数据库&quot;&gt;&lt;/a&gt;数据保存至数据库&lt;/h5&gt;&lt;p&gt;以 &lt;code&gt;toscrape_book&lt;/code&gt; 项目作为环境，使用 Item Pipeline 实现 Scrapy 爬虫，将爬取到的数据存储到数据库中。爬取网站 &lt;a href=&quot;http://books.toscrape.com/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;http://books.toscrape.com/&lt;/a&gt; 中的书籍信息，其中每一本书的信息包括：&lt;br&gt;&lt;code&gt;书名&lt;/code&gt;、&lt;code&gt;价格&lt;/code&gt;、&lt;code&gt;评价等级&lt;/code&gt;、&lt;code&gt;产品编码&lt;/code&gt;、&lt;code&gt;库存量&lt;/code&gt;、&lt;code&gt;评价数量&lt;/code&gt;。&lt;br&gt;&lt;img src=&quot;https://dongfei.oss-cn-shanghai.aliyuncs.com/blog/toscrape1.jpg&quot; alt=&quot;toscrape1&quot;&gt;&lt;br&gt;
    
    </summary>
    
      <category term="Python" scheme="http://blog.dongfei.xin/categories/Python/"/>
    
      <category term="Scrapy" scheme="http://blog.dongfei.xin/categories/Python/Scrapy/"/>
    
    
      <category term="Scrapy" scheme="http://blog.dongfei.xin/tags/Scrapy/"/>
    
  </entry>
  
  <entry>
    <title>精通 Scrapy 网络爬虫（动态网页篇）</title>
    <link href="http://blog.dongfei.xin/2018-04-01/%E7%B2%BE%E9%80%9A%20Scrapy%20%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%EF%BC%88%E5%8A%A8%E6%80%81%E7%BD%91%E9%A1%B5%E7%AF%87%EF%BC%89/"/>
    <id>http://blog.dongfei.xin/2018-04-01/精通 Scrapy 网络爬虫（动态网页篇）/</id>
    <published>2018-04-01T11:26:21.000Z</published>
    <updated>2018-04-12T11:22:42.808Z</updated>
    
    <content type="html"><![CDATA[<blockquote class="blockquote-center">自己不牛逼，认识的人再多也没用</blockquote><h5 id="爬取动态页面"><a href="#爬取动态页面" class="headerlink" title="爬取动态页面"></a>爬取动态页面</h5><p>静态页面的内容始终不变，爬取相对容易，但在现实中，日前绝大多数网站的页面都是动态页面。动态页面中的部分内容是浏览器运行页面中的 JavaScript 脚本动态生成的，爬取相对困难。<br>先来看一个简单的动态页面的例子，在浏览器中打开 <a href="http://quotes.toscrape.com/js/" target="_blank" rel="noopener">Quotes</a></p><p><img src="https://dongfei.oss-cn-shanghai.aliyuncs.com/blog/quotes.jpg" alt="quotes"></p><a id="more"></a><p>页面中有 10 条名人名言，每一条都包含在一个 <code>&lt;div class=&quot;quto&quot;&gt;</code>元素中。现在，我们在 scrapy shell 环境下尝试爬取页面中的名人名言：<br><figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; scrapy shell http:<span class="comment">//quotes.toscrape.com/js/</span></span><br><span class="line">..................</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; response.css(<span class="string">'div.quote'</span>)</span><br><span class="line">[]</span><br><span class="line">&gt;&gt;&gt;</span><br></pre></td></tr></table></figure></p><p>从结果看出。爬取失败了，在页面中没有找到任何包含名人名言的<code>&lt;div class=&quot;quto&quot;&gt;</code>元素，这些<code>&lt;div class=&quot;quto&quot;&gt;</code>就是动态内容，从服务器下载的页面中并不包含它们。浏览器执行了页面中的一段 JavasScipt 代码后，它们才被显示出来。查看源码：<br><img src="https://dongfei.oss-cn-shanghai.aliyuncs.com/blog/jsjsjs.jpg" alt="jsjsjs.jpg"></p><p>上面是动态网页中最简单的一个例子，数据被硬编码于 JavaScript 代码中，实际中更常见的是 JavaScript 通过 HTTP 请求跟网站动态交互获取数据（AJAX），然后使用数据更新 HTML 页面。爬取此类动态网页需要先执行页面中的 JavaScript 代码渲染页面，再进行爬取。下面介绍使用 JavaScript 渲染引擎渲染页面。</p><h5 id="Splash-渲染引擎"><a href="#Splash-渲染引擎" class="headerlink" title="Splash 渲染引擎"></a>Splash 渲染引擎</h5><p>Splash 是 Scrapy 官方推荐的 JavaScript 渲染引擎，它是使用 Webkit 开发的轻量级无界面浏览器，提供基于 HTTP 接口的 JavaScript 渲染服务，支持以下功能:</p><ul><li>为用户返回经过渲染的 HTML 页面或页面截图。</li><li>并发渲染多个页面。</li><li>关闭图片加载，加速渲染。</li><li>在页面中执行用户自定义的 JavaScript 代码。</li><li>执行用户自定义的渲染脚本（lua），功能类似于 PhantomJS。</li></ul><p><a href="https://github.com/scrapy-plugins/scrapy-splash#installation" target="_blank" rel="noopener">项目链接</a></p><p><a href="http://splash.readthedocs.io/en/latest/scripting-tutorial.html" target="_blank" rel="noopener">API 文档</a></p><p>安装 Splash，需要依赖容器 Docker</p><p>Linux 下安装 Docker 和 Splash ：<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-<span class="builtin-name">get</span> install docker</span><br><span class="line">sudo docker pull scrapinghub/splash</span><br></pre></td></tr></table></figure></p><p>Windows 下安装参考：</p><ul><li><a href="https://blog.csdn.net/sanyuedexuanlv/article/details/78759743" target="_blank" rel="noopener">安裝 Docker for Windows</a></li><li><a href="https://www.cnblogs.com/zhxshseu/p/5970a5a763c8fe2b01cd2eb63a8622b2.html" target="_blank" rel="noopener">Docker 使用阿里云docker镜像加速</a></li><li><a href="https://www.cnblogs.com/my8100/p/splash_install.html" target="_blank" rel="noopener">Scrapy相关：Splash 安装 A javascript rendering service 渲染</a></li></ul><p>安装完成后，在本机的 8050 和 8051 端口开启 Splash 服务:   </p><figure class="highlight dns"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">C:\Users\<span class="number">29485</span></span><br><span class="line">$ docker run -p <span class="number">8050:8050</span> -p <span class="number">8051:8051</span> scrapinghub/splash</span><br><span class="line"><span class="number">2018-04-02</span> <span class="number">08</span>:<span class="number">38:30+0000</span> [-] Log opened.</span><br><span class="line"><span class="number">2018-04-02</span> <span class="number">08</span>:<span class="number">38:30.228904</span> [-] Splash version: <span class="number">3</span>.<span class="number">2</span></span><br><span class="line"><span class="number">2018-04-02</span> <span class="number">08</span>:<span class="number">38:30.246529</span> [-] Qt <span class="number">5</span>.<span class="number">9</span>.<span class="number">1</span>, PyQt <span class="number">5</span>.<span class="number">9</span>, WebKit <span class="number">602</span>.<span class="number">1</span>, sip <span class="number">4</span>.<span class="number">19</span>.<span class="number">3</span>, Twisted <span class="number">16</span>.<span class="number">1</span>.<span class="number">1</span>, Lua <span class="number">5</span>.<span class="number">2</span></span><br><span class="line"><span class="number">2018-04-02</span> <span class="number">08</span>:<span class="number">38:30.248380</span> [-] Python <span class="number">3</span>.<span class="number">5</span>.<span class="number">2</span> (default, Nov <span class="number">23</span> <span class="number">2017</span>, <span class="number">16</span>:<span class="number">37</span>:<span class="number">01</span>) [GCC <span class="number">5</span>.<span class="number">4</span>.<span class="number">0 20160609</span>]</span><br><span class="line"><span class="number">2018-04-02</span> <span class="number">08</span>:<span class="number">38:30.249581</span> [-] Open files limit: <span class="number">1048576</span></span><br><span class="line"><span class="number">2018-04-02</span> <span class="number">08</span>:<span class="number">38:30.249750</span> [-] Can't bump open files limit</span><br><span class="line"><span class="number">2018-04-02</span> <span class="number">08</span>:<span class="number">38</span>:<span class="number">30.373421</span> [-] Xvfb is started: ['Xvfb', ':<span class="number">177507167</span>', '-screen', '<span class="number">0</span>', '<span class="number">1024</span>x768x24', '-nolisten', 'tcp']</span><br><span class="line">QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-root'</span><br><span class="line"><span class="number">2018-04-02</span> <span class="number">08</span>:<span class="number">38</span>:<span class="number">30.757496</span> [-] proxy profiles support is enabled, proxy profiles path: /etc/splash/proxy-profiles</span><br><span class="line"><span class="number">2018-04-02</span> <span class="number">08</span>:<span class="number">38</span>:<span class="number">31.028505</span> [-] verbosity=<span class="number">1</span></span><br><span class="line"><span class="number">2018-04-02</span> <span class="number">08</span>:<span class="number">38</span>:<span class="number">31.028650</span> [-] slots=<span class="number">50</span></span><br><span class="line"><span class="number">2018-04-02</span> <span class="number">08</span>:<span class="number">38</span>:<span class="number">31.028836</span> [-] argument_cache_max_entries=<span class="number">500</span></span><br><span class="line"><span class="number">2018-04-02</span> <span class="number">08</span>:<span class="number">38</span>:<span class="number">31.030539</span> [-] Web UI: enabled, Lua: enabled (sandbox: enabled)</span><br><span class="line"><span class="number">2018-04-02</span> <span class="number">08</span>:<span class="number">38</span>:<span class="number">31.030702</span> [-] Server listening on <span class="number">0.0.0.0</span>:<span class="number">8050</span></span><br><span class="line"><span class="number">2018-04-02</span> <span class="number">08</span>:<span class="number">38:31.032203</span> [-] Site starting on <span class="number">8050</span></span><br><span class="line"><span class="number">2018-04-02</span> <span class="number">08</span>:<span class="number">38</span>:<span class="number">31.032426</span> [-] Starting factory &lt;twisted.web.server.Site object at <span class="number">0</span>x7f<span class="number">4d140d57f0</span>&gt;</span><br></pre></td></tr></table></figure><p>Splash 功能丰富，包含多个服务端点，这里只介绍其中两个最常用的端点</p><ul><li><p>render.html<br>提供 JavaScript 页面渲染服务。</p></li><li><p>execute<br>执行用户自定义的渲染脚本（lua），利用该端点可在页面中执行 JavaScript 代码</p></li></ul><h6 id="render-html-端点"><a href="#render-html-端点" class="headerlink" title="render.html 端点"></a>render.html 端点</h6><p>JavaScript 页面渲染服务是 Splash 中最基础的服务，调用方式如下：<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">服务端点             render.html</span><br><span class="line">请求地址             http:<span class="comment">//:locallhost:8050/render.html</span></span><br><span class="line">请求方式             GET / POST</span><br><span class="line">返回类型             html</span><br></pre></td></tr></table></figure></p><p>render.html 端点支持的参数如下所示：<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">参数              是否必选        类型         描述</span><br><span class="line">url                 必选         string    需要渲染页面的 url</span><br><span class="line">timeout             可选         float    渲染页面超时时间     </span><br><span class="line">proxy               可选         string    代理服务器地址 </span><br><span class="line">wait                可选         float     等待页面渲染的时间</span><br><span class="line">images              可选         integer   是否下载图片，默认为 1</span><br><span class="line">js_source           可选         string    用户自定义的 JavaScript 代码的，在页面渲染前执行</span><br></pre></td></tr></table></figure></p><p>这里仅列出部分常用参数，详细内容参见官方文档。<br>下面是使用 requests 库调用 render.html 端点服务对页面 <code>http://quotes.toscrape.com/js/</code> 进行渲染的示例代码。</p><figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import requests</span><br><span class="line">&gt;&gt;&gt; <span class="keyword">from</span> scrapy.selector import Selector</span><br><span class="line">&gt;&gt;&gt; splash_url = <span class="string">"http://localhost:8050/render.html"</span></span><br><span class="line">&gt;&gt;&gt; args = &#123;'url':<span class="string">"http://quotes.toscrape.com/js/"</span>, '<span class="keyword">timeout</span>':<span class="number">5</span>, 'image':<span class="number">0</span>&#125;</span><br><span class="line">&gt;&gt;&gt; response = requests.<span class="keyword">get</span>(splash_url,params=args)</span><br><span class="line">&gt;&gt;&gt; sel = Selector(response)</span><br><span class="line">&gt;&gt;&gt; sel.css('<span class="keyword">div</span>.<span class="literal">quote</span> span.<span class="built_in">text</span>::<span class="built_in">text</span>').extract()</span><br><span class="line">['“The world <span class="keyword">as</span> we have created <span class="keyword">it</span> <span class="keyword">is</span> a process <span class="keyword">of</span> our thinking. It cannot be changed <span class="keyword">without</span> changing our thinking.”', '“It <span class="keyword">is</span> our c</span><br><span class="line">hoices, Harry, <span class="keyword">that</span> show what we truly are, far more than our abilities.”', '“There are only two ways <span class="keyword">to</span> live your life. One <span class="keyword">is</span> <span class="keyword">as</span> tho</span><br><span class="line">ugh nothing <span class="keyword">is</span> a miracle. The other <span class="keyword">is</span> <span class="keyword">as</span> though everything <span class="keyword">is</span> a miracle.”', '“The person, be <span class="keyword">it</span> gentleman <span class="keyword">or</span> lady, who has <span class="keyword">not</span> pleasu</span><br><span class="line">re <span class="keyword">in</span> a good novel, must be intolerably stupid.”', <span class="string">"“Imperfection is beauty, madness is genius and it's better to be absolutely ridicu</span></span><br><span class="line"><span class="string">lous than absolutely boring.”"</span>, '“Try <span class="keyword">not</span> <span class="keyword">to</span> become a man <span class="keyword">of</span> success. Rather become a man <span class="keyword">of</span> value.”', '“It <span class="keyword">is</span> better <span class="keyword">to</span> be hated fo</span><br><span class="line">r what you are than <span class="keyword">to</span> be loved <span class="keyword">for</span> what you are <span class="keyword">not</span>.”', <span class="string">"“I have not failed. I've just found 10,000 ways that won't work.”"</span>, <span class="string">"“A wo</span></span><br><span class="line"><span class="string">man is like a tea bag; you never know how strong it is until it's in hot water.”"</span>, '“A <span class="built_in">day</span> <span class="keyword">without</span> sunshine <span class="keyword">is</span> like, you know, night.</span><br><span class="line">”']</span><br><span class="line">&gt;&gt;&gt;</span><br></pre></td></tr></table></figure><p>在上述代码中，依据文档中的描述设置参数 url、timeout、images，然后发送 HTTP 请求服务接口地址。从运行结果看出，页面渲染成功，我们爬取到了页面中的 10 条名人名言。</p><h6 id="execute-端点"><a href="#execute-端点" class="headerlink" title="execute 端点"></a>execute 端点</h6><p>在爬取某些页面时，我们想在页面中执行一些用户自定义的 JavaScript 代码，例如用 JavaScript 模拟点击页面中的按钮，或调用页面中的 JavaScript 函数与服务器交互。利用 Splash 的 execute 端点提供的服务可以实现这样的功能。</p><figure class="highlight smali"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">服务端点            execute</span><br><span class="line">请求地址            http://localhost:8050/execute</span><br><span class="line">请求方式            POST</span><br><span class="line">返回类型            自定义</span><br></pre></td></tr></table></figure><p>execute 端点支持的参数如下：<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">参数          必选/可选       类型         描述</span><br><span class="line">lua_source      必选          string      用户自定义的 lua 脚本</span><br><span class="line">timeout         可选          float       渲染页面超时时间</span><br><span class="line">proxy           可选          string      代理服务器地址</span><br></pre></td></tr></table></figure></p><p>我们可以将 execute 端点的服务看作一个可用 lua 语言编程的浏览器，功能类似于 PhantomJS。使用时需传递一个用户自定义的 lua 脚本给 Splash,该 lua 脚本中包含用户<br>想要模拟的浏览器行为，例如:</p><ul><li>打开某 url 地址的页面</li><li>等待页面加载及渲染</li><li>执行 JavaScript 代码</li><li>获取 HTTP 响应头部</li><li>获取 Cookie</li></ul><p>下面使用 requests 库调用 execute 端点服务的示例代码：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> requests</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> json</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>lua_script = <span class="string">'''</span></span><br><span class="line"><span class="string"><span class="meta">... </span>function main(splash):</span></span><br><span class="line"><span class="string"><span class="meta">... </span>    splash:go("http://blog.dongfei.xin/")</span></span><br><span class="line"><span class="string"><span class="meta">... </span>    spalsh:wait(0.5)</span></span><br><span class="line"><span class="string"><span class="meta">... </span>    local title = splash:evaljs("document.title")</span></span><br><span class="line"><span class="string"><span class="meta">... </span>    return &#123;title = title&#125;</span></span><br><span class="line"><span class="string"><span class="meta">... </span>end</span></span><br><span class="line"><span class="string"><span class="meta">... </span>'''</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>splash_url = <span class="string">'http://localhost:8050/execute'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>data = json.dumps(&#123;<span class="string">'lua_source'</span>: lua_script&#125;)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>headers = &#123;<span class="string">'content-type'</span>:<span class="string">'application/json'</span>&#125;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>response= requests.post(splash_url,headers=headers,data=data)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>response.content</span><br><span class="line"><span class="string">b'&#123;"title":"那小子真帅"&#125;'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>response.json()</span><br><span class="line">&#123;<span class="string">'title'</span>:<span class="string">'那小子真帅'</span>&#125;</span><br></pre></td></tr></table></figure></p><p>用户自定义的 lua 脚本中必须包含一个 main 函数作为程序入口，main 函数被调用时会传入一个 splash 对象（lua 中的对象），用户可以调用该对象上的方法操纵 Splash 。例如，在上面的例子中，先调用 go 方法打开某页面，再调用 wait 方法等待页面渲染，然后调用 evaljs 方法执行一个 JavaScript 表达式，并将结果转化为相应的 lua 对象，最终 Splash 根据 main 函数的返回值构造 HTTP 响应返回给用户，main 函数的返回值可以是字符串，也可以是 lua 中的表（类似 Python 字典）表会被编码成 json 串。</p><p>splash 对象常用的属性和方法。</p><ul><li>splash.args 属性</li></ul><p>用户传入参数的表，通过该属性可以访问用户传入的参数，如 splash.args.url、<br>splash.args.wait。</p><ul><li>splash.js_enabled 属性</li></ul><p>用于开启/禁止 JavaScript 渲染，默认为 true。</p><ul><li>splash.images_enabled 属性</li></ul><p>用于开启/禁止图片加载，默认为 true。</p><ul><li>splash:go 方法</li></ul><p>splash:go{url,baseurl=nil,headers=nil,http_method=”GET”,body=nil,formdata=nil}类似于在浏览器中打开某 url 地址的页面，页面所需资源会被加载，并进行 JavaScript 渲染，可以通过参数指定 HTTP 请求头部、请求方法、表单数据等。</p><ul><li>splash:wait 方法</li></ul><p>splash:wait{time,cancel_on_redirect=false,cancel_on_error=true} 等待页面渲染，time 参数为等待的秒数。</p><ul><li>splash:evaljs 方法</li></ul><p>splash:evaljs(snippet)<br>在当前页面下，执行一段 JavaScript 代码，并返回最后一句表达式的值。</p><ul><li>splash:runjs 方法</li></ul><p>splash:runjs(snippet)<br>在当前页面下，执行一段 JavaScript 代码，与 evaljs 方法相比，该函数只执行 JavaScript 代码，不返回值。</p><ul><li>splash:url 方法</li></ul><p>splash:url()<br>获取当前页面的 url</p><ul><li>splash:html 方法</li></ul><p>splash:html()<br>获取当前页面的 HTML 文本。</p><ul><li>splash:get_cookies 方法</li></ul><p>splash:get_cookies()<br>获取全部 Cookie 信息。</p><h5 id="在-Scrapy-中使用Splash"><a href="#在-Scrapy-中使用Splash" class="headerlink" title="在 Scrapy 中使用Splash"></a>在 Scrapy 中使用Splash</h5><p>在 Scrapy 中调用 Splash 服务，需要安装 scrapy-splash：<code>pip install scrapy-splash</code></p><p>在项目配置文件 setting.py 中对 scrapy-plash 进行配置：<br><figure class="highlight vala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># Splash服务器地址 </span></span><br><span class="line">SPLASH_URL = <span class="string">'http://localhost:8050/'</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># 开启 Splash 的两个下载中间件并调整 HttpCompressionMiddleware 的次序</span></span><br><span class="line"></span><br><span class="line">DOWNLOADER_MIDDLEWARES = &#123;</span><br><span class="line">    <span class="string">'scrapy_splash.SplashCookiesMiddleware'</span>: <span class="number">723</span>,</span><br><span class="line">    <span class="string">'scrapy_splash.SplashMiddleware'</span>: <span class="number">725</span>,</span><br><span class="line">    <span class="string">'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware'</span>: <span class="number">810</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta"># 设置去重过滤器</span></span><br><span class="line">DUPEFILTER_CLASS = <span class="string">'scrapy_splash.SplashAwareDupeFilter'</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># 用来支持 cache_args （可选）</span></span><br><span class="line">SPIDER_MIDDLEWARES = &#123;</span><br><span class="line">   <span class="string">'scrapy_splash.SplashDeduplicateArgsMiddleware'</span>: <span class="number">100</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>使用 scrapy_splash 调用 Splash 服务非常筒单，scrapy_splash 中定义了一个 SplashRequest 类，用户只需使用 scrapy_splash.SplashRequest （替代scrapy .Request）提交请求即可。下面是 SplashRequest 构造器方法中的一些常用参数。</p><ul><li>url</li></ul><p>与 scrapy.Request 中的 url 相同，也就是待爬取页面的 url（注意，不是Splash 服务器地址）</p><ul><li>headers</li></ul><p>与 scrapy.Request 中的 headers 相同。</p><ul><li>cookies</li></ul><p>与 scrapy.Request 中的 cookies 相同。</p><ul><li>args</li></ul><p>传递给 Splash 的参数（除 url 以外），如 wait、timeout、images、js_source 等。</p><ul><li>cache_args</li></ul><p>如果 args 中的某些参数每次调用都重复传递并且数据量较大（例如一段 JavaScript 代码），此时可以把该参数名填入 cache_args 列表中，让 Splash 服务器缓存该参数，如 <code>SplashRequest(url,args={&#39;js_source&#39;: js,&#39;wait&#39;: 0.5},cache_args=[&#39;js_source&#39;])</code>。</p><ul><li>endpoint</li></ul><p>Splash 服务端点，默认为 <code>render.html</code>，即 JavaScript 页面渲染服务，该参数可以设置为 <code>render.json</code>、<code>render.har</code>、<code>render.png</code>、<code>render.jpeg</code>、<code>execute</code>等，更多服务端点可以查阅文档。</p><ul><li>splash_url</li></ul><p>Splash 服务器地址，默认为 None，即使用配置文件中 SPLASH_URL 的地址。</p><h5 id="项目实战-爬取-toscrape-中的名人名言"><a href="#项目实战-爬取-toscrape-中的名人名言" class="headerlink" title="项目实战: 爬取 toscrape 中的名人名言"></a>项目实战: 爬取 toscrape 中的名人名言</h5><ul><li>项目需求</li></ul><p>爬取网站 <a href="http://quotes.toscrape.com/js/" target="_blank" rel="noopener">http://quotes.toscrape.com/js/</a> 中的名人名言信息。</p><ul><li>编码实现</li></ul><p>项目目录下使用 scrapy genspider 命令创建 Spider：<br><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">scrapy</span> <span class="selector-tag">genspider</span> <span class="selector-tag">quotes</span> <span class="selector-tag">quotes</span><span class="selector-class">.toscrape</span><span class="selector-class">.com</span></span><br></pre></td></tr></table></figure></p><p>这个案例中。我们只使用 Splash 的 render.html 端点渲染页面，再进行爬取即<br>可实现 QuotesSpider，代码如下： </p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line">__author_<span class="number">_</span> = <span class="string">"東飛"</span></span><br><span class="line">__date_<span class="number">_</span> = <span class="string">"2017-11-29"</span></span><br><span class="line">import scrapy</span><br><span class="line">from scrapy_splash import SplashRequest</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">QuotesSpider</span>(<span class="title">scrapy</span>.<span class="title">Spider</span>):</span></span><br><span class="line">    name = <span class="string">'quotes'</span></span><br><span class="line">    allowed_domains = [<span class="string">'quotes.toscrape.com'</span>]</span><br><span class="line">    start_urls = [<span class="string">'http://quotes.toscrape.com/'</span>]</span><br><span class="line"></span><br><span class="line">    custom_settings = &#123;</span><br><span class="line">        <span class="comment"># Splash服务器地址</span></span><br><span class="line">        <span class="string">'SPLASH_URL'</span>: <span class="string">'http://localhost:8050/'</span>,</span><br><span class="line">        <span class="comment"># 开启 Splash 的两个下载中间件并调整 HttpCompressionMiddleware 的次序</span></span><br><span class="line">        <span class="string">'DOWNLOADER_MIDDLEWARES'</span>: &#123;</span><br><span class="line">            <span class="string">'scrapy_splash.SplashCookiesMiddleware'</span>: <span class="number">723</span>,</span><br><span class="line">            <span class="string">'scrapy_splash.SplashMiddleware'</span>: <span class="number">725</span>,</span><br><span class="line">            <span class="string">'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware'</span>: <span class="number">810</span>,</span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="comment"># 设置去重过滤器</span></span><br><span class="line">        <span class="string">'DUPEFILTER_CLASS'</span>: <span class="string">'scrapy_splash.SplashAwareDupeFilter'</span>,</span><br><span class="line">        <span class="comment"># 用来支持 cache_args （可选）</span></span><br><span class="line">        <span class="string">'SPIDER_MIDDLEWARES'</span>: &#123;</span><br><span class="line">            <span class="string">'scrapy_splash.SplashDeduplicateArgsMiddleware'</span>: <span class="number">100</span>,</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">start_requests</span><span class="params">(<span class="keyword">self</span>)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">for</span> url <span class="keyword">in</span> <span class="keyword">self</span>.<span class="symbol">start_urls:</span></span><br><span class="line">            <span class="keyword">yield</span> SplashRequest(url, args=&#123;<span class="string">'images'</span>: <span class="number">0</span>, <span class="string">'timeout'</span>: <span class="number">3</span>&#125;)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(<span class="keyword">self</span>, response)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">for</span> sel <span class="keyword">in</span> response.css(<span class="string">'div.quote'</span>)<span class="symbol">:</span></span><br><span class="line">            quote = <span class="keyword">self</span>.css(<span class="string">'span.text::text'</span>).extract_first()</span><br><span class="line">            author = <span class="keyword">self</span>.css(<span class="string">'small.author::text'</span>).extract_first()</span><br><span class="line">            <span class="keyword">yield</span> &#123;<span class="string">'quote'</span>: quote, <span class="string">'author'</span>: author&#125;</span><br><span class="line"></span><br><span class="line">        href = response.css(<span class="string">'li.next&gt;a::attr(href)'</span>).extract_first()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="symbol">href:</span></span><br><span class="line">            url = response.urljoin(href)</span><br><span class="line">            <span class="keyword">yield</span> SplashRequest(url, args=&#123;<span class="string">'images'</span>: <span class="number">0</span>, <span class="string">'timeout'</span>: <span class="number">3</span>&#125;)</span><br></pre></td></tr></table></figure><p>上述代码中。使用 SplashRequest 提交请求，SplashRequest 的构造器中无须传递<br>endpoint 参数，因为该参数默认值便是 <code>render.html</code> 。使用 args 参数禁止 Splash 加载图片，并设置渲染超时时间。<br>运行爬虫，观察结果：</p><figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"> scrapy crawl quotes -o .\data\quotes.csv</span><br><span class="line"></span><br><span class="line"> F:\VirtualEnv\jobboleArticle\ArticleSpider\ArticleSpider</span><br><span class="line">(jobboleArticle) $ cat -n .\data\quotes.csv</span><br><span class="line">     <span class="number">1</span>  <span class="literal">quote</span>,author</span><br><span class="line">     <span class="number">2</span>  “The world <span class="keyword">as</span> we have created <span class="keyword">it</span> <span class="keyword">is</span> a process <span class="keyword">of</span> our thinking. It cannot be changed <span class="keyword">without</span> changing our thinking.”,Albert Ein</span><br><span class="line">stein</span><br><span class="line">     <span class="number">3</span>  <span class="string">"“It is our choices, Harry, that show what we truly are, far more than our abilities.”"</span>,J.K. Rowling</span><br><span class="line">     <span class="number">4</span>  “There are only two ways <span class="keyword">to</span> live your life. One <span class="keyword">is</span> <span class="keyword">as</span> though nothing <span class="keyword">is</span> a miracle. The other <span class="keyword">is</span> <span class="keyword">as</span> though everything <span class="keyword">is</span> a mirac le.”,Albert Einstein</span><br></pre></td></tr></table></figure><h5 id="项目实战-爬取京东商城中的书籍信息"><a href="#项目实战-爬取京东商城中的书籍信息" class="headerlink" title="项目实战: 爬取京东商城中的书籍信息"></a>项目实战: 爬取京东商城中的书籍信息</h5><ul><li>项目需求</li></ul><p>爬取京东商城中所有 Python 书籍的名字和价格信息。</p><ul><li>页面分析</li></ul><p>在<a href="http://www.jd.com" target="_blank" rel="noopener">京东网站</a>的书籍分类下搜索 Python 关键字得到的页面<br><img src="https://dongfei.oss-cn-shanghai.aliyuncs.com/blog/jdd1.jpg" alt=""></p><p>结果有很多页，在每个书籍列表页面中可以数有 60 本书但在 scrapy shell<br>中爬取该页面时遇到了问题，仅在页面中找到了 30 本书，少了30本，代码如下:<br><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="symbol">F:</span>\VirtualEnv\jobboleArticle\ArticleSpider\ArticleSpider</span><br><span class="line">(jobboleArticle) $ scrapy shell</span><br><span class="line"></span><br><span class="line">.............</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; url = <span class="string">"https://search.jd.com/Search?keyword=python&amp;enc=utf-8&amp;wq=python"</span></span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; fetch(url)</span><br><span class="line"><span class="number">2018</span>-<span class="number">04</span>-<span class="number">06</span> <span class="number">16</span><span class="symbol">:</span><span class="number">52</span><span class="symbol">:</span><span class="number">03</span> [scrapy.core.engine] <span class="symbol">INFO:</span> Spider opened</span><br><span class="line"><span class="number">2018</span>-<span class="number">04</span>-<span class="number">06</span> <span class="number">16</span><span class="symbol">:</span><span class="number">52</span><span class="symbol">:</span><span class="number">04</span> [scrapy.core.engine] <span class="symbol">DEBUG:</span> Crawled (<span class="number">200</span>) &lt;GET <span class="symbol">https:</span>/<span class="regexp">/search.jd.com/</span>Search?keyword=python&amp;enc=utf-<span class="number">8</span>&amp;wq=python&gt; (<span class="symbol">referer:</span> None)</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; len(response.css(<span class="string">'ul.gl-warp&gt;li'</span>))</span><br><span class="line"><span class="number">30</span></span><br></pre></td></tr></table></figure></p><p>原来页面中的 60 本书不是同时加载的，开始只有 30 本书，当我们使用鼠标滚轮滚动到页面下方某位置时，后 30 本书才由 JavaScript 脚本加载，通过实验可以验证这个说法，实验绝视如下：</p><p>（1） 页面刚加载时，在 Chrome 开发者工具的 console 中用 jQuery 代码查看当前<br>有多少本书，此时为 30。</p><p>（2） 之后滚动鼠标滚轮到某一位置时，可以看到 JavaScript 发送 HTTP 请求和服务<br>器交互（XHR）。</p><p>（3） 然用 jQuery 代码查看当前有多少本书，已经变成了 60 ，如图所示，</p><p><img src="https://dongfei.oss-cn-shanghai.aliyuncs.com/blog/jdd2.jpg" alt=""></p><p>既然如此，爬取这个页面时，可以先执行一段 JavaScript 代码，将滚动条拖到页面下方某位置，触发加载后 30 本书的事件，在开发者工具的 console 中进行实验，用 document.getElementsByXXX 方法随意选中页面下方的某元素，比如<code>下一页</code> 按钮所在的<code>&lt;div&gt;</code>元素，然后在该元素对象上调用 scrollIntoView(true) 完成拖曳动作，此时查看书籍数量，变成了 60 。如图：</p><p><img src="https://dongfei.oss-cn-shanghai.aliyuncs.com/blog/jdd3.jpg" alt=""></p><p>爬取一个页面的问题解决了，再宋研究如何从页面中找到下一页的 url 地址。下一页链接的 href 属性并不是一个 url，而在其 onclick 属性中包含了一条 JavaScript 代码，单击<code>下一页</code>按钮时会调函数<code>SEARCH.page(n,true)</code>。虽然<br>可以用 Splash 执行函数来跳转到下一页，但还是很麻烦，经观察发现，每个页面 url 的差异仅在于 page 参数不同，第一页<code>page=1</code>、第二页 <code>page=3</code>、第三页 <code>page=5</code> …… 以 2 递增，并且页面中还包含商品总数信息。因此，我们可以推算出所有页面的 url 。</p><p><img src="https://dongfei.oss-cn-shanghai.aliyuncs.com/blog/jdd4.jpg" alt=""></p><ul><li>编码实现</li></ul><p>项目目录下使用 scrapy genspider 命令创建 Spider：<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy genspider jd_book search<span class="selector-class">.jd</span><span class="selector-class">.com</span></span><br></pre></td></tr></table></figure></p><p>经上述分析，在爬取每一个书籍列表页面时都需要执行一段 JavaScript 代码，以让全部书籍加载，因此选用 Splash 的 execute 端点渲染页面，再进行爬取即<br>可实现 JdBookSpider，代码如下： </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line">__author__ = <span class="string">"東飛"</span></span><br><span class="line">__date__ = <span class="string">"2017-11-29"</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> Request</span><br><span class="line"><span class="keyword">from</span> scrapy_splash <span class="keyword">import</span> SplashRequest</span><br><span class="line"></span><br><span class="line">lua_script = <span class="string">'''</span></span><br><span class="line"><span class="string">function main(splash)</span></span><br><span class="line"><span class="string">    splash:go(splash.args.url)</span></span><br><span class="line"><span class="string">    splash:wait(2)</span></span><br><span class="line"><span class="string">    splash:runjs("document.getElementsByClassName('page')[0].scrollIntoView(true)")</span></span><br><span class="line"><span class="string">    splash:wait(2)</span></span><br><span class="line"><span class="string">    return splash:html()</span></span><br><span class="line"><span class="string">end</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JdBookSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">'jd_book'</span></span><br><span class="line">    allowed_domains = [<span class="string">'search.jd.com'</span>]</span><br><span class="line">    base_url = <span class="string">'https://search.jd.com/Search?keyword=python&amp;enc=utf-8&amp;book=y&amp;wq=python'</span></span><br><span class="line"></span><br><span class="line">    custom_settings = &#123;</span><br><span class="line">        <span class="comment"># Splash服务器地址</span></span><br><span class="line">        <span class="string">'SPLASH_URL'</span>: <span class="string">'http://localhost:8050/'</span>,</span><br><span class="line">        <span class="comment"># 开启 Splash 的两个下载中间件并调整 HttpCompressionMiddleware 的次序</span></span><br><span class="line">        <span class="string">'DOWNLOADER_MIDDLEWARES'</span>: &#123;</span><br><span class="line">            <span class="string">'ArticleSpider.middlewares.RandomUserAgentMiddleware'</span>: <span class="number">722</span>,</span><br><span class="line">            <span class="string">'scrapy_splash.SplashCookiesMiddleware'</span>: <span class="number">723</span>,</span><br><span class="line">            <span class="string">'scrapy_splash.SplashMiddleware'</span>: <span class="number">725</span>,</span><br><span class="line">            <span class="string">'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware'</span>: <span class="number">810</span>,</span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="comment"># 设置去重过滤器</span></span><br><span class="line">        <span class="string">'DUPEFILTER_CLASS'</span>: <span class="string">'scrapy_splash.SplashAwareDupeFilter'</span>,</span><br><span class="line">        <span class="comment"># 用来支持 cache_args （可选）</span></span><br><span class="line">        <span class="string">'SPIDER_MIDDLEWARES'</span>: &#123;</span><br><span class="line">            <span class="string">'scrapy_splash.SplashDeduplicateArgsMiddleware'</span>: <span class="number">100</span>,</span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="comment"># 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.146 Safari/537.36',</span></span><br><span class="line">        <span class="comment"># 'HTTPERROR_ALLOWED_CODES': [400],</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">start_requests</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># 请求第一页，无须 js 渲染</span></span><br><span class="line">        <span class="keyword">yield</span> Request(self.base_url, callback=self.parse_urls, dont_filter=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="comment"># 获取一个页面中每本书的名字和价格</span></span><br><span class="line">        <span class="keyword">for</span> sel <span class="keyword">in</span> response.css(<span class="string">'ul.gl-warp.clearfix &gt; li.gl-item'</span>):</span><br><span class="line">            <span class="keyword">yield</span> &#123;</span><br><span class="line">                <span class="string">'name'</span>: sel.css(<span class="string">'div.p-name'</span>).xpath(<span class="string">'string(.//em)'</span>).extract_first(),</span><br><span class="line">                <span class="string">'price'</span>: sel.css(<span class="string">'div.p-price i::text'</span>).extract_first(),</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_urls</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="comment"># 获取商品总数，计算出总页数</span></span><br><span class="line">        total = int(response.css(<span class="string">'span#J_resCount::text'</span>).re_first(<span class="string">'\d+'</span>))</span><br><span class="line">        pageNum = total // <span class="number">60</span> + (<span class="number">1</span> <span class="keyword">if</span> total % <span class="number">60</span> <span class="keyword">else</span> <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 构造每页的 url，向 Splash 的 execute 端点发送请求</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(pageNum):</span><br><span class="line">            url = <span class="string">'%s&amp;page=%s'</span> % (self.base_url, <span class="number">2</span> * i + <span class="number">1</span>)</span><br><span class="line">            <span class="keyword">yield</span> SplashRequest(</span><br><span class="line">                    url,</span><br><span class="line">                    endpoint=<span class="string">'execute'</span>,</span><br><span class="line">                    args=&#123;<span class="string">'lua_source'</span>: lua_script&#125;,</span><br><span class="line">                    cache_args=[<span class="string">'lua_source'</span>]</span><br><span class="line">            )</span><br></pre></td></tr></table></figure><p>解释上述代码如下：</p><ul><li>start_requests 方法</li></ul><p>start_requests 提交对第一个页面的请求，这个页面不需要渲染，因为我们只想从中获取页面总数，使用 scrapy.Request 提交请求，并指定 parse_urls 作为解析函数。</p><ul><li>parse_urls 方法</li></ul><p>从第一个页面中提取商品总数，用其计算页面总数，之后按照前面分析出的页面 url 规律构造每一个页面的 url 。这些页面都是需要渲染的，使用 SplashRequest 提交请求，除了渲染页面以外，还需要执行一段 JavaScript 代码（为了加载后 30 本书），因此使用 Splash 的 execute 端点将 endpoint 参数置为 <code>execute</code> 。通过 args 参数的 lua_source 字段传递我们要执行的 lua 脚本，由于爬取每个页面时都要执行该脚本，因此可以使用 cache_args 参数将该脚本缓存到 Splash 服务器。</p><ul><li>parse 方法</li></ul><p>一个页面中提取 60 本书的名字和价格信息。 </p><ul><li>lua_script  </li></ul><p>自定义的 lua 脚 本，其中的逻辑很简单:<br>打开页面 ==》 等待渲染 ==》 执行 js 触发数据加载（后 30 本书） ==》 等待渲染 ==》 返四 html</p><p>编码和配置的完成，运行爬虫：</p><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">(<span class="keyword">jobboleArticle) </span>$ <span class="keyword">scrapy </span>crawl <span class="keyword">jd_book </span>-o .\data\<span class="keyword">books.csv</span></span><br><span class="line"><span class="keyword"></span></span><br><span class="line"><span class="keyword"></span></span><br><span class="line"><span class="keyword">(jobboleArticle) </span>$ cat -n .\data\<span class="keyword">books.csv</span></span><br><span class="line"><span class="keyword"></span></span><br><span class="line"><span class="keyword"></span></span><br><span class="line"><span class="keyword">5928 </span> 包邮 Python Web开发实战+Python Web开发 测试驱动方法  <span class="number">2</span>本,<span class="number">153</span>.<span class="number">00</span></span><br><span class="line"><span class="number">5929</span>  包邮 Python编程导论 <span class="number">2</span>版+ Python <span class="number">3</span>学习笔记  <span class="number">2</span>本 编程程序,<span class="number">118</span>.<span class="number">00</span></span><br><span class="line"><span class="number">5930</span>  包邮数据科学家养成手册+Python大战机器学习:数据科学家的第一个小目标  <span class="number">2</span>本,<span class="number">112</span>.<span class="number">00</span></span><br><span class="line"><span class="number">5931</span>  包邮 Python参考手册（第<span class="number">4</span>版修订版）+Python数据抓取技术与实战  <span class="number">2</span>本,<span class="number">104</span>.<span class="number">00</span></span><br><span class="line"><span class="number">5932</span>  包邮  Python数据处理+Python网络数据采集 <span class="number">2</span>本 python编程入门书籍,<span class="number">115</span>.<span class="number">00</span></span><br><span class="line"><span class="number">5933</span>  包邮 Python网络编程 第<span class="number">3</span>版+Python网络数据采集  <span class="number">2</span>本,<span class="number">103</span>.<span class="number">00</span></span><br><span class="line"><span class="number">5934</span>  包邮Metasploit渗透测试指南（修订版）+Python黑帽子:黑客与渗透测试编程之道,<span class="number">101</span>.<span class="number">00</span></span><br><span class="line"><span class="number">5935</span>  包邮 Python性能分析与优化+Python算法教程  <span class="number">2</span>本,<span class="number">85</span>.<span class="number">00</span></span><br><span class="line"><span class="number">5936</span>  包邮 数据科学实战手册（R+Python）+干净的数据:数据清洗入门与实践 <span class="number">2</span>本,<span class="number">80</span>.<span class="number">00</span></span><br><span class="line"><span class="number">5937</span>  包邮 Web接口开发与自动化测试——基于Python语言+软件自动化测试开发  <span class="number">2</span>本,<span class="number">88</span>.<span class="number">00</span></span><br><span class="line"><span class="number">5938</span>  零起点Python大数据与量化交易 何海群 <span class="number">9787121306594</span>,<span class="number">74</span>.<span class="number">30</span></span><br><span class="line"><span class="number">5939</span>  从Python开始学编程,<span class="number">29</span>.<span class="number">40</span></span><br><span class="line"><span class="number">5940</span>  区域包邮 Python游戏编程入门+Maya Python 游戏与影视编程指南  <span class="number">2</span>本,<span class="number">96</span>.<span class="number">00</span></span><br><span class="line"><span class="number">5941</span>  包邮  Python与机器学习实战+TensorFlow技术解析与实战 <span class="number">2</span>本,<span class="number">112</span>.<span class="number">00</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;blockquote class=&quot;blockquote-center&quot;&gt;自己不牛逼，认识的人再多也没用&lt;/blockquote&gt;

&lt;h5 id=&quot;爬取动态页面&quot;&gt;&lt;a href=&quot;#爬取动态页面&quot; class=&quot;headerlink&quot; title=&quot;爬取动态页面&quot;&gt;&lt;/a&gt;爬取动态页面&lt;/h5&gt;&lt;p&gt;静态页面的内容始终不变，爬取相对容易，但在现实中，日前绝大多数网站的页面都是动态页面。动态页面中的部分内容是浏览器运行页面中的 JavaScript 脚本动态生成的，爬取相对困难。&lt;br&gt;先来看一个简单的动态页面的例子，在浏览器中打开 &lt;a href=&quot;http://quotes.toscrape.com/js/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Quotes&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://dongfei.oss-cn-shanghai.aliyuncs.com/blog/quotes.jpg&quot; alt=&quot;quotes&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Python" scheme="http://blog.dongfei.xin/categories/Python/"/>
    
      <category term="Scrapy" scheme="http://blog.dongfei.xin/categories/Python/Scrapy/"/>
    
    
      <category term="Scrapy" scheme="http://blog.dongfei.xin/tags/Scrapy/"/>
    
  </entry>
  
  <entry>
    <title>精通 Scrapy 网络爬虫（模拟登陆篇）</title>
    <link href="http://blog.dongfei.xin/2018-03-30/%E7%B2%BE%E9%80%9A%20Scrapy%20%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%EF%BC%88%E6%A8%A1%E6%8B%9F%E7%99%BB%E9%99%86%E7%AF%87%EF%BC%89/"/>
    <id>http://blog.dongfei.xin/2018-03-30/精通 Scrapy 网络爬虫（模拟登陆篇）/</id>
    <published>2018-03-30T10:33:10.000Z</published>
    <updated>2018-04-12T11:22:46.770Z</updated>
    
    <content type="html"><![CDATA[<blockquote class="blockquote-center" style="border: ">没事了，约约几个老伙计，喝喝茶，下下棋<br></blockquote><p><img src="https://dongfei.oss-cn-shanghai.aliyuncs.com/Example%20web%20scraping%20website%20.jpg" alt="http://example.webscraping.com/"></p><a id="more"></a><h5 id="模拟登陆"><a href="#模拟登陆" class="headerlink" title="模拟登陆"></a>模拟登陆</h5><p>登录的实质是向服务器发送含有登录表单数据的 HTTP 请求（通常是 POST）</p><h6 id="使用-FromRequest"><a href="#使用-FromRequest" class="headerlink" title="使用 FromRequest"></a>使用 FromRequest</h6><p>Scrapy 提供了一个 FormRequest 类（Request的子类），专门用于构造含有表单数据的请求，FormRequest 的构造器方法有一个formdata 参数，接收字典形式的表单数据</p><p>在scrapy shell 环境下演示如何使用FormRequest 模拟登录。首先爬取登录页面：<a href="http://example.webscraping.com/places/default/user/login?_next=/places/default/index" target="_blank" rel="noopener">链接</a></p><p><img src="https://dongfei.oss-cn-shanghai.aliyuncs.com/Example%20web%20scraping%20website%20123.jpg" alt=""></p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;</span>&gt; scrapy shell <span class="symbol">http:</span>/<span class="regexp">/example.webscraping.com/places</span><span class="regexp">/default/user</span><span class="regexp">/login?_next=/places</span><span class="regexp">/default/index</span></span><br><span class="line"></span><br><span class="line">表单数据应包含的信息：帐号和密码信息，再加 <span class="number">3</span>个隐藏 &lt;input&gt; 中的信息（&lt;div style=<span class="string">"display:none"</span>）。</span><br><span class="line">先把这些信息收集到一个字典中，然后使用这个表单数据字典构造 FormRequest 对象</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; sel = response.xpath(<span class="string">'//div[@style]/input'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; sel</span><br><span class="line">[&lt;Selector xpath=<span class="string">'//div[@style]/input'</span> data=<span class="string">'&lt;input name="_next" type="hidden" value='</span>&gt;，</span><br><span class="line">&lt;Selector xpath=<span class="string">'//div[@style]/input'</span> data=<span class="string">'&lt;input name="_formkey" type="hidden" value='</span>&gt;,</span><br><span class="line">&lt;Selector xpathe=<span class="string">'//div[@style]/input'</span> data=<span class="string">'&lt;input name="_formname" type="hidden" value='</span>&gt;]</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt;<span class="comment"># 构造表单数据字典</span></span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; fd = dict(zip(sel.xpath(<span class="string">'./@name'</span>).extract(),sel.xpath(<span class="string">'./@value'</span>).extract()))</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; fd</span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">'_formkey'</span>: <span class="string">'432dcb0c-0d8s443fbb50-9644cft212b'</span>,</span><br><span class="line">    <span class="string">'_formname'</span>: <span class="string">'login'</span>,</span><br><span class="line">    <span class="string">'_next'</span>: <span class="string">'/'</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; <span class="comment"># 填写账号和密码信息</span></span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; fd[<span class="string">"email"</span>] = <span class="string">"3543503058@qq.com"</span></span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; fd[<span class="string">"password"</span>] = <span class="string">"webscraping.com"</span></span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; fd</span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">'_formkey'</span>: <span class="string">'432dcb0c-0d8s443fbb50-9644cft212b'</span>,</span><br><span class="line">    <span class="string">'_formname'</span>: <span class="string">'login'</span>,</span><br><span class="line">    <span class="string">'_next'</span>: <span class="string">'/'</span>,</span><br><span class="line">    <span class="string">'email'</span>: <span class="string">'3543503058@qq.com'</span>,</span><br><span class="line">    <span class="string">'password'</span>: <span class="string">'webscraping.com'</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; from scrapy.http import FormRequest</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; request = FormRequest(<span class="string">'http://example.webscraping.com/places/default/user/login?_next=/places/default/index'</span>, formdata = fd)</span><br></pre></td></tr></table></figure><p>以上指直接构造 FormRequest 对象的方式，除此之外还有一种更为简单的方式，即调用 FormRequest 的 from_response 方法，调用时需要传入一个 Response 对象作为第一参数，该方法会解析 Response 对象所包含页面中的 <code>&lt;form&gt;</code> 元素，帮助用户创建 FromRequest 对象，并将隐藏 <code>&lt;input&gt;</code> 中的信息自动填入表单数据，使用这种方式，我们只需通过 formdata 参数填写账号和密码即可，代码如下：</p><figure class="highlight xquery"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; scrapy shell http://example.webscraping.com/places/<span class="keyword">default</span>/user/login?_<span class="keyword">next</span>=/places/<span class="keyword">default</span>/index</span><br><span class="line">&gt;&gt;&gt; from scrapy.http <span class="keyword">import</span> FormRequest</span><br><span class="line">&gt;&gt;&gt; fd = &#123;<span class="string">'email'</span>:<span class="string">"3543503058@qq.com"</span>, <span class="string">'password'</span>:<span class="string">"webscraping.com"</span>&#125;</span><br><span class="line">&gt;&gt;&gt; request = FormRequest.from_response(response, formdata = fd )</span><br><span class="line">&gt;&gt;&gt; fetch(request)</span><br><span class="line"><span class="number">2018</span>-<span class="number">03</span>-<span class="number">30</span> <span class="number">21</span>:<span class="number">16</span>:<span class="number">51</span> [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (<span class="number">303</span>) <span class="keyword">to</span> &lt;GET http://example.webscraping.com/places/<span class="keyword">default</span>/index&gt; from &lt;POST http://example.webscraping.com/places/<span class="keyword">default</span>/user/login?_<span class="keyword">next</span>=<span class="meta">%2Fplaces</span><span class="meta">%2Fdefault</span><span class="meta">%2Findex</span>&gt;</span><br><span class="line"><span class="number">2018</span>-<span class="number">03</span>-<span class="number">30</span> <span class="number">21</span>:<span class="number">16</span>:<span class="number">51</span> [scrapy.core.engine] DEBUG: Crawled (<span class="number">200</span>) &lt;GET http://example.webscraping.com/places/<span class="keyword">default</span>/index&gt; (referer: None)</span><br></pre></td></tr></table></figure><p>在 log 信息中，可以看到和浏览器提交表单时类似的情形：POST 请求的响应状态码为 303，之后 Scrapy 自动再发送下一个 GET 请求下载跳转页面。此时，可以通过在页面中查找特殊字符串或在浏览器中查看页面是否登录成功。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">'Welcome jason'</span> <span class="keyword">in</span> response.text</span><br><span class="line"><span class="keyword">True</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>view(response)</span><br><span class="line"><span class="keyword">True</span></span><br></pre></td></tr></table></figure><p>验证结果表明模拟登录成功了。显然，Scrapy 发送的第 2 个 GET 请求携带了第 1 个 POST 请求获取的 Cookie 信息，为请求附加 Cookie 信息的工作是由 Scrapy 内置的下载中间件 CookiesMiddleware 自动完成的。现在，我们可以继续发送请求，爬取那些只有登录后才能获取的信息了，这里以爬取用户个人信息为例：</p><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; fetch(<span class="string">"http://example.webscraping.com/places/default/user/profile"</span>)</span><br><span class="line"><span class="number">2018</span><span class="number">-03</span><span class="number">-30</span> <span class="number">21</span>:<span class="number">31</span>:<span class="number">04</span> [scrapy.core.engine] <span class="type">DEBUG</span>: <span class="type">Crawled</span> (<span class="number">200</span>) &lt;<span class="type">GET</span> http://example.webscraping.com/places/<span class="keyword">default</span>/user/profile&gt; (<span class="title">referer</span>: <span class="type">None</span>)</span><br><span class="line">&gt;&gt;&gt; view(response)</span><br><span class="line"><span class="type">True</span></span><br></pre></td></tr></table></figure><p><img src="https://dongfei.oss-cn-shanghai.aliyuncs.com/Example%20web%20scraping%20website%2012.jpg" alt="个人信息"></p><p><strong>实现登录 LoginSpider</strong></p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line">__author_<span class="number">_</span> = <span class="string">"東飛"</span></span><br><span class="line">__date_<span class="number">_</span> = <span class="string">"2017-11-29"</span></span><br><span class="line"></span><br><span class="line">import scrapy</span><br><span class="line">from scrapy.http import Request, FormRequest</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LoginSpider</span>(<span class="title">scrapy</span>.<span class="title">Spider</span>):</span></span><br><span class="line">    name = <span class="string">'login'</span></span><br><span class="line">    allowed_domains = [<span class="string">'example.webscraping.com'</span>]</span><br><span class="line">    start_urls = [<span class="string">'http://example.webscraping.com/places/default/user/profile'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(<span class="keyword">self</span>, response)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="comment"># 解析登录后下载的页面，此例中为用户个人信息页面</span></span><br><span class="line">        keys = response.css(<span class="string">'table label::text'</span>).re(<span class="string">'(.+):'</span>)</span><br><span class="line">        values = response.css(<span class="string">'table td.w2p_fw::text'</span>).extract()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">yield</span> dict(zip(keys, values))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># ------------------------登录-----------------------</span></span><br><span class="line">    <span class="comment"># 登录页面的url</span></span><br><span class="line">    login_url = <span class="string">'http://example.webscraping.com/places/default/user/login?_next=/places/default/index'</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">start_requests</span><span class="params">(<span class="keyword">self</span>)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">yield</span> Request(<span class="keyword">self</span>.login_url, callback=<span class="keyword">self</span>.login)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">login</span><span class="params">(<span class="keyword">self</span>, response)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="comment"># 登录页面的解析函数，构造FormRequest 对象提交表单</span></span><br><span class="line">        fd = &#123;</span><br><span class="line">            <span class="string">'email'</span>: <span class="string">"3543503058@qq.com"</span>,</span><br><span class="line">            <span class="string">'password'</span>: <span class="string">"webscraping.com"</span></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">yield</span> FormRequest.from_response(</span><br><span class="line">                response,</span><br><span class="line">                formdata=fd,</span><br><span class="line">                callback=<span class="keyword">self</span>.parse_login</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_login</span><span class="params">(<span class="keyword">self</span>, response)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="comment"># 登录成功后，继续爬取 start_urls 中的页面</span></span><br><span class="line">        <span class="keyword">if</span> <span class="string">'Welcome jason'</span> <span class="keyword">in</span> response.<span class="symbol">text:</span></span><br><span class="line">            <span class="keyword">yield</span> from <span class="keyword">super</span>().start_requests()  <span class="comment"># Python 3 语法</span></span><br></pre></td></tr></table></figure><p>解释上述代马如下:</p><ul><li><p>覆写基类的 start_requests 方法，最先请求登录页面。</p></li><li><p>login 方法为登录页面的解析函数，在该方法中进行模拟登录，构造表单请求并提交。</p></li><li><p>parse_login 方法为表单请求的响应处理函数，在该方法中通过在页面查找特殊字符串 ‘Welcome jason’ 判断是否登录成功，如果成功，调用基类的 start_requests 方法，继续爬取 start_urls 中的页面</p></li></ul><h5 id="识别验证码"><a href="#识别验证码" class="headerlink" title="识别验证码"></a>识别验证码</h5><h6 id="OCR识别"><a href="#OCR识别" class="headerlink" title="OCR识别"></a>OCR识别</h6><p>OCR 是光学字符识别的缩写，用于在图像中提取文本信息，tesseract-ocr 是利用该技术实现的一个验证码识别库，在 Python 中可以通过第三方库 pytesseract 调用它。下面介绍如何使用 pytesseract 识别验证码：<br><img src="https://dongfei.oss-cn-shanghai.aliyuncs.com/cqke.jpg" alt="cqke"></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">首先安装 tesseract-ocr,在Ubuntu 下可以使用apt-get 安装:</span><br><span class="line">sudo apt-get install tesseract-ocr</span><br><span class="line"></span><br><span class="line">安装 pytesseract，依赖于 Python 图像处理库 PIL 或 Pillow PIL （PIL 和 Pillow PIL 功能类似，任选其一）。</span><br><span class="line">使用 pip 安装:</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> pip install pillow</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> pip install pytesseract</span></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; from PIL import Image</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; import pytesseract</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; img = Image.open(<span class="string">'code.png'</span>)</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; img = img.convert(<span class="string">'L'</span>)</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; pytesseract.image_tsti</span></span><br><span class="line">'cqKE'</span><br></pre></td></tr></table></figure><p>上面的代码中，先使用 Image.open 打开图片，为了提高识别率，调用 Image 对象的 convert 方法把图片转换为黑白图，然后将黑白图传递给 pytesseract.image_to_string 方法进行识别，这里我们幸运地识别成功了。经测试，此段代码对于 X 网站中的验证码识别率可以达到 72%，这已经足够高了。</p><p>下面我们以之前的 LoginSpider 为模板实现一个使用 pytesseract 识别验证码登录的 Spider：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line">__author__ = <span class="string">"東飛"</span></span><br><span class="line">__date__ = <span class="string">"2017-11-29"</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> Request, FormRequest</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> io <span class="keyword">import</span> BytesIO</span><br><span class="line"><span class="keyword">import</span> pytesseract</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> scrapy.log <span class="keyword">import</span> logger</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CaptchaLoginSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">"login_captcha"</span></span><br><span class="line">    start_urls = [<span class="string">'http://XXX.com/'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">    <span class="comment"># X 网站登录页面的 url (虚构的)</span></span><br><span class="line">    login_url = <span class="string">'http://XXX.com/login'</span></span><br><span class="line">    user = <span class="string">'jasonme@XXX.com'</span></span><br><span class="line">    password = <span class="string">'12345678'</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">start_requests</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">yield</span> Request(self.login_url, callback=self.login, dont_filter=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">login</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="comment"># 该方法既是登录页面的解析函数，又是下载验证码图片的响应处理函数</span></span><br><span class="line">        <span class="comment"># 如果 response.meta['login_response'] 存在，当前 response 为验证码图片的响应</span></span><br><span class="line">        <span class="comment"># 否则当前 response 为登录页面的响应</span></span><br><span class="line">        login_response = response.meta.get[<span class="string">'login_response'</span>]</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> login_response:</span><br><span class="line">            <span class="comment"># Step 1:</span></span><br><span class="line">            <span class="comment"># 此时 response 为登录页面的响应，从中提取验证码图片的 url，下载验证码图片</span></span><br><span class="line"></span><br><span class="line">            captchaUrl = response.css(<span class="string">'label.field.prepend-icon img::attr(src)'</span>).extract_first()</span><br><span class="line"></span><br><span class="line">            captchaUrl = response.urljoin(captchaUrl)</span><br><span class="line">            <span class="comment"># 构造 Request 时，将当前 response 保存到 meta 字典中</span></span><br><span class="line">            <span class="keyword">yield</span> Request(</span><br><span class="line">                    captchaUrl,</span><br><span class="line">                    callback=self.login,</span><br><span class="line">                    meta=&#123;</span><br><span class="line">                        login_response: response</span><br><span class="line">                    &#125;,</span><br><span class="line">                    dont_filter=<span class="keyword">True</span></span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># Step 2:</span></span><br><span class="line">            <span class="comment"># 此时，response 为验证码图片的响应，response.body 是图片二进制数据</span></span><br><span class="line">            <span class="comment"># login_response 为登录页面的响应，用其构造表单请求并发送</span></span><br><span class="line">            formdata = &#123;</span><br><span class="line">                <span class="string">'email'</span>: self.user,</span><br><span class="line">                <span class="string">'pass'</span>: self.password,</span><br><span class="line">                <span class="comment"># 使用OCR 识别</span></span><br><span class="line">                <span class="string">'code'</span>: self.get_captcha_by_OCR(response.body),</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">yield</span> FormRequest.from_response(</span><br><span class="line">                    login_response,</span><br><span class="line">                    callback=self.parse_login,</span><br><span class="line">                    formdata=formdata,</span><br><span class="line">                    dont_filter = <span class="keyword">True</span></span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_login</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="comment"># 根据响应结果判断是否登录成功</span></span><br><span class="line">        info = json.loads(response.text)</span><br><span class="line">        <span class="keyword">if</span> info[<span class="string">'error'</span>] == <span class="string">'0'</span>:</span><br><span class="line">            logger.info(<span class="string">'登录成功:-)'</span>)</span><br><span class="line">        <span class="keyword">return</span> super().start_requests()</span><br><span class="line"></span><br><span class="line">        logger.info(<span class="string">'登录失败:-(, 重新登录....'</span>)</span><br><span class="line">        <span class="keyword">return</span> self.start_requests()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_captcha_by_OCR</span><span class="params">(self, data)</span>:</span></span><br><span class="line">        <span class="comment"># OCR 识别</span></span><br><span class="line">        img = Image.open(BytesIO(data))</span><br><span class="line">        img = img.convert(<span class="string">'L'</span>)</span><br><span class="line">        captcha = pytesseract.image_to_string(img)</span><br><span class="line">        img.close()</span><br><span class="line">        <span class="keyword">return</span> captcha</span><br></pre></td></tr></table></figure><p>解释上述代码如下:</p><ul><li>login 方法</li></ul><p>带有登证玛的登录，需要额外发送一个 HTTP 请求来获取验证码图片，这里的 login 方法既处理下载登录页面的响应，又处理下载验证码图片的响应。</p><p>解析登录页面时，提取验证码图片的 url ，发送请求下载图片，并将登录页面的<br> Response 对象保存到 Request 对象的 meta 字典中。</p><p>处理下载验证码图片的响应时，调用 get_captcha_by_OCR 方法识别图片中的验证码，然后将之前保存的登录页面的 Response 对象取出，构造 FormRequest 对象并提交。</p><ul><li>get_captcha_by_OCR 方法</li></ul><p>参教 data 是验证码图片的二进制数据，类型为 bytes ，想要使用 Image.open 函数构造 Image 对象先要把图片的二进制数据转换成某种类文件对象，这里使用 BytesIO<br>进行包裹，获得 Image 对象后先将其转换成黑白图，然后调用 pytesseract.image_to_string 方法进行识别。</p><ul><li>parse_login 方法</li></ul><p>处理表单请求的响应。响应正文是一个 json 串，其中包含了用户验证的结果，先调用 json.loads 将正文转换为 Python 宇典，然后依据其中 error 字段的值判断登录是否<br>成功，若登录成功，则从 start_urls 中的页面开始爬取；若登录失败，则重新登录。</p><h6 id="网络平台识别"><a href="#网络平台识别" class="headerlink" title="网络平台识别"></a>网络平台识别</h6><p>使用 pytesseract 识别的验证码比较简单，对于某些复杂的验证码，pytesseract 的识别率很低或者无法识别。目前，有很多网站专门提供验证码识别服务，可以识别较为复杂的验证码（有些是人工处理的），它们被称之为验证码识别平台，这平台多数是付费使用的，价格大约为 1元钱识别 100 个验证码，平台提供了 HTTP 服务接口，用户可以通过 HTTP 请求将验证码图片发送给平台，平台识别后将结果通过 HTTP 响应返回响应务接口，</p><p>在阿里云市场可以找到很多验证码识别平台：<br><img src="https://dongfei.oss-cn-shanghai.aliyuncs.com/sdcsd.jpg" alt="验证码识别平台"></p><p><img src="https://dongfei.oss-cn-shanghai.aliyuncs.com/sdcsd.jpg" alt="验证码识别平台"></p><p>阅读 API 文档，实现代码如下：</p><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">import urllib, urllib2, sys</span><br><span class="line"></span><br><span class="line">host = 'http://ali-make-mark.showapi.com'</span><br><span class="line">path = '/make-mark-img'</span><br><span class="line">method = 'GET'</span><br><span class="line">appcode = '你自己的AppCode'</span><br><span class="line">querys = 'border=yes&amp;border_color=105%2C179%2C90&amp;border_thickness=1&amp;image_height=50&amp;image_width=200&amp;noise_color=black&amp;obscurificator_impl=com.google.code.kaptcha.impl.WaterRipple&amp;textproducer_char_length=5&amp;textproducer_char_space=2&amp;textproducer_char_string=abcde2345678gfynmnpwx&amp;textproducer_font_color=black&amp;textproducer_font_names=Arial%2C+Courier&amp;textproducer_font_size=40'</span><br><span class="line">bodys = &#123;&#125;</span><br><span class="line">url = host + path + '?' + querys</span><br><span class="line"></span><br><span class="line">request = urllib2.Request(url)</span><br><span class="line">request.add_header('Authorization', 'APPCODE ' + appcode)</span><br><span class="line">response = urllib2.urlopen(request)</span><br><span class="line">content = response.read()</span><br><span class="line">if (content):</span><br><span class="line">    print(content)</span><br></pre></td></tr></table></figure><h6 id="人工识别"><a href="#人工识别" class="headerlink" title="人工识别"></a>人工识别</h6><p>通常网站只需登录一次便可爬取，在其他识别方式不管用时，人工识别一次验证码也是可行的，其实现也非常简单————在 Scrapy 下载完验证码图片后，调用 Image.show 方法将图片显示出来，然后调用 Python 内置的 input 函数，等待用户肉眼识别后输入识别结果。<br>实现三种方式整体代码如下：</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CaptchaLoginspider</span>(<span class="title">scrapy</span>.<span class="title">Spider</span>):</span></span><br><span class="line"></span><br><span class="line">    .....</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_captcha_by_OCR</span><span class="params">(data)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="comment"># OCR 识别</span></span><br><span class="line">        img = Image.open(BytesIO(data))</span><br><span class="line">        img = img.convert(<span class="string">'L'</span>)</span><br><span class="line">        captcha = pytesseract.image_to_string(img)</span><br><span class="line">        img.close()</span><br><span class="line">        <span class="keyword">return</span> captcha</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_captcha_by_network</span><span class="params">(<span class="keyword">self</span>, data)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="comment"># 平台识别</span></span><br><span class="line">        import requests</span><br><span class="line"></span><br><span class="line">        url = <span class="string">"http://ali-checkcode.showapi.com/checkcode"</span></span><br><span class="line">        appcode = <span class="string">'f23cca37f287418a90e2f9226492734"</span></span><br><span class="line"><span class="string">        form = &#123;&#125;</span></span><br><span class="line"><span class="string">        form['</span>convert_to_jpg<span class="string">'] = '</span><span class="number">0</span><span class="string">'</span></span><br><span class="line"><span class="string">        form['</span>img_base64<span class="string">'] = base64.b64encode(data)</span></span><br><span class="line"><span class="string">        form['</span>typeId<span class="string">'] = '</span><span class="number">3040</span><span class="string">'</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        headers = &#123;'</span>Authorization<span class="string">': '</span>APPCODE<span class="string">' + appcode&#125;</span></span><br><span class="line"><span class="string">        response = requests.post(url, headers=headers, data=form&#125;</span></span><br><span class="line"><span class="string">        res = response.json()</span></span><br><span class="line"><span class="string">        if res['</span>showapi_res_body<span class="string">'] == 0:</span></span><br><span class="line"><span class="string">            return res['</span>showapi_res_body<span class="string">']['</span>Result<span class="string">']</span></span><br><span class="line"><span class="string">        return '</span><span class="string">'</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    def getcaptcha_by_user(self, data):</span></span><br><span class="line"><span class="string">        # 人工识别</span></span><br><span class="line"><span class="string">        img = Image.open(BytesIO(data))</span></span><br><span class="line"><span class="string">        img.show()</span></span><br><span class="line"><span class="string">        captcha = input("输入验证码:")</span></span><br><span class="line"><span class="string">        img.close()</span></span><br><span class="line"><span class="string">        return captcha</span></span><br></pre></td></tr></table></figure><h5 id="Cookie-登录"><a href="#Cookie-登录" class="headerlink" title="Cookie 登录"></a>Cookie 登录</h5><p>目前有的网站验证码越来越复杂，已经复杂到人类难以识别的程度，有些时候提交表单登录的路子难以走通。此时，我们可以换一种登录爬取的思路，在使用浏览器登录网站后，包含用户身份信息的 Cookie 会被浏览器保存在本地，如果 Scrapy 爬虫能直接使用浏览器中的 Cookie 发送 HTTP 请求，就可以绕过提交表单登录的步骤。</p><h6 id="获取浏览器-Cookie"><a href="#获取浏览器-Cookie" class="headerlink" title="获取浏览器 Cookie"></a>获取浏览器 Cookie</h6><p>使用第三方 Python 库 browsercookie 可以获取 Chrome 和 Firefox 浏览器 中的 Cookie。使用 pip 安装 browsercookie<br><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip <span class="keyword">install</span> browsercookie</span><br></pre></td></tr></table></figure></p><p>需要安装 pycrypto 依赖，默认不支持 Python3 版本，解决参考：<a href="https://blog.csdn.net/a624806998/article/details/78596543" target="_blank" rel="noopener">https://blog.csdn.net/a624806998/article/details/78596543</a></p><p>browsercookie 的使用非常简单，示例代码如下:<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; import browsercookie</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; chrome_cookiejar = browsercookie.chrome()  <span class="comment"># 获取 Chrome 浏览器中的 Cookie</span></span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; firefox_cookiejar = browsercookie.firefox()   <span class="comment"># 获取 Firefox 浏览器中的 Cookie</span></span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; <span class="built_in">type</span>(chrome_cookiejar)</span></span><br><span class="line">&lt;class 'http.cookiejar.CookieJar'&gt;</span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; from pprint import pprint</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; <span class="keyword">for</span> cookie <span class="keyword">in</span> chrome_cookiejar:</span></span><br><span class="line">...     pprint(cookie)</span><br></pre></td></tr></table></figure></p><p>browsercookie 的 chrome 和 firefox 方法分别返回 Chomne 和 Firefox 浏览器中的<br>cookie，返回值是一个 http.cookiejar.CookieJar 对象，对 CookieJar 对象进行选代，可以访问其中的每个 Cookie 对象。</p><h6 id="CookiesMiddleware-源码分析"><a href="#CookiesMiddleware-源码分析" class="headerlink" title="CookiesMiddleware 源码分析"></a>CookiesMiddleware 源码分析</h6><p><strong>源码：Lib/site-packages/scrapy/downloadermiddlewares/cookies.py</strong> </p><p>Scrapy 爬虫所使用的 Cookie 由内置下载中间件 CookiesMiddleware<br>自动处理。</p><p>其中几个核心方法如下：</p><ul><li>from_crawler 方法</li></ul><p>从配置文件中读取 COOKIES ENABLED，决定是否启用该中间件。如果启用，调<br>用构造器创建对象，否则抛出 NotConfigured 异常，Scrapy 将忽略该中间件。</p><ul><li>__init__ 方法</li></ul><p>使用标准库中的 collections.defaultdict 创建一个默认字典 self.jar，该字典中每一项的值都是一个 scrapy.http.cookies.CookieJar 对象，CookiesMiddleware 可以让 Scrapy 爬虫同时使用多个不同的 Cookiclar，<br>例如，在某网站我们注册了两个账号 account1 和 account2，假设一个爬虫想同时登录两个账号对网站进行爬取，为了<br>了避免 Cookie 冲突（通俗地讲，登录一个会替换掉另一个），此时可以让每个账号发送的 HTTP 请求使用不同的 CookieJar，在构造 Request 对象时，可以通过 meta 参數的。<br>cookieJar 字段指定所要使用的 CookieJar，如：<br><figure class="highlight vala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># 账号 account1 发送的请求</span></span><br><span class="line">Request(url1, meta=&#123;<span class="string">'cookiejar'</span>:<span class="string">'account1'</span>&#125;)</span><br><span class="line">Request(url2, meta=&#123;<span class="string">'cookiejar'</span>:<span class="string">'account1'</span>&#125;)</span><br><span class="line">Request(url3, meta=&#123;<span class="string">'cookiejar'</span>:<span class="string">'account1'</span>&#125;)  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta"># 账号 account2 发送的请求</span></span><br><span class="line">Request(url1, meta=&#123;<span class="string">'cookiejar'</span>:<span class="string">'account2'</span>&#125;)</span><br><span class="line">Request(url2, meta=&#123;<span class="string">'cookiejar'</span>:<span class="string">'account2'</span>&#125;)</span><br><span class="line">Request(url3, meta=&#123;<span class="string">'cookiejar'</span>:<span class="string">'account2'</span>&#125;)</span><br></pre></td></tr></table></figure></p><ul><li>process_request 方法</li></ul><p>处理每一个待发送的 Request 对象，尝试从 request.meta[‘cookiejar’]获取用户指定使<br>用的 CookieJar，如果用户未指定，就使用默认的 CookieJar(self.jars[None]) 。调用<br>self._get_request_cookies 方法获取发送请求 request 应携带的 Cookie 信息，填写到 HTTP 请求头部。</p><ul><li>process_response 方法</li></ul><p>处理每一个 Response 对象，依然通过 request.meta[‘cookiejar’] 获取 Cookielar 对象，调用 extract_cookies 方法将 HTTP 响应头部中的 Cookie 信息保存到 CookieJar 对象中。</p><p>另外需要注意的是，这里的 CookieJar 是 scrapy.http.cookies.CookieJar 而在之前通过 browsercookie 获取浏览器中的 CookieJar 是标准库中的 http.cookiejar.CookieJar，它们是不同的类，前者对后者进行了包装，两者可以相互转化。</p><h6 id="实现-BrowserCookiesMiddleware"><a href="#实现-BrowserCookiesMiddleware" class="headerlink" title="实现 BrowserCookiesMiddleware"></a>实现 BrowserCookiesMiddleware</h6><p>CookiesMiddleware 自动处理 Cookie 的特性给用户提供了便利，但它不能使用浏览器的 Cookie，我们可以利用 browsercookie 进行改良，实现使用浏览器 Cookie 的中间件，代码如下：</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line">__author_<span class="number">_</span> = <span class="string">"東飛"</span></span><br><span class="line">__date_<span class="number">_</span> = <span class="string">"2017-11-29"</span></span><br><span class="line"></span><br><span class="line">import browsercookie</span><br><span class="line">from scrapy.downloadermiddlewares.cookies import CookiesMiddleware</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BrowserCookiesMiddleware</span>(<span class="title">CookiesMiddleware</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(<span class="keyword">self</span>, debug=False)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">super</span>().__init_<span class="number">_</span>(debug)</span><br><span class="line">        <span class="keyword">self</span>.load_browser_cookies()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">load_browser_cookies</span><span class="params">(<span class="keyword">self</span>)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="comment"># 加载 Chrome 浏览器中的 Cookie</span></span><br><span class="line">        jar = <span class="keyword">self</span>.jars[<span class="string">'chrome'</span>]</span><br><span class="line">        chrome_cookiejar = browsercookie.chrome()</span><br><span class="line">        <span class="keyword">for</span> cookie <span class="keyword">in</span> <span class="symbol">chrome_cookiejar:</span></span><br><span class="line">            jar.set_cookie(cookie)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 加载 Firefox 浏览器中的 Cookie</span></span><br><span class="line">        firefox_cookiejar = browsercookie.firefox()</span><br><span class="line">        jar = <span class="keyword">self</span>.jars[<span class="string">'firefox'</span>]</span><br><span class="line">        <span class="keyword">for</span> cookie <span class="keyword">in</span> <span class="symbol">firefox_cookiejar:</span></span><br><span class="line">            jar.set_cookie(cookie)</span><br></pre></td></tr></table></figure><p>了解了 CookiesMiddleware 的工作原理，便不难理解 BrowserCookiesMiddleware 的<br>实现了，其核心思想是：在构造 BrowserCookiesMiddleware 对象时，使用 browsercookie 将浏览器中的 Cookie 提取，存储到 CookieJar 字典 self.jars 中，解释代码如下:</p><ul><li><p>继承 CookiesMiddleware 并实现构造器方法，在构造器方法中先调用基类的构造器<br>方法，然后调用 self.load_browser_cookies 方法加载浏览器 Cookie。</p></li><li><p>在 load_browser_cookies 方法中，使用 self.jars[‘chrome’] 和 self.jars[‘firefox’] 从默认字典中获得两个 CookieJar 对象，然后调用 chrome 和 firefox 方法，分别获取两个浏览器中的 Cookie，将它们填入各自的 CookieJar对象中。</p></li></ul><h6 id="爬取知乎个人信息"><a href="#爬取知乎个人信息" class="headerlink" title="爬取知乎个人信息"></a>爬取知乎个人信息</h6><p>通过使用 BrowserCookiesMiddleware 获取知乎用户的个人信息<br><img src="https://dongfei.oss-cn-shanghai.aliyuncs.com/%E7%9F%A5%E4%B9%8E%E4%B9%8B.jpg" alt="zhihuzhi"></p><p>将 BrowserCookiesMiddleware 源码复制到项目下的 middlewares.py 中，<br>在配置文件 settings.py 中添加如下配置：</p><figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># 伪装成常规浏览器</span></span><br><span class="line">USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.<span class="number">3325.14</span>6 Safari/537.36'</span><br><span class="line"></span><br><span class="line">用 BrowserCookiesMiddleware 替代 CookiesMiddleware 启用前者，关团后者</span><br><span class="line"></span><br><span class="line">DOWNLOADER_MIDDLEWARES = &#123;</span><br><span class="line">    'ArticleSpider.middlewares.BrowserCookiesMiddleware': <span class="number">701</span>,</span><br><span class="line">    'scrapy.downloadermiddlewares.cookies.CookiesMiddleware': None,    </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>由于需求非常简单，因此不再编写 Spider，直接在scrapy shell 环境中进行演示。意，为了使用项目中的配置，需要在 <strong>项目目录</strong>下启动scrapy shell 命令:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">F:\VirtualEnv\jobboleArticle\ArticleSpider</span><br><span class="line">(jobboleArticle) $ scrapy shell</span><br><span class="line">............</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; from scrapy import Request</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; url = <span class="string">"https://www.zhihu.com/settings/profile"</span></span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; fetch(Request(url,meta=&#123;<span class="string">'cookiejar'</span>:<span class="string">'chrome'</span>&#125;))</span></span><br><span class="line">2018-03-31 15:28:23 [scrapy.core.engine] INFO: Spider opened</span><br><span class="line">2018-03-31 15:28:23 [scrapy.downloadermiddlewares.cookies] DEBUG: Sending cookies to: &lt;GET https://www.zhihu.com/settings/profile&gt;</span><br><span class="line">Cookie: __DAYU_PP=Izvyf3MInbqQ6AzMZNeM2826864c6ffb; __DAYU_PP=Yee7A2aUjvjMIZEJfQYz2db6dcd51be2; __utma=51854390.1271732890.1522479670.1522479670.1522479670.1; __utmb=51854390.0.10.1522479670; __utmv=51854390.100-1|2=registration_date=20151218=1^3=entry_date=20151218=1; __utmz=51854390.1522479670.1.1.utmcsr=zhihu.com|utmccn=(referral)|utmcmd=referral|utmcct=/people/geng-dong-fei/activities; _zap=ae9aa6a5-49bc-4480-a722-e6be1565a91c; d_c0="AGDC4NfPzwyPTrS7n_0PM5L0qcPMAK_vqyc=|1512884449"; q_c1=166834564cdd4bf79f4d349fcb16e920|1521010814000|1512884448000; z_c0="2|1:0|10:1514289890|4:z_c0|92:Mi4xV1Zoa0FnQUFBQUFBWU1MZzE4X1BEQ1lBQUFCZ0FsVk40b2d2V3dEMS16cFo0MXlmNlVVbzE5N2JZNlZGTkpGQ013|648affd73f7b115cfb0a0d5e0e844d3325ad0dfb94c1de102f790606f5dcd9c3"</span><br><span class="line"></span><br><span class="line">2018-03-31 15:28:23 [scrapy.downloadermiddlewares.cookies] DEBUG: Received cookies from: &lt;200 https://www.zhihu.com/settings/profile&gt;</span><br><span class="line">Set-Cookie: _xsrf=bad5bdecf7b3c52016b3a774309cfa82; expires=Mon, 30 Apr 2018 07:28:20 GMT; Path=/</span><br><span class="line"></span><br><span class="line">2018-03-31 15:28:23 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https://www.zhihu.com/settings/profile&gt; (referer: None)</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; view(response)</span></span><br><span class="line">True</span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt;<span class="comment"># 提取页面中的姓名和个性域名信息</span></span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt;response.css(<span class="string">'span.name::text'</span>).extract_first()</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt;response.css(<span class="string">'input.zg-form-text-input::attr(value)'</span>).extract_first()</span></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt;response.xpath(<span class="string">'//input[@class="zg-form-text-input"]/@value'</span>).extract_first()</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;blockquote class=&quot;blockquote-center&quot; style=&quot;border: &quot;&gt;没事了，约约几个老伙计，喝喝茶，下下棋&lt;br&gt;&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;https://dongfei.oss-cn-shanghai.aliyuncs.com/Example%20web%20scraping%20website%20.jpg&quot; alt=&quot;http://example.webscraping.com/&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Python" scheme="http://blog.dongfei.xin/categories/Python/"/>
    
      <category term="Scrapy" scheme="http://blog.dongfei.xin/categories/Python/Scrapy/"/>
    
    
      <category term="Scrapy" scheme="http://blog.dongfei.xin/tags/Scrapy/"/>
    
  </entry>
  
  <entry>
    <title>精通 Scrapy 网络爬虫（框架篇）</title>
    <link href="http://blog.dongfei.xin/2018-03-25/%E7%B2%BE%E9%80%9A%20Scrapy%20%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%EF%BC%88%E6%A1%86%E6%9E%B6%E7%AF%87%EF%BC%89/"/>
    <id>http://blog.dongfei.xin/2018-03-25/精通 Scrapy 网络爬虫（框架篇）/</id>
    <published>2018-03-25T13:00:58.000Z</published>
    <updated>2018-04-12T11:22:37.584Z</updated>
    
    <content type="html"><![CDATA[<blockquote class="blockquote-center">优秀的人，不是不合群，而是他们合群的人里面没有你</blockquote><h5 id="Scrapy-框架结构"><a href="#Scrapy-框架结构" class="headerlink" title="Scrapy 框架结构"></a>Scrapy 框架结构</h5><p><img src="https://timgsa.baidu.com/timg?image&amp;quality=80&amp;size=b9999_10000&amp;sec=1522577588&amp;di=3ed3c6dd189771dd817e2b75386f310b&amp;imgtype=jpg&amp;er=1&amp;src=http%3A%2F%2Fwww.bkjia.com%2Fuploads%2Fallimg%2F150531%2F051S24401-0.png" alt="Scrapy 框架结构"></p><ul><li><p>ENGINE：引擎，框架的核心，其他所有组件在其控制下协同工作</p></li><li><p>SCHEDULER：调度器，负责对 Spider 提交的下载请求进行调度</p></li><li><p>DOWNLOADER：下载器，负责下载页面（发送HTTP请求/接收HTTP响应）</p></li><li><p>SPIDER：爬虫，负责提取页面中的数据，并产生对新页面的下载请求</p></li><li><p>MIDDLEWARE：中间件，负责对 Request 对象和 Response对象进行处理</p></li><li><p>ITEM PIPELINE：数据管道，负责对爬取到的数据进行处理</p></li></ul><a id="more"></a><h5 id="数据流中的对象"><a href="#数据流中的对象" class="headerlink" title="数据流中的对象"></a>数据流中的对象</h5><p><strong>REQUEST：Scrapy 中的 HTTP 请求对象</strong><br><strong>RESPONSE：Scrapy 中的 HTTP 响应对象</strong><br><strong>ITEM：提取的页面数据对象</strong></p><p>Request 对象的构造器方法 </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">源码：scrapy\http\request</span><br><span class="line"></span><br><span class="line">Request(url[, callback=<span class="keyword">None</span>, method=<span class="string">'GET'</span>, headers=<span class="keyword">None</span>,     body=<span class="keyword">None</span>,</span><br><span class="line">              cookies=<span class="keyword">None</span>,  meta=<span class="keyword">None</span>,    encoding=<span class="string">'utf-8'</span>, priority=<span class="number">0</span>,</span><br><span class="line">              dont_filter=<span class="keyword">False</span>, errback=<span class="keyword">None</span>, flags=<span class="keyword">None</span>])</span><br><span class="line">常用参数：</span><br><span class="line">url：</span><br><span class="line">（必选），请求页面的 url 地址</span><br><span class="line"></span><br><span class="line">callback：</span><br><span class="line">页面解析函数，Callable 类型，Request 对象请求的页面下载完成后，</span><br><span class="line">由该参数指定页面解析函数被调用。如果未传递该参数，默认调用 Spider 的 parse 方法</span><br><span class="line"></span><br><span class="line">headers：</span><br><span class="line">HTTP 请求头部字典，dict 类型。设置为 <span class="keyword">None</span>，禁止发送。如 &#123;<span class="string">'Cookie'</span>: <span class="keyword">None</span>&#125;</span><br><span class="line"></span><br><span class="line">cookies:</span><br><span class="line">Cookie 信息字典，dict 类型</span><br><span class="line"></span><br><span class="line">meta：</span><br><span class="line">Request 的元数据字典，dict 类型，用于给框架中的其他组件传递信息。</span><br><span class="line">比如中间件 Item Pipeline。其他组件可以使用 Request 对象的 meta 属性访问该元数据字典（request.meta）</span><br><span class="line"></span><br><span class="line">encoding；</span><br><span class="line">对 url 和 body 参数进行编码，默认为 utf<span class="number">-8</span></span><br><span class="line"></span><br><span class="line">dont_filter：</span><br><span class="line">同一个 url 多次请求进行过滤，避免重复下载。默认为 <span class="keyword">False</span>，若爬取内容时刻变化，应设置为 <span class="keyword">True</span>，强制下载</span><br><span class="line"></span><br><span class="line">errback：</span><br><span class="line">请求出现异常或者出现 HTTP 错误时（如 <span class="number">404</span> 页面不存在）的回调函数</span><br></pre></td></tr></table></figure><p>Respose 对象的构造器方法</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">源码：scrapy\http\response</span><br><span class="line"></span><br><span class="line">常用参数：</span><br><span class="line">meta：</span><br><span class="line">获取传递的 meta 数据，通过 response<span class="selector-class">.meta</span> 获取信息</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="title">xpath</span><span class="params">(query)</span></span>：</span><br><span class="line">Xpath 选择器提取页面数据</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="title">css</span><span class="params">(query)</span></span>：</span><br><span class="line">CSS 选择器提取页面数据</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="title">urljoin</span><span class="params">(url)</span></span>：</span><br><span class="line">构造绝对 url .传入的 url 参数是一个相对地址</span><br></pre></td></tr></table></figure><h5 id="核心类：scripy-Spider"><a href="#核心类：scripy-Spider" class="headerlink" title="核心类：scripy.Spider"></a>核心类：scripy.Spider</h5><p><strong>源码：scrapy\spiders__init__.py</strong></p><p>重要的属性和方法:</p><ul><li>start_urls：请求 url 列表</li><li>start_requests()：调用 make_requests_from_url 方法，发送 Request 请求</li><li>make_requests_from_url()</li><li>parse()：页面解析函数</li></ul><h5 id="使用-Selector-提取页面数据"><a href="#使用-Selector-提取页面数据" class="headerlink" title="使用 Selector 提取页面数据"></a>使用 Selector 提取页面数据</h5><p><strong>源码：/scrapy/selector</strong></p><ul><li>创建对象</li></ul><p>参数：文档字符串或者 Response 对象<br><strong>注意 Response 内置 Selector</strong></p><ul><li>选择对象</li></ul><p>调用 Selector 对象的 xpath 方法或 css 方法，返回 SelectorList 对象</p><ul><li>提取数据</li></ul><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="title">extract</span><span class="params">()</span></span></span><br><span class="line"><span class="function"><span class="title">extract_first</span><span class="params">()</span></span> </span><br><span class="line"><span class="function"><span class="title">re</span><span class="params">()</span></span></span><br><span class="line"><span class="function"><span class="title">re_first</span><span class="params">()</span></span></span><br></pre></td></tr></table></figure><ul><li>XPath 选择器语法</li></ul><p>根节点、元素节点、属性节点、文本节点</p><p>参考：<br><a href="http://blog.csdn.net/zcc_0015/article/details/51434714" target="_blank" rel="noopener">关于scrapy网络爬虫的xpath书写经验总结</a><br><a href="http://www.w3school.com.cn/xpath/xpath_functions.asp" target="_blank" rel="noopener">XPath 参考手册</a></p><figure class="highlight erlang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">常用标志：</span><br><span class="line"></span><br><span class="line">/           E1/E2</span><br><span class="line">.           ./E</span><br><span class="line">..</span><br><span class="line">ELEMENT</span><br><span class="line">//ELEMENT       E1//E2</span><br><span class="line">*</span><br><span class="line"><span class="function"><span class="title">text</span><span class="params">()</span></span></span><br><span class="line"><span class="function">@ATTR</span></span><br><span class="line"><span class="function">@*</span></span><br><span class="line"><span class="function">ELEMENT[谓语]     谓语：<span class="title">last</span><span class="params">()</span>、<span class="title">position</span><span class="params">()</span>、</span></span><br><span class="line"><span class="function"><span class="title">string</span><span class="params">(arg)</span>：返回参数的字符串值</span></span><br><span class="line"><span class="function"><span class="title">contains</span><span class="params">(str1，str2)</span></span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">示例：</span></span><br><span class="line"><span class="function"><span class="title">response</span>.<span class="title">xpath</span><span class="params">('//a[<span class="number">3</span>]')</span></span></span><br><span class="line"><span class="function"><span class="title">response</span>.<span class="title">xpath</span><span class="params">('//a[last()]')</span></span></span><br><span class="line"><span class="function"><span class="title">response</span>.<span class="title">xpath</span><span class="params">('//a[position()&lt;<span class="number">3</span>]')</span></span></span><br><span class="line"><span class="function"><span class="title">response</span>.<span class="title">xpath</span><span class="params">('string(/html/body/a)')</span></span></span><br><span class="line"><span class="function"><span class="title">response</span>.<span class="title">xpath</span><span class="params">('//p[contains(@class,<span class="string">"info"</span>)]')</span></span></span><br></pre></td></tr></table></figure><ul><li>CSS 选择器语法</li></ul><p>参考：</p><p><a href="http://www.w3school.com.cn/cssref/css_selectors.asp" target="_blank" rel="noopener">CSS 选择器语法</a></p><p><img src="http://dongfei.oss-cn-shanghai.aliyuncs.com/css.jpg" alt="CSS 选择器语法"></p><h5 id="使用-Item-封装数据"><a href="#使用-Item-封装数据" class="headerlink" title="使用 Item 封装数据"></a>使用 Item 封装数据</h5><p><strong>源码：\scrapy\item.py</strong></p><ul><li>Item 基类</li></ul><p>自定义数据类（如MovieItem）的基类，继承 scrapy.item 类</p><ul><li>Field 类</li></ul><p>用来描述自定义数据类包含哪些字段</p><p><strong>注意拓展 item 子类，只需要继承已定义类的即可</strong></p><h5 id="使用-Item-Pipeline-处理数据"><a href="#使用-Item-Pipeline-处理数据" class="headerlink" title="使用 Item Pipeline 处理数据"></a>使用 Item Pipeline 处理数据</h5><p><strong>源码：scrapy\pipelines</strong></p><p>作用：</p><ul><li>清洗数据</li><li>验证数据的有效性</li><li>过滤掉重复的数据</li><li>将数据存入数据库 </li></ul><p>每一个 Item Pipeline 只需要实现某些特定的方法即可</p><ul><li><p>process_item(item,spider)     必须实现，抛出 DropItem 异常后，item 立即抛弃不再传递给下一个 Pipeline</p></li><li><p>open_spider(self,spider)   Spider打开时调用该方法，如连接数据库</p></li><li><p>close_spider(self,spider)   Spider关闭时调用该方法，如关闭数据库</p></li><li><p>from_crawler(cls,crawler)   读取配置文件内容</p></li></ul><h5 id="使用-LinkExtractor-提取页面链接"><a href="#使用-LinkExtractor-提取页面链接" class="headerlink" title="使用 LinkExtractor 提取页面链接"></a>使用 LinkExtractor 提取页面链接</h5><ul><li>使用 Selector</li></ul><p>使用选择器进行页面链接的提取</p><ul><li>使用 LinkExtractor</li></ul><p>位于 scrapy.linkextractors 模块，使用一个或者多个构造器参数描述提取规则，注意 LinkExtractor 构造器的所有参数都有默认值，如果构造对象时不传递任何参数（使用默认值），就提取页面中的所有链接。</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">示例一：</span><br><span class="line">from scrapy.linkextractors import LinkExtractor</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MySpider</span>(<span class="title">scrapy</span>.<span class="title">spider</span>):</span></span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 提取下一页链接</span></span><br><span class="line">    <span class="comment"># &lt;li class="next"&gt;&lt;a href="source/page-1.html"&gt;next&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(<span class="keyword">self</span>,response)</span></span><span class="symbol">:</span></span><br><span class="line">        le = LinkExtractor(restrict_css=<span class="string">'ul.pager li.next'</span>)</span><br><span class="line">        links = le.extract_links(response)</span><br><span class="line">        <span class="keyword">if</span> <span class="symbol">links:</span></span><br><span class="line">            next_url = links[<span class="number">0</span>].url </span><br><span class="line">            <span class="comment"># 得到的是绝对 url 地址，无须再调用 response.urljoin 方法</span></span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(next_url,callback = parse)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 示例二：</span></span><br><span class="line"><span class="comment"># 获取页面所有链接</span></span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt;le = LinkExtractor()</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt;links = le.extract_links(response)</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt;[link.url for link in links]</span><br></pre></td></tr></table></figure><p>LinkExtractor 构造器主要参数：<br><figure class="highlight less"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">allow</span>|<span class="selector-tag">deny</span>：接收一个正则表达式或一个正则表达式列表</span><br><span class="line"><span class="selector-tag">allow_domains</span>|<span class="selector-tag">_domains</span>：接收一个域名或一个域名列表</span><br><span class="line"><span class="selector-tag">restrict_xpaths</span>： 接收一个<span class="selector-tag">XPath</span>表达式或一个<span class="selector-tag">XPath</span>表达式列表</span><br><span class="line"><span class="selector-tag">restrict_css</span>： 接收一个<span class="selector-tag">CSS</span>表达式或一个<span class="selector-tag">CSS</span>表达式列表</span><br><span class="line"><span class="selector-tag">tags</span>：接收一个标签（字符串）或一个标签列表，提取指定标签内的链接，默认为<span class="selector-attr">['a','area']</span></span><br><span class="line"><span class="selector-tag">attrs</span>：接收一个属性（字符串）或一个属性列表，提取指定属性内的链接，默认为<span class="selector-attr">['href']</span></span><br><span class="line"><span class="selector-tag">process_calue</span>：接收一个形如 <span class="selector-tag">func</span>(value) 的回调函数。用于处理提取到的每一个链接</span><br><span class="line"></span><br><span class="line">示例：</span><br><span class="line"><span class="selector-tag">LinkExtractor</span>(allow=<span class="string">'/index/\d+.html$'</span>)</span><br><span class="line"><span class="selector-tag">LinkExtractor</span>(restrict_xpaths=<span class="string">'//div[@id="bottom"]'</span>)</span><br><span class="line"><span class="selector-tag">LinkExtractor</span>(restrict_css=<span class="string">'div#bottom'</span>)</span><br><span class="line"><span class="selector-tag">LinkExtractor</span>(tags=<span class="string">'script'</span>, attr=<span class="string">'src'</span>)</span><br></pre></td></tr></table></figure></p><h5 id="使用-Exporter-导出数据"><a href="#使用-Exporter-导出数据" class="headerlink" title="使用 Exporter 导出数据"></a>使用 Exporter 导出数据</h5><p>Scrapy 中负责导出数据的组件为 Exporter（导出器），其内部实现了多个 Exporter，每个 Exporter 实现一种数据格式的导出。</p><ul><li>JSON (JsonItemExporter)</li><li>JSON lines (JsonLinesItemExporter)</li><li>CSV (CsvItemExporter)</li><li>XML (XmlItemExporter)</li><li>Pickle (PickleItemExporter)</li><li>Marshal (MarshalItemExporter)</li></ul><p>通过以下两种方式指定爬虫如何导出数据：</p><ul><li>通过命令行参数指定</li><li>通过配置文件指定</li></ul><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">示例一：</span><br><span class="line"><span class="keyword">scrapy </span>crawl <span class="keyword">books </span>-o <span class="keyword">books.csv</span></span><br><span class="line"><span class="keyword">scrapy </span>crawl <span class="keyword">books </span>-t csv -o <span class="keyword">books.data</span></span><br><span class="line"><span class="keyword">scrapy </span>crawl <span class="keyword">books </span>-o <span class="string">'export_data/%(name)s/%(time)s.csv'</span></span><br><span class="line"><span class="keyword">scrapy </span>crawl <span class="keyword">books </span>-o <span class="keyword">books.csv </span>--nolog</span><br><span class="line">参数：</span><br><span class="line">-o：导出文件路径 </span><br><span class="line">-t：导出数据格式 </span><br><span class="line">%(name)s：Spider 的名字</span><br><span class="line">%(time)s： 文件创建的时间</span><br></pre></td></tr></table></figure><p>配置字典 FEED_EXPORTERS 包括两部分：</p><ul><li>默认配置文件中的 FEED_EXPORTERS_BASE</li><li>用户配置文件中的 FEED_EXPORTERS</li></ul><p>默认的配置字典 FEED_EXPORTERS 位于 scrapy.settings.default_settings 模块</p><p>用户配置文件参数在 settings.py 中设置：<br><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">FEED_URL：导出文件路径</span><br><span class="line">FEED_URL = 'export_data/%(name)s.data'</span><br><span class="line"></span><br><span class="line">FEED_FORMAT：导出数据格式</span><br><span class="line">FEED_FORMAT = 'csv'</span><br><span class="line"></span><br><span class="line">FEED_EXPORT_ENCODING：导出文件编码，默认 json 使用数字编码，其他使用 utf-8</span><br><span class="line">FEED_EXPORT_ENCODING = 'gbk'</span><br><span class="line"></span><br><span class="line">FEED_EXPORT_FILEDS：导出数据包含的字段（有序）</span><br><span class="line">FEED_EXPORT_FILEDS = ['name','author','price']</span><br><span class="line"></span><br><span class="line">FEED_EXPORTERS：用户自定义 Exporter字典，添加新的导出数据格式时使用 </span><br><span class="line">FEED_EXPORTERS = &#123;'excel':'projectname.exporters.ExcelItemExporter'&#125;</span><br></pre></td></tr></table></figure></p><p>导出新的数据格式，参照 Scrapy 内部的 Exporter 类（scrapy.exporters 模块）<br>将数据写入 Excel 文件可以使用第三方库 xlwt</p><h5 id="使用-FilesPipeline-和-ImagesPipeline-下载文件和图片"><a href="#使用-FilesPipeline-和-ImagesPipeline-下载文件和图片" class="headerlink" title="使用 FilesPipeline 和 ImagesPipeline 下载文件和图片"></a>使用 FilesPipeline 和 ImagesPipeline 下载文件和图片</h5><p><strong>源码：scrapy/pipelines</strong></p><p>Scrpay 框架内部提供了 FilesPipeline 和 ImagesPipeline，专门用于下载文件和图片，用户只需要通过 item 的一个特殊字段将要下载文件或图片的 url 传递给它们，它们就会自动将文件或图片下载到本地，并将下载结果信息存入 item 的另一个特殊字段，以便用户在导出文件中查询。</p><p>配置如下：</p><ul><li><p>配置文件 setting.py 中启用 FilesPipeline，通常将其置于其他 Item Pipeline 之前</p></li><li><p>配置文件 setting.py 中使用 FILES_SRORE 指定文件下载目录</p></li><li><p>将所需要下载文件的 url 地址收集到一个列表，赋值给 item 的 file_urls 字段（items[‘file_urls’]）。FilesPipeline 在处理每一项 item 时，会读取 item[‘file_urls’]，对其每一个 url 进行下载。</p></li></ul><p>示例如下：</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">ITEM_PIPELINES = &#123;<span class="string">'scrapy.pipeline.files.FilesPipeline'</span> : <span class="number">1</span>&#125;</span><br><span class="line"></span><br><span class="line">FILES_SRORE = <span class="string">'/home/download/scrapy'</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(<span class="keyword">self</span>,response)</span></span><span class="symbol">:</span></span><br><span class="line">    item = &#123;&#125;</span><br><span class="line">    <span class="comment"># 下载列表</span></span><br><span class="line">    item[<span class="string">'file_urls'</span>] = []</span><br><span class="line">    <span class="keyword">for</span> url <span class="keyword">in</span> response.xpath(<span class="string">'//s/@href'</span>).extract()<span class="symbol">:</span></span><br><span class="line">        download_url = response.urljoin(url)</span><br><span class="line">        <span class="comment"># 将 url 加入下载列表</span></span><br><span class="line">        item[<span class="string">'file_urls'</span>] .append(download_url)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">yield</span> item</span><br></pre></td></tr></table></figure><p>当 FilesPipeline 下载完 item[‘file_urls’] 中的所有文件后，会将各文件的下载结果信息收集到另一个列表，赋值给 item 的files 字段（item[‘files’]）。下载结果信息包括以下内容：</p><ul><li>Path 文件下载到本地的路径（相对于 FILES_SRORE 的相对路径）。</li><li>Checksum 文件的校验和</li><li>url 文件的 url 地址</li></ul><p>ImagesPipeline 是 FilesPipeline 的子类，使用上和 FilesPipeline 大同小异。只是在所使用的 item 字段和配置选项上略有不同。</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">                FilesPipeline                   </span><br><span class="line"></span><br><span class="line">导入路径    scrapy<span class="selector-class">.pipeline</span><span class="selector-class">.files</span><span class="selector-class">.FilesPipeline</span></span><br><span class="line"></span><br><span class="line">Item 字段     file_urls，files</span><br><span class="line"></span><br><span class="line">下载目录        FILES_SRORE</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">                ImagesPipeline</span><br><span class="line"></span><br><span class="line">导入路径    scrapy<span class="selector-class">.pipeline</span><span class="selector-class">.files</span><span class="selector-class">.ImagesPipeline</span></span><br><span class="line"></span><br><span class="line">Item 字段     image_urls，images</span><br><span class="line"></span><br><span class="line">下载目录        IMAGES_STORE</span><br></pre></td></tr></table></figure><p>ImagesPipeline 在 FilesPipeline 的基础上针对图片增加了一些特有的功能</p><ul><li>为图片生成缩略图</li></ul><p>开启该功能，只需要在配置文件 setting.py 中设置 IMAGES_THUMBS，它是一个字典，每一项的值是缩略图的尺寸，代码如下：<br><figure class="highlight gcode"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">IMAGES_THUMBS = &#123;</span><br><span class="line">    <span class="string">'small'</span>: <span class="comment">(50,50)</span>,</span><br><span class="line">    <span class="string">'big'</span>: <span class="comment">(270,270)</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>开启该功能后，下载一张图片时，本地会出现 3 张图片（一张原图片，两张缩略图）。路径如下：<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[IMAGES_STORE]/full/<span class="number">23239</span>.png</span><br><span class="line">[IMAGES_STORE]/thumbs/small/<span class="number">23239</span>.png</span><br><span class="line">[IMAGES_STORE]/thumbs/big/<span class="number">23239</span>.png</span><br></pre></td></tr></table></figure></p><ul><li>过滤尺寸过小的图片</li></ul><p>开启该功能，需要在配置文件 setting.py 中设置 IMAGES_MIN_WIDTH 和 IMAGES_MIN_HEIGHT ，他们分别指定图片最小的宽和高，代码如下：</p><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">IMAGES_MIN_WIDTH</span> = <span class="number">110</span></span><br><span class="line"><span class="attr">IMAGES_MIN_HEIGHT</span> = <span class="number">110</span></span><br></pre></td></tr></table></figure><p>开启该功能后如果下载了一张 105 X 200 的图片，该图片的宽度不符合标准会被抛弃。</p><p>文件名是其 url 的 sha1 散列值。可以修改 FilesPipeline 的文件命名规则，只需要实现一个 FilesPipeline 的子类，覆写 file_path 方法来实现所期望的文件命名规则。示例如下：<br><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="title">from</span> scrapy.pipeline.files.<span class="type">FilesPipeline</span> <span class="keyword">import</span> FilesPipeline</span><br><span class="line"><span class="title">from</span> urllib.parse <span class="keyword">import</span> urlparse</span><br><span class="line"><span class="title">from</span> os.path <span class="keyword">import</span> basename,dirname,join</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="type">MyFilesPipeline</span>(<span class="type">FilesPipeline</span>):</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">    def file_path(<span class="title">self</span>,<span class="title">request</span>,<span class="title">response</span>=<span class="type">None</span>,<span class="title">info</span>=<span class="type">None</span>):</span></span><br><span class="line"><span class="class">        path = urlparse(<span class="title">request</span>.<span class="title">url</span>).path</span></span><br><span class="line"><span class="class">        return join(<span class="title">basename</span>(<span class="title">dirname</span>(<span class="title">path</span>)),basename(<span class="title">path</span>))</span></span><br></pre></td></tr></table></figure></p><p>下载扩展 <strong>MediaPipeline</strong>，参考链接：<a href="http://scrapy-chs.readthedocs.io/zh_CN/1.0/topics/media-pipeline.html#module-scrapy.pipeline.files" target="_blank" rel="noopener">http://scrapy-chs.readthedocs.io/zh_CN/1.0/topics/media-pipeline.html#module-scrapy.pipeline.files</a></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote class=&quot;blockquote-center&quot;&gt;优秀的人，不是不合群，而是他们合群的人里面没有你&lt;/blockquote&gt;

&lt;h5 id=&quot;Scrapy-框架结构&quot;&gt;&lt;a href=&quot;#Scrapy-框架结构&quot; class=&quot;headerlink&quot; title=&quot;Scrapy 框架结构&quot;&gt;&lt;/a&gt;Scrapy 框架结构&lt;/h5&gt;&lt;p&gt;&lt;img src=&quot;https://timgsa.baidu.com/timg?image&amp;amp;quality=80&amp;amp;size=b9999_10000&amp;amp;sec=1522577588&amp;amp;di=3ed3c6dd189771dd817e2b75386f310b&amp;amp;imgtype=jpg&amp;amp;er=1&amp;amp;src=http%3A%2F%2Fwww.bkjia.com%2Fuploads%2Fallimg%2F150531%2F051S24401-0.png&quot; alt=&quot;Scrapy 框架结构&quot;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;ENGINE：引擎，框架的核心，其他所有组件在其控制下协同工作&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;SCHEDULER：调度器，负责对 Spider 提交的下载请求进行调度&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;DOWNLOADER：下载器，负责下载页面（发送HTTP请求/接收HTTP响应）&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;SPIDER：爬虫，负责提取页面中的数据，并产生对新页面的下载请求&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;MIDDLEWARE：中间件，负责对 Request 对象和 Response对象进行处理&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;ITEM PIPELINE：数据管道，负责对爬取到的数据进行处理&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Python" scheme="http://blog.dongfei.xin/categories/Python/"/>
    
      <category term="Scrapy" scheme="http://blog.dongfei.xin/categories/Python/Scrapy/"/>
    
    
      <category term="Scrapy" scheme="http://blog.dongfei.xin/tags/Scrapy/"/>
    
  </entry>
  
  <entry>
    <title>爬虫参考指南</title>
    <link href="http://blog.dongfei.xin/2018-03-24/%E7%88%AC%E8%99%AB%E5%8F%82%E8%80%83%E6%8C%87%E5%8D%97/"/>
    <id>http://blog.dongfei.xin/2018-03-24/爬虫参考指南/</id>
    <published>2018-03-24T02:10:51.000Z</published>
    <updated>2018-04-08T07:08:48.701Z</updated>
    
    <content type="html"><![CDATA[<p><strong>调试FAQ</strong></p><ul><li><a href="blog.csdn.net/zcc_0015/article/details/51434714">关于scrapy网络爬虫的xpath书写经验总结</a></li><li><a href="https://segmentfault.com/a/1190000003509661" target="_blank" rel="noopener">scrapy 的 shell 命令</a></li></ul><p><strong>工具参考</strong></p><ul><li><a href="https://cuiqingcai.com/4959.html" target="_blank" rel="noopener">跟繁琐的命令行说拜拜！Gerapy分布式爬虫管理框架来袭！ | 静觅</a></li><li><a href="https://blog.scrapinghub.com/" target="_blank" rel="noopener">The Scrapinghub Blog – Turn Web Content Into Useful Data</a></li><li><a href="http://www.hicrawler.com/" target="_blank" rel="noopener">免费的网页数据采集工具, 高智能云爬虫</a></li><li><a href="http://www.newcrawler.com/zh-cn/index.html" target="_blank" rel="noopener">鸟巢采集器 | 免费的网页数据采集工具</a></li><li><a href="http://www.shenjianshou.cn/" target="_blank" rel="noopener">在线网络爬虫/大数据分析/机器学习开发平台-神箭手云</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;调试FAQ&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;blog.csdn.net/zcc_0015/article/details/51434714&quot;&gt;关于scrapy网络爬虫的xpath书写经验总结&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a hr
      
    
    </summary>
    
      <category term="Python" scheme="http://blog.dongfei.xin/categories/Python/"/>
    
      <category term="Scrapy" scheme="http://blog.dongfei.xin/categories/Python/Scrapy/"/>
    
    
      <category term="Python" scheme="http://blog.dongfei.xin/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>前端背景特效总结一波</title>
    <link href="http://blog.dongfei.xin/2018-01-31/%E5%89%8D%E7%AB%AF%E8%83%8C%E6%99%AF%E7%89%B9%E6%95%88%E6%80%BB%E7%BB%93%E4%B8%80%E6%B3%A2/"/>
    <id>http://blog.dongfei.xin/2018-01-31/前端背景特效总结一波/</id>
    <published>2018-01-31T12:54:17.000Z</published>
    <updated>2018-03-20T06:25:29.146Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://alloyteam.github.io/curvejs/" target="_blank" rel="noopener">魔幻线条curvejs</a> 这个感觉实用性不强</p><p><a href="https://www.zybuluo.com/mdeditor#" target="_blank" rel="noopener">粒子特效插件particles-自己的文章</a><br><a href="https://github.com/Barrior/JParticles" target="_blank" rel="noopener">JParticles是多个特效的一个整合</a></p><p><a href="https://github.com/kennethcachia/shape-shifter" target="_blank" rel="noopener">shape-shifter 用粒子组成文字</a><br><a href="http://www.kennethcachia.com/shape-shifter/" target="_blank" rel="noopener">shape-shifter Demo</a></p><p><a href="https://github.com/juliangarnier/anime" target="_blank" rel="noopener">anime.js</a> 动画库 其实跟上面的不同 但可以用这个做许多特效 而且例子不错<br><a id="more"></a></p><p><a href="https://github.com/greensock/GreenSock-JS/" target="_blank" rel="noopener">GreenSock-JS</a> 动画库 这个动画的例子真的很棒<br><a href="https://greensock.com/examples-showcases" target="_blank" rel="noopener">GreenSock-JS DEMO</a><br><a href="http://www.w2bc.com/Article/9438" target="_blank" rel="noopener">TweenMax的实例</a><br><a href="http://www.jq22.com/code312" target="_blank" rel="noopener">TweenMax的实例2</a></p><p>看来TweenMax用来做菱形这个背景应该是很拿手的 有空可以看一下</p><p><a href="https://github.com/rishabhp/bideo.js" target="_blank" rel="noopener">HTML5 Background Video</a></p><p><a href="http://www.qdfuns.com/notes/14464/3be240066e8ab4d47f0fd65e2a3060c5.html" target="_blank" rel="noopener">H5腾讯QQ登录界面背景动画特效</a></p><p>不断生成的粒子特效</p><p>其实是three.js的一个例子<br><a href="https://threejs.org/examples/#canvas_particles_waves" target="_blank" rel="noopener">HTML5 3D 粒子波浪动画特效</a></p><p>菱形背景特效</p><p>固定版菱形背景特效</p><p><a href="https://github.com/qrohlf/trianglify" target="_blank" rel="noopener">用的是这个 trianglify插件</a></p><p>用three.js做的低多边形 这个是我比较喜欢的</p><p>转载自：<a href="http://elickzhao.github.io/" target="_blank" rel="noopener">elickzhao’s Blog</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://alloyteam.github.io/curvejs/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;魔幻线条curvejs&lt;/a&gt; 这个感觉实用性不强&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.zybuluo.com/mdeditor#&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;粒子特效插件particles-自己的文章&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;https://github.com/Barrior/JParticles&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;JParticles是多个特效的一个整合&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/kennethcachia/shape-shifter&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;shape-shifter 用粒子组成文字&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;http://www.kennethcachia.com/shape-shifter/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;shape-shifter Demo&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/juliangarnier/anime&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;anime.js&lt;/a&gt; 动画库 其实跟上面的不同 但可以用这个做许多特效 而且例子不错&lt;br&gt;
    
    </summary>
    
      <category term="JavaScript" scheme="http://blog.dongfei.xin/categories/JavaScript/"/>
    
    
      <category term="JavaScript" scheme="http://blog.dongfei.xin/tags/JavaScript/"/>
    
  </entry>
  
  <entry>
    <title>Scrapy-Elasticsearch-Django 搭建搜索网站</title>
    <link href="http://blog.dongfei.xin/2018-01-12/Scrapy-Elasticsearch-Django%20%E6%90%AD%E5%BB%BA%E6%90%9C%E7%B4%A2%E7%BD%91%E7%AB%99/"/>
    <id>http://blog.dongfei.xin/2018-01-12/Scrapy-Elasticsearch-Django 搭建搜索网站/</id>
    <published>2018-01-12T03:19:38.000Z</published>
    <updated>2018-04-12T11:23:48.272Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://dongfei.oss-cn-shanghai.aliyuncs.com/elasticsearch.jpg" alt="https://www.elastic.co"><br><strong>Elasticsearch官方</strong>：<a href="https://www.elastic.co/use-cases" target="_blank" rel="noopener">https://www.elastic.co/use-cases</a></p><p>其他类似产品：<strong>Elasticsearch solr 、sphinx、ELK</strong><br><a id="more"></a></p><font color="red">Elasticsearch 安装 </font><ul><li>安装：<strong>JavaSE JDK</strong> <a href="http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html" target="_blank" rel="noopener">http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html</a></li><li>安装：<strong>Elasticsearch</strong> <a href="https://www.elastic.co/downloads/elasticsearch" target="_blank" rel="noopener">https://www.elastic.co/downloads/elasticsearch</a></li><li>推荐安装：<strong>Elasticsearch-rtf</strong> <a href="https://github.com/medcl/elasticsearch-rtf" target="_blank" rel="noopener">https://github.com/medcl/elasticsearch-rtf</a></li></ul><p>检测安装是否成功：<br><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">切换至<span class="selector-tag">elasticsearch</span>安装目录、<span class="selector-tag">cmd</span>运行：<span class="selector-tag">elasticsearch</span></span><br><span class="line">127<span class="selector-class">.0</span><span class="selector-class">.0</span><span class="selector-class">.1</span><span class="selector-pseudo">:9200</span></span><br></pre></td></tr></table></figure></p><font color="red">head 插件和 kibana 的安装 </font><br><strong>head安装：</strong> 数据库管理、基于浏览器<br><strong>Github地址：</strong> <a href="https://github.com/mobz/elasticsearch-head" target="_blank" rel="noopener">https://github.com/mobz/elasticsearch-head</a><br><br>需要安装：<strong>node.js npm+cnpm</strong><br>测试：<br><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">cd</span> <span class="selector-tag">elasticsearch-head</span></span><br><span class="line"><span class="selector-tag">npm</span> <span class="selector-tag">install</span></span><br><span class="line"><span class="selector-tag">npm</span> <span class="selector-tag">run</span> <span class="selector-tag">start</span></span><br><span class="line">打开：127<span class="selector-class">.0</span><span class="selector-class">.0</span><span class="selector-class">.1</span><span class="selector-pseudo">:9100</span></span><br></pre></td></tr></table></figure><br><br>报错权限问题：<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">配置Elasticsearch.yml</span><br><span class="line">http<span class="selector-class">.cors</span><span class="selector-class">.enabled</span>: true</span><br><span class="line">http<span class="selector-class">.cors</span><span class="selector-class">.allow-origin</span>: <span class="string">"*"</span></span><br><span class="line">http<span class="selector-class">.cors</span><span class="selector-class">.allow-methods</span>: OPTIONS, HEAD, GET, POST, PUT, DELETE</span><br><span class="line">http<span class="selector-class">.cors</span><span class="selector-class">.allow-headers</span>: <span class="string">"X-Requested-With, Content-Type, Content-Length, X-User"</span></span><br></pre></td></tr></table></figure><br><br><strong>kibana安装、与elasticsearch版本对应：</strong><br><strong>下载地址：</strong><a href="https://www.elastic.co/downloads/past-releases/" target="_blank" rel="noopener">https://www.elastic.co/downloads/past-releases/</a><br>测试、切换至解压目录运行 <code>kibana.bat</code><br><br><font color="red">Elasticsearch 基本的索引和文档 CRUD 操作</font><ul><li>index（索引）==》数据库</li><li>type（类型）==》表</li><li>documents（文档）==》行</li><li>fields ==》列 </li></ul><p><strong>初始化数据库</strong><br><figure class="highlight xquery"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">PUT lagou</span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">"settings"</span>: &#123;</span><br><span class="line">      <span class="string">"index"</span> :&#123;</span><br><span class="line">        <span class="string">"number_of_shards"</span>:<span class="number">5</span>, 分片</span><br><span class="line">        <span class="string">"number_of_replicas"</span>:<span class="number">1</span> 副本</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p><strong>获取数据库信息</strong><br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="builtin-name">GET</span> lagou/_settings</span><br><span class="line"><span class="builtin-name">GET</span> _all/_settings</span><br><span class="line"><span class="builtin-name">GET</span> _all/_settings</span><br><span class="line"><span class="builtin-name">GET</span> .kibana,lagou/_settings</span><br></pre></td></tr></table></figure></p><p><strong>修改 _settings</strong><br><figure class="highlight dts"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">PUT lagou/_settings</span><br><span class="line">&#123;</span><br><span class="line">   <span class="string">"number_of_shards"</span>:<span class="number">3</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p><strong>获取索引信息</strong><br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="builtin-name">GET</span> _all</span><br><span class="line"><span class="builtin-name">GET</span> lagou</span><br></pre></td></tr></table></figure></p><p><strong>保存文档</strong><br><figure class="highlight xquery"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">PUT lagou/job/<span class="number">1</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"title"</span>:<span class="string">"python分布式爬虫开发 "</span>,</span><br><span class="line">  <span class="string">"salary_min"</span>:<span class="number">15000</span>,</span><br><span class="line">  <span class="string">"city"</span>:<span class="string">"北京"</span>,</span><br><span class="line">  <span class="string">"company"</span>:&#123;</span><br><span class="line">    <span class="string">"name"</span>:<span class="string">"百度"</span>,</span><br><span class="line">    <span class="string">"company_addr"</span>:<span class="string">"北京市软件园"</span></span><br><span class="line">    </span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="string">"publish_date"</span>:<span class="string">"2017-4-16"</span>,</span><br><span class="line">  <span class="string">"comments"</span>:<span class="number">15</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p><strong>添加数据</strong><br><figure class="highlight xquery"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">POST lagou/job/</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"title"</span>:<span class="string">"python django 开发工程师"</span>,</span><br><span class="line">  <span class="string">"salary_min"</span>:<span class="number">30000</span>,</span><br><span class="line">  <span class="string">"city"</span>:<span class="string">"上海"</span>,</span><br><span class="line">  <span class="string">"company"</span>:&#123;</span><br><span class="line">    <span class="string">"name"</span>:<span class="string">"美团科技"</span>,</span><br><span class="line">    <span class="string">"company_addr"</span>:<span class="string">"北京市软件园A区"</span></span><br><span class="line">    </span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="string">"publish_date"</span>:<span class="string">"2017-4-16"</span>,</span><br><span class="line">  <span class="string">"comments"</span>:<span class="number">20</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p><strong>查看数据</strong><br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="builtin-name">GET</span> lagou/job/1</span><br><span class="line"><span class="builtin-name">GET</span> lagou/job/1?<span class="attribute">_source</span>=title</span><br><span class="line"><span class="builtin-name">GET</span> lagou/job/1?<span class="attribute">_source</span>=title,city</span><br></pre></td></tr></table></figure></p><p><strong>修改数据</strong><br><figure class="highlight xquery"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">PUT lagou/job/<span class="number">1</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"title"</span>:<span class="string">"python分布式爬虫开发 "</span>,</span><br><span class="line">  <span class="string">"salary_min"</span>:<span class="number">15000</span>,</span><br><span class="line">  <span class="string">"company"</span>:&#123;</span><br><span class="line">    <span class="string">"name"</span>:<span class="string">"百度"</span>,</span><br><span class="line">    <span class="string">"company_addr"</span>:<span class="string">"北京市软件园"</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="string">"publish_date"</span>:<span class="string">"2017-4-16"</span>,</span><br><span class="line">  <span class="string">"comments"</span>:<span class="number">15</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">POST lagou/job/<span class="number">1</span>/_<span class="keyword">update</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"doc"</span>: &#123;</span><br><span class="line">    <span class="string">"comment"</span>:<span class="number">20</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p><strong>删除数据</strong><br><figure class="highlight gauss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">DELETE</span> lagou/job/<span class="number">1</span></span><br><span class="line"><span class="keyword">DELETE</span> lagou/job</span><br><span class="line"><span class="keyword">DELETE</span> lagou</span><br></pre></td></tr></table></figure></p><font color="red">Elasticsearch 的 mget 和 bulk 批量操作</font><p><strong>mget 操作</strong><br><figure class="highlight xquery"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line">PUT testdb</span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">"settings"</span>: &#123;</span><br><span class="line">      <span class="string">"index"</span> :&#123;</span><br><span class="line">        <span class="string">"number_of_shards"</span>:<span class="number">5</span>, </span><br><span class="line">        <span class="string">"number_of_replicas"</span>:<span class="number">1</span> </span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">PUT testdb/job1/<span class="number">1</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"title"</span>:<span class="string">"job1_1"</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">PUT testdb/job1/<span class="number">2</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"title"</span>:<span class="string">"job1_2"</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">PUT testdb/job2/<span class="number">1</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"title"</span>:<span class="string">"job2_1"</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">PUT testdb/job2/<span class="number">2</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"title"</span>:<span class="string">"job2_2"</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">GET _mget</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"docs"</span>:[</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="string">"_index"</span>:<span class="string">"testdb"</span>,</span><br><span class="line">      <span class="string">"_type"</span>:<span class="string">"job1"</span>,</span><br><span class="line">      <span class="string">"_id"</span>:<span class="number">1</span></span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="string">"_index"</span>:<span class="string">"testdb"</span>,</span><br><span class="line">      <span class="string">"_type"</span>:<span class="string">"job1"</span>,</span><br><span class="line">      <span class="string">"_id"</span>:<span class="number">2</span></span><br><span class="line">    &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">GET testdb/_mget</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"docs"</span>:[</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="string">"_type"</span>:<span class="string">"job1"</span>,</span><br><span class="line">      <span class="string">"_id"</span>:<span class="number">1</span></span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="string">"_type"</span>:<span class="string">"job1"</span>,</span><br><span class="line">      <span class="string">"_id"</span>:<span class="number">2</span></span><br><span class="line">    &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">GET testdb/job1/_mget</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"docs"</span>:[</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="string">"_id"</span>:<span class="number">1</span></span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="string">"_id"</span>:<span class="number">2</span></span><br><span class="line">    &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">GET testdb/job1/_mget</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"ids"</span>:[<span class="number">1</span>,<span class="number">2</span>]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p><strong>bulk批量操作</strong><br><figure class="highlight xquery"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">POST _bulk</span><br><span class="line">&#123;<span class="string">"index"</span>:&#123;<span class="string">"_index"</span>:<span class="string">"lagou"</span>,<span class="string">"_type"</span>:<span class="string">"job"</span>,<span class="string">"_id"</span>:<span class="string">"1"</span>&#125;&#125;</span><br><span class="line">&#123;<span class="string">"title"</span>:<span class="string">"python django 开发工程师"</span>,<span class="string">"salary_min"</span>:<span class="number">30000</span>,<span class="string">"city"</span>:<span class="string">"上海"</span>,<span class="string">"company"</span>:&#123;<span class="string">"name"</span>:<span class="string">"美团科技"</span>,<span class="string">"company_addr"</span>:<span class="string">"北京市软件园A区"</span>&#125;,<span class="string">"publish_date"</span>:<span class="string">"2017-4-16"</span>,<span class="string">"comments"</span>:<span class="number">20</span>&#125;</span><br><span class="line">&#123;<span class="string">"index"</span>:&#123;<span class="string">"_index"</span>:<span class="string">"lagou"</span>,<span class="string">"_type"</span>:<span class="string">"job2"</span>,<span class="string">"_id"</span>:<span class="string">"2"</span>&#125;&#125;</span><br><span class="line">&#123;<span class="string">"title"</span>:<span class="string">"python分布式爬虫开发 "</span>,<span class="string">"salary_min"</span>:<span class="number">15000</span>,<span class="string">"city"</span>:<span class="string">"北京"</span>,<span class="string">"company"</span>:&#123;<span class="string">"name"</span>:<span class="string">"百度"</span>,<span class="string">"company_addr"</span>:<span class="string">"北京市软件园"</span>&#125;,<span class="string">"publish_date"</span>:<span class="string">"2017-4-16"</span>,<span class="string">"comments"</span>:<span class="number">15</span>&#125;</span><br></pre></td></tr></table></figure></p><font color="red">Elasticsearch 的 mapping 映射管理</font><br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="builtin-name">GET</span> lagou/_mapping/job</span><br><span class="line"><span class="builtin-name">GET</span> _all/_mapping</span><br></pre></td></tr></table></figure><br><br><font color="red">Scrapy 写入数据到 Elasticsearch</font><p><strong>elasticsearch-Python</strong> 数据转换包 <strong>elasticsearch-dsl-py</strong><br>Github地址：<a href="https://github.com/elastic/elasticsearch-dsl-py" target="_blank" rel="noopener">https://github.com/elastic/elasticsearch-dsl-py</a><br>安装：<code>pip install elasticsearch-dsl</code></p><p>定义数据类型、添加<code>ElasticsearchPipeline</code></p><p><strong>ES完成搜索建议-搜索建议字段保存</strong><br>自动补全文档：<br><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/search-suggesters-completion.html" target="_blank" rel="noopener">https://www.elastic.co/guide/en/elasticsearch/reference/current/search-suggesters-completion.html</a></p><p><strong>查看分析器分析结果</strong><br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="builtin-name">GET</span> _analyze</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"analyzer"</span>: <span class="string">"ik_smart"</span>, </span><br><span class="line">  <span class="string">"text"</span>: <span class="string">"Python网络开发工程师 "</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p><strong>注意安装指定版本</strong><br><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip install elasticsearch-dsl==<span class="number">5.1</span><span class="number">.0</span></span><br><span class="line">pip install elasticsearch==<span class="number">5.1</span><span class="number">.0</span></span><br></pre></td></tr></table></figure></p><p><font color="red">Django 实现 elasticsearch 的搜索建议</font><br>创建开发环境：<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkvirtualenv<span class="selector-class">.bat</span> --python=D:\ProgramData\Anaconda3\python<span class="selector-class">.exe</span> lcv_search</span><br></pre></td></tr></table></figure></p><p>安装django：<br><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip <span class="keyword">install</span> django</span><br></pre></td></tr></table></figure></p><p>GitHub相关：<strong>Django scrapy</strong></p><p><font color="red">scrapyd 部署 scrapy 项目</font><br>Github地址：<a href="https://github.com/scrapy/scrapyd" target="_blank" rel="noopener">https://github.com/scrapy/scrapyd</a><br>scrapyd 相当于服务器<br>安装：<br><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip <span class="keyword">install </span><span class="keyword">scrapyd</span></span><br><span class="line"><span class="keyword">pip </span><span class="keyword">install </span><span class="keyword">scrapyd-client</span></span><br></pre></td></tr></table></figure></p><p>创建 scrapyd-deploy.bat 文件：<br><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">scrapyd-deploy </span>-l</span><br><span class="line"><span class="keyword">scrapy </span>list</span><br></pre></td></tr></table></figure></p><p>部署项目：<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scrapyd-deploy dongfei -<span class="selector-tag">p</span> ArticleSpider</span><br><span class="line"></span><br><span class="line">curl http:<span class="comment">//localhost:6800/daemonstatus.json</span></span><br></pre></td></tr></table></figure></p><p>部署某一个spider：<br><figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl <span class="string">http:</span><span class="comment">//localhost:6800/schedule.json -d project=ArticleSpider -d spider=jobbole</span></span><br></pre></td></tr></table></figure></p><p>删除：<br><figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl <span class="string">http:</span><span class="comment">//localhost:6800/delproject.json -d project=ArticleSpider</span></span><br></pre></td></tr></table></figure></p><p>官方API文档：<br><a href="http://scrapyd.readthedocs.io/en/stable/api.html#daemonstatus-json" target="_blank" rel="noopener">http://scrapyd.readthedocs.io/en/stable/api.html#daemonstatus-json</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://dongfei.oss-cn-shanghai.aliyuncs.com/elasticsearch.jpg&quot; alt=&quot;https://www.elastic.co&quot;&gt;&lt;br&gt;&lt;strong&gt;Elasticsearch官方&lt;/strong&gt;：&lt;a href=&quot;https://www.elastic.co/use-cases&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.elastic.co/use-cases&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;其他类似产品：&lt;strong&gt;Elasticsearch solr 、sphinx、ELK&lt;/strong&gt;&lt;br&gt;
    
    </summary>
    
      <category term="Python" scheme="http://blog.dongfei.xin/categories/Python/"/>
    
      <category term="Scrapy" scheme="http://blog.dongfei.xin/categories/Python/Scrapy/"/>
    
    
      <category term="Scrapy" scheme="http://blog.dongfei.xin/tags/Scrapy/"/>
    
      <category term="Elasticsearch" scheme="http://blog.dongfei.xin/tags/Elasticsearch/"/>
    
      <category term="Django" scheme="http://blog.dongfei.xin/tags/Django/"/>
    
  </entry>
  
  <entry>
    <title>Scrapy-Redis 分布式爬虫基础</title>
    <link href="http://blog.dongfei.xin/2018-01-12/Scrapy-Redis%20%E5%88%86%E5%B8%83%E5%BC%8F%E7%88%AC%E8%99%AB%E5%9F%BA%E7%A1%80/"/>
    <id>http://blog.dongfei.xin/2018-01-12/Scrapy-Redis 分布式爬虫基础/</id>
    <published>2018-01-12T02:47:29.000Z</published>
    <updated>2018-01-12T03:05:08.633Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://dongfei.oss-cn-shanghai.aliyuncs.com/redis.jpg" alt="http://dongfei.oss-cn-shanghai.aliyuncs.com/redis.jpg"></p><p>Redis 文档 <a href="http://redisdoc.com/" target="_blank" rel="noopener">http://redisdoc.com/</a></p><p><font color="red">安装</font><br>Redis 官方安装 ：<a href="https://redis.io/download" target="_blank" rel="noopener">https://redis.io/download</a></p><p>Linux 安装： <code>sudo apt-get install redis-server</code></p><p>Windows 安装 ：</p><ul><li><a href="https://github.com/MicrosoftArchive/redis/releases" target="_blank" rel="noopener">https://github.com/MicrosoftArchive/redis/releases</a></li><li><a href="https://github.com/ServiceStack/redis-windows" target="_blank" rel="noopener">https://github.com/ServiceStack/redis-windows</a></li></ul><a id="more"></a><p><strong>设置服务、默认端口：6379</strong></p><p><font color="red">Redis 数据类型</font><br><strong>字符串</strong><br><figure class="highlight gams"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> course_name <span class="comment">"scrapy-redis"</span></span><br><span class="line">get <span class="comment">course_name</span></span><br><span class="line">getrange <span class="comment">course_name 2 5</span></span><br><span class="line">strlen <span class="comment">course_name</span></span><br><span class="line">incr/decr course_count</span><br><span class="line">append course_count <span class="comment">"body"</span></span><br></pre></td></tr></table></figure></p><p><strong>散列、哈希</strong><br><figure class="highlight armasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="symbol">hset</span> course_dict <span class="keyword">boddy </span><span class="string">"python scrapy"</span></span><br><span class="line"><span class="symbol">hget</span> course_dict <span class="keyword">boddy</span></span><br><span class="line"><span class="keyword">hgetall </span>course_dict </span><br><span class="line"><span class="symbol">hexists</span> couser_dict <span class="keyword">boddy</span></span><br><span class="line"><span class="keyword">hdel </span>course_dict <span class="keyword">boddy </span></span><br><span class="line"><span class="symbol">hkeys</span> course_dict</span><br><span class="line"><span class="symbol">hvals</span> course_dict</span><br></pre></td></tr></table></figure></p><p><strong>列表</strong><br><figure class="highlight tcl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">lpush/rpush mylist <span class="string">"imooc"</span></span><br><span class="line"><span class="keyword">lrange</span> mylist <span class="number">0</span> <span class="number">10</span> </span><br><span class="line">blpop/brpop mylist1[mylist2] timeout</span><br><span class="line">lpop/rpop mylist </span><br><span class="line">llen mylist </span><br><span class="line"><span class="keyword">lindex</span> mylist  index</span><br></pre></td></tr></table></figure></p><p><strong>集合</strong><br><figure class="highlight accesslog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sadd myset <span class="string">"imooc.com"</span></span><br><span class="line">scard myset</span><br><span class="line">sdiff myset<span class="string">[myset2]</span></span><br><span class="line">sinter myset<span class="string">[myset2]</span></span><br><span class="line">spop myset</span><br><span class="line">srandmember myset</span><br><span class="line">smembers myset</span><br></pre></td></tr></table></figure></p><p><strong>可排序集合</strong><br><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">zadd myset <span class="number">0</span> <span class="string">"python"</span> <span class="number">5</span> <span class="string">"scrapy"</span> <span class="number">35</span> <span class="string">"Django"</span></span><br><span class="line">zrangebyscore myset <span class="number">0</span> <span class="number">3</span></span><br><span class="line">zcount myset <span class="number">5</span> <span class="number">100</span></span><br></pre></td></tr></table></figure></p><p><font color="red">Scrapy-redis 分布式爬虫调试</font><br><strong>Github地址</strong>：<a href="https://github.com/rmax/scrapy-redis" target="_blank" rel="noopener">https://github.com/rmax/scrapy-redis</a><br><strong>安装：</strong><code>pip install redis</code></p><p><strong>建立项目：</strong><br><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">workon <span class="keyword">jobboleArticle</span></span><br><span class="line"><span class="keyword">scrapy </span>startproject <span class="keyword">ScrapyRedisTest</span></span><br></pre></td></tr></table></figure></p><p><strong>设置：</strong><br><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">ROBOTSTXT_OBEY = False</span><br><span class="line">SCHEDULER = <span class="string">"scrapy_redis.scheduler.Scheduler"</span></span><br><span class="line">DUPEFILTER_CLASS = <span class="string">"scrapy_redis.dupefilter.RFPDupeFilter"</span></span><br><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">    'scrapy_redis.pipelines.RedisPipeline': 300</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p><strong>调试：</strong><br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">key *</span><br><span class="line"></span><br><span class="line">type jobbole:requests</span><br><span class="line"></span><br><span class="line">zrangebyscore jobbole:requests 0 100`</span><br></pre></td></tr></table></figure></p><p>集成 <code>bloomfilter</code> 到 <code>scrapy-redis</code><br><a href="https://github.com/liyaopinner/BloomFilter_imooc" target="_blank" rel="noopener">https://github.com/liyaopinner/BloomFilter_imooc</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://dongfei.oss-cn-shanghai.aliyuncs.com/redis.jpg&quot; alt=&quot;http://dongfei.oss-cn-shanghai.aliyuncs.com/redis.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;Redis 文档 &lt;a href=&quot;http://redisdoc.com/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;http://redisdoc.com/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;font color=&quot;red&quot;&gt;安装&lt;/font&gt;&lt;br&gt;Redis 官方安装 ：&lt;a href=&quot;https://redis.io/download&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://redis.io/download&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Linux 安装： &lt;code&gt;sudo apt-get install redis-server&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Windows 安装 ：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/MicrosoftArchive/redis/releases&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/MicrosoftArchive/redis/releases&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/ServiceStack/redis-windows&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/ServiceStack/redis-windows&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="数据库" scheme="http://blog.dongfei.xin/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
      <category term="Redis" scheme="http://blog.dongfei.xin/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/Redis/"/>
    
    
      <category term="Redis" scheme="http://blog.dongfei.xin/tags/Redis/"/>
    
  </entry>
  
  <entry>
    <title>TortoiseGit 安装与配置</title>
    <link href="http://blog.dongfei.xin/2018-01-12/TortoiseGit%20%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/"/>
    <id>http://blog.dongfei.xin/2018-01-12/TortoiseGit 安装与配置/</id>
    <published>2018-01-12T01:42:55.000Z</published>
    <updated>2018-01-12T02:44:43.996Z</updated>
    
    <content type="html"><![CDATA[<p>TortoiseGit 简称 <strong>tgit</strong>， 中文名 <strong>海龟Git</strong>。海龟Git只支持神器 Windows 系统，有一个前辈海龟SVN， TortoiseSVN 和 TortoiseGit 都是非常优秀的开源的版本库客户端。分为32位版与64位版并且支持各种语言，包括简体中文。</p><font color="red">下载</font><br>TortoiseGit 下载页面:<br>- <a href="http://download.tortoisegit.org/tgit/" target="_blank" rel="noopener">http://download.tortoisegit.org/tgit/</a><br>- <a href="https://tortoisegit.org/download/" target="_blank" rel="noopener">https://tortoisegit.org/download/</a><br><br>根据 Windows 操作系统版本选择相应的程序安装包和中文语言包<br><img src="http://dongfei.oss-cn-shanghai.aliyuncs.com/TortoiseGit%20%E4%B8%8B%E8%BD%BD.png" alt=""><br><a id="more"></a><br><font color="red">安装程序</font><p>先安装程序包，然后安装语言包（LanguagePack）。 因为 TortoiseGit 只是一个程序壳，必须依赖一个 Git Core，所以必须要安装的 <a href="https://git-scm.com/downloads" target="_blank" rel="noopener">Git</a>。下面以64位版本为演示（64，32位除文件名不一样，其他的操作都一致）</p><p>双击安装程序 <code>TortoiseGit-2.3.0.0-64bit.msi</code> 弹出安装向导界面:<br><img src="http://dongfei.oss-cn-shanghai.aliyuncs.com/203_tgitWizard.png" alt="http://dongfei.oss-cn-shanghai.aliyuncs.com/203_tgitWizard.png"></p><p>下一步，进入版权信息界面、 直接点击下一步（Next）即可。<br><img src="http://dongfei.oss-cn-shanghai.aliyuncs.com/204_tgit_License.png" alt="http://dongfei.oss-cn-shanghai.aliyuncs.com/204_tgit_License.png"><br>下一步，选择SSH客户端、 可以选择 TortoiseGitPlink（位于TortoiseGit安装目录<code>/bin</code> 下）， 也可以选择 Git 默认的 SSH 客户端，位于 Git安装目录<code>/bin/ssh.exe</code>（如果配置了 Path，那直接是 ssh.exe）<br><img src="http://dongfei.oss-cn-shanghai.aliyuncs.com/204_2_tgit_Network.png" alt="http://dongfei.oss-cn-shanghai.aliyuncs.com/204_2_tgit_Network.png"><br>接着是选择安装目录，可以保持默认，或者安装到开发环境目录下，安装的程序组件保持默认即可：<br><img src="http://dongfei.oss-cn-shanghai.aliyuncs.com/205_tgit_dir.png" alt="http://dongfei.oss-cn-shanghai.aliyuncs.com/205_tgit_dir.png"><br>下一步到确认安装界面，点击 Install 按钮安装即可，如下图所示：<br><img src="http://dongfei.oss-cn-shanghai.aliyuncs.com/206_install_tgit.png" alt="http://dongfei.oss-cn-shanghai.aliyuncs.com/206_install_tgit.png"><br>安装完成，点击 Finish 按钮即可：<br><img src="http://dongfei.oss-cn-shanghai.aliyuncs.com/207_tgit_installed.png" alt="http://dongfei.oss-cn-shanghai.aliyuncs.com/207_tgit_installed.png"><br>如果以前有老版本，则选择覆盖，关闭旧程序并尝试重启即可</p><font color="red">安装语言包</font><p>双击打开 <code>TortoiseGit-LanguagePack-2.3.0.0-64bit-zh_CN.msi</code>，则弹出语言包安装向导：<br><img src="http://dongfei.oss-cn-shanghai.aliyuncs.com/208_LanguageWizard.png" alt="http://dongfei.oss-cn-shanghai.aliyuncs.com/208_LanguageWizard.png"></p><p>点击下一步， 语言包会自动安装完成：<br><img src="http://dongfei.oss-cn-shanghai.aliyuncs.com/209_LangPackFinished.png" alt="http://dongfei.oss-cn-shanghai.aliyuncs.com/209_LangPackFinished.png"><br>点击完成按钮即可。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;TortoiseGit 简称 &lt;strong&gt;tgit&lt;/strong&gt;， 中文名 &lt;strong&gt;海龟Git&lt;/strong&gt;。海龟Git只支持神器 Windows 系统，有一个前辈海龟SVN， TortoiseSVN 和 TortoiseGit 都是非常优秀的开源的版本库客户端。分为32位版与64位版并且支持各种语言，包括简体中文。&lt;/p&gt;
&lt;font color=&quot;red&quot;&gt;下载&lt;/font&gt;&lt;br&gt;TortoiseGit 下载页面:&lt;br&gt;- &lt;a href=&quot;http://download.tortoisegit.org/tgit/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;http://download.tortoisegit.org/tgit/&lt;/a&gt;&lt;br&gt;- &lt;a href=&quot;https://tortoisegit.org/download/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://tortoisegit.org/download/&lt;/a&gt;&lt;br&gt;&lt;br&gt;根据 Windows 操作系统版本选择相应的程序安装包和中文语言包&lt;br&gt;&lt;img src=&quot;http://dongfei.oss-cn-shanghai.aliyuncs.com/TortoiseGit%20%E4%B8%8B%E8%BD%BD.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;
    
    </summary>
    
      <category term="工具助手" scheme="http://blog.dongfei.xin/categories/%E5%B7%A5%E5%85%B7%E5%8A%A9%E6%89%8B/"/>
    
    
      <category term="TortoiseGit" scheme="http://blog.dongfei.xin/tags/TortoiseGit/"/>
    
  </entry>
  
  <entry>
    <title>ElasticSearch 系列问题集锦</title>
    <link href="http://blog.dongfei.xin/2017-12-24/ElasticSearch%20%E7%B3%BB%E5%88%97%E9%97%AE%E9%A2%98%E9%9B%86%E9%94%A6/"/>
    <id>http://blog.dongfei.xin/2017-12-24/ElasticSearch 系列问题集锦/</id>
    <published>2017-12-24T02:00:51.000Z</published>
    <updated>2018-03-24T03:04:58.795Z</updated>
    
    <content type="html"><![CDATA[<p><strong>配置FAQ</strong></p><ul><li><a href="https://www.biaodianfu.com/centos-7-install-elasticsearch.html" target="_blank" rel="noopener">Elasticsearch在Centos 7上的安装与配置</a></li><li><a href="http://blog.csdn.net/qq_21387171/article/details/53577115" target="_blank" rel="noopener">elasticsearch常见错误与配置简介 </a></li><li><a href="http://blog.csdn.net/sinat_29581293/article/details/53894033" target="_blank" rel="noopener">ElasticSearch关闭重启命令</a></li></ul><a id="more"></a><p><strong>数据操作</strong></p><ul><li><a href="https://elasticsearch.cn/book/elasticsearch_definitive_guide_2.x/" target="_blank" rel="noopener">Elasticsearch: 权威指南 | Elastic</a></li><li><a href="https://github.com/taskrabbit/elasticsearch-dump" target="_blank" rel="noopener">elasticsearch-dump: Import and export tools for elasticsearch</a></li><li><a href="https://www.zhangluya.com/?p=543" target="_blank" rel="noopener">使用elasticsearch-dump进行es索引数据迁移 – Jesse’s Blog</a></li><li><a href="kms.h3c.com/View.aspx?id=51640">elasticdump 工具使用介绍</a></li><li><a href="https://www.cnblogs.com/yjf512/p/4897294.html" target="_blank" rel="noopener">elasticsearch 查询（match和term）</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;配置FAQ&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://www.biaodianfu.com/centos-7-install-elasticsearch.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Elasticsearch在Centos 7上的安装与配置&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://blog.csdn.net/qq_21387171/article/details/53577115&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;elasticsearch常见错误与配置简介 &lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://blog.csdn.net/sinat_29581293/article/details/53894033&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;ElasticSearch关闭重启命令&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Linux" scheme="http://blog.dongfei.xin/categories/Linux/"/>
    
      <category term="ElasticSearch" scheme="http://blog.dongfei.xin/categories/Linux/ElasticSearch/"/>
    
    
      <category term="Linux" scheme="http://blog.dongfei.xin/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>CentOS7 安装 Python3 和 Pip3</title>
    <link href="http://blog.dongfei.xin/2017-12-14/CentOS7%20%E5%AE%89%E8%A3%85%20Python3%20%E5%92%8C%20Pip3/"/>
    <id>http://blog.dongfei.xin/2017-12-14/CentOS7 安装 Python3 和 Pip3/</id>
    <published>2017-12-14T10:48:27.000Z</published>
    <updated>2017-12-23T06:20:04.814Z</updated>
    
    <content type="html"><![CDATA[<p><code>CentOS 7</code> 默认安装了 <code>Python 2</code>，当需要使用 <code>Python 3</code> 的时候，可以手动下载 Python 源码后编译安装。</p><p><font color="red">一、安装 Python 3</font><br><strong>安装准备</strong><br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sudo mkdir /usr/<span class="built_in">local</span>/python3 <span class="comment"># 创建安装目录</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 下载 Python 源文件</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> wget --no-check-certificate https://www.python.org/ftp/python/3.6.0/Python-3.6.0.tgz</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 注意：wget获取https的时候要加上：--no-check-certificate</span></span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> tar -xzvf Python-3.6.0.tgz <span class="comment"># 解压缩包</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">cd</span> Python-3.6.0 <span class="comment"># 进入解压目录</span></span></span><br></pre></td></tr></table></figure></p><a id="more"></a><p><strong>编译安装</strong><br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sudo ./configure --prefix=/usr/<span class="built_in">local</span>/python3 <span class="comment"># 指定创建的目录</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> sudo make</span></span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> sudo make install</span></span><br></pre></td></tr></table></figure></p><p><strong>配置： 两个版本共存</strong><br>创建 python3 的软链接：<br><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo ln -s <span class="regexp">/usr/</span>local<span class="regexp">/python3/</span>bin<span class="regexp">/python3 /u</span>sr<span class="regexp">/bin/</span>python3</span><br></pre></td></tr></table></figure></p><p>这样就可以通过 <em>python</em> 命令使用 Python 2，<em>python3</em> 来使用 Python 3。<br><strong>修改默认为 Python 3</strong><br>将 /usr/bin 中的 python 备份<br><figure class="highlight mel"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo mv <span class="keyword">python</span> <span class="keyword">python</span>.bak</span><br></pre></td></tr></table></figure></p><p>然后创建 python3 的软链接<br><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo ln -s <span class="regexp">/usr/</span>local<span class="regexp">/python3/</span>bin<span class="regexp">/python3 /u</span>sr<span class="regexp">/bin/</span>python</span><br></pre></td></tr></table></figure></p><p>这样默认的 Python 版本就替换为 Python 3 了。</p><p><strong>因为 yum 使用 Python 2，因此替换为 Python 3 后可能无法正常工作，因此修改 yum 配置文件</strong><br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sudo vim /usr/bin/yum</span></span><br></pre></td></tr></table></figure></p><p>将第一行指定的 python 版本改为 python2.7（<code>#!/usr/bin/python</code> 改为 <code>#!/usr/bin/python2.7</code>）</p><p><font color="red">二、安装 pip</font><br><strong>yum 安装</strong><br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 首先安装 epel 扩展源</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> sudo yum -y install epel-release</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 安装 python-pip</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> sudo yum -y install python-pip</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 清除 cache</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> sudo yum clean all</span></span><br></pre></td></tr></table></figure></p><p>通过这种方式貌似只能安装 pip2，想要安装 Python 3 的 pip，可以通过以下的源代码安装方式。</p><p><strong>源码安装</strong><br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 下载源代码</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> wget --no-check-certificate https://github.com/pypa/pip/archive/9.0.1.tar.gz</span></span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> tar -zvxf 9.0.1 -C pip-9.0.1    <span class="comment"># 解压文件</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">cd</span> pip-9.0.1</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 使用 Python 3 安装</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> python3 setup.py install</span></span><br></pre></td></tr></table></figure></p><p>创建链接<br><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo ln -s <span class="regexp">/usr/</span>local<span class="regexp">/python3/</span>bin<span class="regexp">/pip /u</span>sr<span class="regexp">/bin/</span>pip3</span><br></pre></td></tr></table></figure></p><p>升级 pip<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> pip install --upgrade pip</span></span><br></pre></td></tr></table></figure></p><p><strong>创建 <code>python</code> 虚拟环境</strong><br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">pip3 <span class="keyword">install</span> virtualenv virtualenvwrapper</span><br><span class="line"></span><br><span class="line">mkvirtualenv -p /usr/<span class="keyword">bin</span>/python2<span class="number">.7</span> py2</span><br><span class="line">mkvirtualenv -p /usr/<span class="keyword">local</span>/python3/<span class="keyword">bin</span>/python3<span class="number">.6</span>  py3</span><br></pre></td></tr></table></figure></p><p>参考链接：</p><ul><li><a href="https://blog.huzhifeng.com/2016/09/12/CentOS-7-Install-Python3-pip3/" target="_blank" rel="noopener">CentOS 7.2 安装 Python3 和 pip3</a> </li><li><a href="https://ehlxr.me/2017/01/07/CentOS-7-%E5%AE%89%E8%A3%85-Python3%E3%80%81pip3/" target="_blank" rel="noopener">CentOS 7 安装Python3、pip3</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;code&gt;CentOS 7&lt;/code&gt; 默认安装了 &lt;code&gt;Python 2&lt;/code&gt;，当需要使用 &lt;code&gt;Python 3&lt;/code&gt; 的时候，可以手动下载 Python 源码后编译安装。&lt;/p&gt;
&lt;p&gt;&lt;font color=&quot;red&quot;&gt;一、安装 Python 3&lt;/font&gt;&lt;br&gt;&lt;strong&gt;安装准备&lt;/strong&gt;&lt;br&gt;&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;bash&quot;&gt; sudo mkdir /usr/&lt;span class=&quot;built_in&quot;&gt;local&lt;/span&gt;/python3 &lt;span class=&quot;comment&quot;&gt;# 创建安装目录&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;#&lt;/span&gt;&lt;span class=&quot;bash&quot;&gt; 下载 Python 源文件&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;bash&quot;&gt; wget --no-check-certificate https://www.python.org/ftp/python/3.6.0/Python-3.6.0.tgz&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;#&lt;/span&gt;&lt;span class=&quot;bash&quot;&gt; 注意：wget获取https的时候要加上：--no-check-certificate&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;bash&quot;&gt; tar -xzvf Python-3.6.0.tgz &lt;span class=&quot;comment&quot;&gt;# 解压缩包&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;bash&quot;&gt; &lt;span class=&quot;built_in&quot;&gt;cd&lt;/span&gt; Python-3.6.0 &lt;span class=&quot;comment&quot;&gt;# 进入解压目录&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Python" scheme="http://blog.dongfei.xin/categories/Python/"/>
    
      <category term="环境搭建" scheme="http://blog.dongfei.xin/categories/Python/%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/"/>
    
    
      <category term="Python" scheme="http://blog.dongfei.xin/tags/Python/"/>
    
  </entry>
  
</feed>
