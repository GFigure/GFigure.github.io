<!doctype html><html class="theme-next mist use-motion" lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><script src="//cdn.bootcss.com/pace/1.0.2/pace.min.js"></script><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><meta name="google-site-verification" content="evAx-EdEyUEd-j9zqy2DLYlZNcDZYig2uwZF6K6SOSU"><meta name="baidu-site-verification" content="PqQ6XK9DFN"><link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"><link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css"><link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css"><link href="/css/main.css?v=5.1.1" rel="stylesheet" type="text/css"><meta name="keywords" content="Scrapy,"><link rel="alternate" href="/atom.xml" title="那小子真帅" type="application/atom+xml"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.1"><meta name="description" content="使用 HTTP 代理HTTP 代理服务器可以比作客户端与 Web 服务器（网站）之间的一个信息中转站，客户端发送的 HTTP 请求和 Web 服务器返回的 HTTP 响应通过代理服务器转发给对方，如图所示："><meta name="keywords" content="Scrapy"><meta property="og:type" content="article"><meta property="og:title" content="精通 Scrapy 网络爬虫（代理篇）"><meta property="og:url" content="http://blog.dongfei.xin/2018-04-08/精通-Scrapy-网络爬虫（代理篇）/index.html"><meta property="og:site_name" content="那小子真帅"><meta property="og:description" content="使用 HTTP 代理HTTP 代理服务器可以比作客户端与 Web 服务器（网站）之间的一个信息中转站，客户端发送的 HTTP 请求和 Web 服务器返回的 HTTP 响应通过代理服务器转发给对方，如图所示："><meta property="og:locale" content="zh-Hans"><meta property="og:image" content="https://dongfei.oss-cn-shanghai.aliyuncs.com/blog/proxy1.jpg"><meta property="og:image" content="https://dongfei.oss-cn-shanghai.aliyuncs.com/blog/proxy2.jpg"><meta property="og:image" content="https://dongfei.oss-cn-shanghai.aliyuncs.com/blog/proxy3.jpg"><meta property="og:updated_time" content="2018-04-12T11:22:50.354Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="精通 Scrapy 网络爬虫（代理篇）"><meta name="twitter:description" content="使用 HTTP 代理HTTP 代理服务器可以比作客户端与 Web 服务器（网站）之间的一个信息中转站，客户端发送的 HTTP 请求和 Web 服务器返回的 HTTP 响应通过代理服务器转发给对方，如图所示："><meta name="twitter:image" content="https://dongfei.oss-cn-shanghai.aliyuncs.com/blog/proxy1.jpg"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Mist",sidebar:{position:"right",display:"post",offset:12,offset_float:0,b2t:!1,scrollpercent:!1},fancybox:!0,motion:!0,duoshuo:{userId:"0",author:"博主"},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><link rel="canonical" href="http://blog.dongfei.xin/2018-04-08/精通-Scrapy-网络爬虫（代理篇）/"><script type="text/javascript" src="https://dongfei.oss-cn-shanghai.aliyuncs.com/high/high-animation.js"></script><script type="text/javascript" src="https://dongfei.oss-cn-shanghai.aliyuncs.com/blog/blog.dongfei.xin.js"></script><title>精通 Scrapy 网络爬虫（代理篇） | 那小子真帅</title></head><body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans"><script>!function(e,a,t,n,g,c,o){e.GoogleAnalyticsObject=g,e.ga=e.ga||function(){(e.ga.q=e.ga.q||[]).push(arguments)},e.ga.l=1*new Date,c=a.createElement(t),o=a.getElementsByTagName(t)[0],c.async=1,c.src="https://www.google-analytics.com/analytics.js",o.parentNode.insertBefore(c,o)}(window,document,"script",0,"ga"),ga("create","100009466-1","auto"),ga("send","pageview")</script><script type="text/javascript">var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?8feab02e675ff80af4ab58058d0fcd46";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><div class="container sidebar-position-right page-post-detail"><div class="headband"></div><a href="https://github.com/GFigure"><img style="position:absolute;top:0;left:0;border:0" src="https://dongfei.oss-cn-shanghai.aliyuncs.com/forkme_left_orange_ff7600.png" alt="Fork me on GitHub"></a><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-wrapper"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">那小子真帅</span> <span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle">码农，程序猿，技术控</p></div><div class="site-nav-toggle"><button><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>首页</a></li><li class="menu-item menu-item-archives"><a href="/archives" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>归档</a></li><li class="menu-item menu-item-categories"><a href="/categories" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i><br>分类</a></li><li class="menu-item menu-item-tags"><a href="/tags" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br>标签</a></li><li class="menu-item menu-item-commonweal"><a href="/404.html" rel="section"><i class="menu-item-icon fa fa-fw fa-heartbeat"></i><br>公益404</a></li><li class="menu-item menu-item-about"><a href="/about" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i><br>关于</a></li><li class="menu-item menu-item-search"><a title="小high一下~" style="color:red" rel="alternate" class="mw-harlem_shake_slow wobble shake" href="javascript:shake()"><i class="fa fa-music"></i> &nbsp;&nbsp;High</a></li><li class="menu-item menu-item-search"><a href="javascript:;" class="popup-trigger"><i class="menu-item-icon fa fa-search fa-fw"></i><br>搜索</a></li></ul><div class="site-search"><div class="popup search-popup local-search-popup"><div class="local-search-header clearfix"><span class="search-icon"><i class="fa fa-search"></i> </span><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span><div class="local-search-input-wrapper"><input autocapitalize="off" autocomplete="off" autocorrect="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input"></div></div><div id="local-search-result"></div></div></div></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://blog.dongfei.xin/2018-04-08/精通-Scrapy-网络爬虫（代理篇）/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="那小子真帅"><meta itemprop="description" content=""><meta itemprop="image" content="http://oqiflua2i.bkt.clouddn.com/%E5%9B%BE%E7%89%878.png"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="那小子真帅"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">精通 Scrapy 网络爬虫（代理篇）</h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-04-08T14:44:23+08:00">2018-04-08 </time></span><span class="post-category"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Python/" itemprop="url" rel="index"><span itemprop="name">Python</span> </a></span>， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Python/Scrapy/" itemprop="url" rel="index"><span itemprop="name">Scrapy</span> </a></span></span><span class="post-comments-count"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><a class="cloud-tie-join-count" href="/2018-04-08/精通-Scrapy-网络爬虫（代理篇）/#comments" itemprop="discussionUrl"><span class="post-comments-count join-count" itemprop="commentCount"></span> </a></span><span id="/2018-04-08/精通-Scrapy-网络爬虫（代理篇）/" class="leancloud_visitors" data-flag-title="精通 Scrapy 网络爬虫（代理篇）"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">阅读次数 </span><span class="leancloud-visitors-count"></span></span></div></header><div class="post-body" itemprop="articleBody"><h5 id="使用-HTTP-代理"><a href="#使用-HTTP-代理" class="headerlink" title="使用 HTTP 代理"></a>使用 HTTP 代理</h5><p>HTTP 代理服务器可以比作客户端与 Web 服务器（网站）之间的一个信息中转站，客户端发送的 HTTP 请求和 Web 服务器返回的 HTTP 响应通过代理服务器转发给对方，如图所示：</p><p><img src="https://dongfei.oss-cn-shanghai.aliyuncs.com/blog/proxy1.jpg" alt="proxy1"></p><a id="more"></a><p>爬虫程序在爬取某些网站时也需要使用代理。例如：</p><ul><li><p>由于网络环境园素，直接爬取速度太慢，使用代理提高爬取速度。</p></li><li><p>某些网站对用户的访问速度进行限制，爬取过快会被封禁 ip ，使用代理防止被封禁。</p></li><li><p>由于地方法律或政治原因，某些网站无法直接访问，使用代理绕过访问限制。</p></li></ul><h5 id="HttpProxyMiddleware"><a href="#HttpProxyMiddleware" class="headerlink" title="HttpProxyMiddleware"></a>HttpProxyMiddleware</h5><p>Scrapy 内部提供了一个下载中间件 HttpProxyMiddleware ，专门用于给 Scrapy 爬虫设置代理。</p><h5 id="使用简介"><a href="#使用简介" class="headerlink" title="使用简介"></a>使用简介</h5><p>HttpProxyMiddleware 默认便是启用的，它会在系统环境变量中搜索当前系统代理（名字格式为 XXX_proxy 的环境变量），作为 scrapy 爬虫使用的代理。</p><p><a href="http://cn-proxy.com/" target="_blank" rel="noopener">最新中国 ip 地址代理服务器</a></p><p>为本机的 Scrapy 爬虫分别设置发送 HTTP 和 HTTPS 请求时所使用的代理，只需要 bash 中添加加环境变量：<br></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">Linux 系统设置环境变量：</span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">export</span>  http_proxy=<span class="string">"http://120.92.118.64:10010"</span>  <span class="comment"># 为 HTTP 请求设置代理</span></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">export</span>  https_proxy=<span class="string">"http://118.114.77.47:8080"</span>  <span class="comment"># 为 HTTPS 请求设置代理</span></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">export</span>  <span class="comment"># 查看变量</span></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">unset</span> http_proxy  <span class="comment"># 删除变量</span></span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Windows 系统设置环境变量：</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">set</span> http_proxy=<span class="string">"http://120.92.118.64:10010"</span> <span class="comment"># 为 HTTP 请求设置代理</span></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">set</span> https_proxy=<span class="string">"http://118.114.77.47:8080"</span> <span class="comment"># 为 HTTPS 请求设置代理</span></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">set</span> <span class="comment"># 查看变量</span></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">set</span> http_proxy= <span class="comment"># 删除变量</span></span></span><br></pre></td></tr></table></figure><p></p><p>配置完成后，Scrapy 爬虫将会使用上面指定的代理下载页面。</p><p>示例，利用网站 <a href="http://httpbin.org" target="_blank" rel="noopener">http://httpbin.org</a> 提供的服务可以窥视我们所发送的 HTTP(S) 请求，如请求源 IP 地址、请求头部、Cookie 信息等。如图展示了该网站各种服务的 API 地址：</p><p><img src="https://dongfei.oss-cn-shanghai.aliyuncs.com/blog/proxy2.jpg" alt="proxy2"></p><p>访问 <a href="http://httpbin.org/ip" target="_blank" rel="noopener">http://httpbin.org/ip</a> 将返回一个包含请求源 IP 地址信息的 json 串，在 scrapy shell 中访问该 url ，查看请求源 IP 地址：</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">(py3) [root@izuf6g6v8wminmgpw6vd89z py3]<span class="comment"># scrapy shell </span></span><br><span class="line"></span><br><span class="line">。。。。。。。。。。。。</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; import json</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; fetch(scrapy.Request(<span class="string">'http://httpbin.org/ip'</span>))</span><br><span class="line"><span class="number">2018</span>-<span class="number">04</span>-08 <span class="number">22</span><span class="symbol">:</span><span class="number">44</span><span class="symbol">:</span><span class="number">26</span> [scrapy.core.engine] <span class="symbol">INFO:</span> Spider opened</span><br><span class="line"><span class="number">2018</span>-<span class="number">04</span>-08 <span class="number">22</span><span class="symbol">:</span><span class="number">44</span><span class="symbol">:</span><span class="number">26</span> [scrapy.core.engine] <span class="symbol">DEBUG:</span> Crawled (<span class="number">200</span>) &lt;GET <span class="symbol">http:</span>/<span class="regexp">/httpbin.org/ip</span>&gt; (<span class="symbol">referer:</span> None)</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; json.loads(response.text)</span><br><span class="line">&#123;<span class="string">'origin'</span>: <span class="string">'120.92.118.64'</span>&#125;</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; fetch(scrapy.Request(<span class="string">'https://httpbin.org/ip'</span>))</span><br><span class="line"><span class="number">2018</span>-<span class="number">04</span>-08 <span class="number">22</span><span class="symbol">:</span><span class="number">44</span><span class="symbol">:</span><span class="number">52</span> [scrapy.core.engine] <span class="symbol">DEBUG:</span> Crawled (<span class="number">200</span>) &lt;GET <span class="symbol">https:</span>/<span class="regexp">/httpbin.org/ip</span>&gt; (<span class="symbol">referer:</span> None)</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; json.loads(response.text)</span><br><span class="line">&#123;<span class="string">'origin'</span>: <span class="string">'118.114.77.47'</span>&#125;</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt;</span><br></pre></td></tr></table></figure><p>在上述实验中，分别以 HTTP 和 HTTPS 发送请求，使用 json 模块对响应结果进行解析，读取请求源 IP 地址（ origin 字段），其值正是代理服务器的 IP。由此证明，Scrapy 爬虫使用了指定的代理。</p><p>上面使用的是无须身份验证的代理服务器，还有一些代理服务器需要用户提供账号、密码进行身份验证，验证成功后才提供代理服务，使用此类代理时，可按以下格式配置：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="builtin-name">export</span> <span class="attribute">http_proxy</span>=<span class="string">"http://dongfei:123456@39.134.10.98:8080"</span></span><br></pre></td></tr></table></figure><h5 id="源码分析"><a href="#源码分析" class="headerlink" title="源码分析"></a>源码分析</h5><p><strong>HttpProxyMiddleware</strong> ：<code>Lib/site-packages/scrapy/downloadermiddlewares/httpproxy.py</code></p><p>分析代码如下：</p><ul><li>__init__ 方法</li></ul><p>在 HttpProxyMiddleware 的构造器中，使用 Python 标准库 urllib 中的 getproxies 函数在系统环境变量中搜索系统代理的相关配置（变量名格式为 [协议]_proxy 的变量）<br>调用 self._get_proxy 方法解析代理配置信息，并将其返回结果保存到 self.proxies 字段中，如果没有找到任何代理配置，就拋出 NotConfigured 异常，HttpProxyMiddleware 被弃用。</p><ul><li>_get_proxy 方法</li></ul><p>解析代理配置信息，返回身份验证信息以及代理服务器 url。</p><ul><li>process_request 方法</li></ul><p>处理每一个待发送的请求，为没有设置过代理的请求（meta 属性不包含 proxy 的请求）调用 self._set_proxy 方法设置代理。</p><ul><li>_set_proxy 方法</li></ul><p>为一个请求设置代理，以请求的协议（HTTP 或 HTTPS ）作为键，从代理服务器信息字典 self.proxies 中选择代理，赋值给 request.meta 的 proxy 字段。对于需要身份验证的代理服务器，添加 HTTP 头部 Proxy-Authorization ，其值是在 _get_proxy 方法中计算得到的。</p><p>经分析得知，在 Scrapy 中为一个请求设置代理的本质就是将代理服务器的 url 时填写到 request.meta[‘proxy’]。</p><h5 id="使用多个代理"><a href="#使用多个代理" class="headerlink" title="使用多个代理"></a>使用多个代理</h5><p>利用 HttpProxyMiddleware 为爬虫设置代理时，对于一种协议（ HTTP 或 HTTPS ）的所有请求只能使用一个代理，如果想使用多个代理，可以在构造每一个 Request 对象时，通过 meta 参数的 proxy 字段手动设置代理 ：<br></p><figure class="highlight lisp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">request = Request('http<span class="symbol">://example</span>.com/1', meta=&#123;'proxy':'http<span class="symbol">://42</span>.<span class="number">178.202</span>.<span class="number">18</span>:<span class="number">8080</span>'&#125;)</span><br><span class="line">request = Request('http<span class="symbol">://example</span>.com/2', meta=&#123;'proxy':'http<span class="symbol">://182</span>.<span class="number">18.202</span>.<span class="number">18</span>:<span class="number">8080</span>'&#125;)</span><br><span class="line">request = Request('http<span class="symbol">://example</span>.com/3', meta=&#123;'proxy':'http<span class="symbol">://89</span>.<span class="number">190.202</span>.<span class="number">18</span>:<span class="number">8080</span>'&#125;)</span><br></pre></td></tr></table></figure><p></p><p>按照与之前相同的做法，在 scrapy shell 进行实验，验证代理是否被使用：</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">(py3) [root@izuf6g6v8wminmgpw6vd89z py3]<span class="comment"># scrapy shell </span></span><br><span class="line"></span><br><span class="line">。。。。。。。。。。。。</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; import json</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; from scrapy import Request</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; r = Request(<span class="string">'http://httpbin.org/ip'</span>, meta=&#123;<span class="string">'proxy'</span>: <span class="string">'http://39.134.10.18:8080'</span>&#125;)</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; fetch(r)</span><br><span class="line"><span class="number">2018</span>-<span class="number">04</span>-08 <span class="number">19</span><span class="symbol">:</span><span class="number">43</span><span class="symbol">:</span><span class="number">06</span> [scrapy.core.engine] <span class="symbol">DEBUG:</span> Crawled (<span class="number">200</span>) &lt;GET <span class="symbol">http:</span>/<span class="regexp">/httpbin.org/ip</span>&gt; (<span class="symbol">referer:</span> None)</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; json.loads(response.text)</span><br><span class="line">&#123;<span class="string">'origin'</span>: <span class="string">'39.134.10.18'</span>&#125;</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; r = Request(<span class="string">'https://httpbin.org/ip'</span>, meta=&#123;<span class="string">'proxy'</span>: <span class="string">'http://42.178.202.18:8080'</span>&#125;)</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; fetch(r)</span><br><span class="line"><span class="number">2018</span>-<span class="number">04</span>-08 <span class="number">19</span><span class="symbol">:</span><span class="number">44</span><span class="symbol">:</span><span class="number">06</span> [scrapy.core.engine] <span class="symbol">DEBUG:</span> Crawled (<span class="number">200</span>) &lt;GET <span class="symbol">https:</span>/<span class="regexp">/httpbin.org/ip</span>&gt; (<span class="symbol">referer:</span> None)</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; json.loads(response.text)</span><br><span class="line">&#123;<span class="string">'origin'</span>: <span class="string">'42.178.202.18'</span>&#125;</span><br></pre></td></tr></table></figure><p>结果表明，Scrapy 爬虫同样使用了指定的代理服务器。</p><p>使用手动方式设置代理时，如果使用的代理需要身份验证，还需要通过 HTTP 头部的 Proxy-Authorization 字段传递包含用户账号和密码的身份验证信息。可以参考 HttpProxyMiddleware._get_proxy 中的相关实现，按以下过程生成身份验证信息：</p><p>（1）将账号、密码拼接成形如 <code>user:passwd</code> 的字符串 s1。<br>（2）按代理服务器要求对 s1 进行编码（如 utf8 ），生成 s2 。<br>（3）再对 s2 进行 Base64 编码，生成 s3。<br>（4）将 s3 拼接到固定字节串 b<code>Basic</code> 后面，得到最终的身份验证信息。</p><p>示例代码如下：</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;</span>&gt; from scrapy import Request</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; import base64</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; req = Request(<span class="string">'http://httpbin.org/ip'</span>, meta=&#123;<span class="string">'proxy'</span>: <span class="string">'http://42.178.202.18:8080'</span>&#125;)</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; user = <span class="string">'dongfei'</span></span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; passwd = <span class="string">'12345678'</span></span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; user_passwd = (<span class="string">'%s:%s'</span><span class="string">%(user,passwd)</span>).encode(<span class="string">'utf8'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; user_passwd</span><br><span class="line">b<span class="string">'dongfei:12345678'</span></span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; req.headers[<span class="string">'Proxy-Authorization'</span>] = b<span class="string">'Basic'</span> + base64.b64encode(user_passwd)</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; fetch(req)</span><br><span class="line">.........................</span><br></pre></td></tr></table></figure><h5 id="获取免费代理"><a href="#获取免费代理" class="headerlink" title="获取免费代理"></a>获取免费代理</h5><p>可以通过 google 或 baidu 找到一些提供免费代理服务器信息的网站。例如：</p><ul><li><a href="http://proxy-list.org" target="_blank" rel="noopener">http://proxy-list.org</a>（国外）</li><li><a href="http://free-proxy-list.net" target="_blank" rel="noopener">http://free-proxy-list.net</a>（国外）</li><li><a href="http://www.xicidaili.com" target="_blank" rel="noopener">http://www.xicidaili.com</a></li><li><a href="http://www.proxy360.cn" target="_blank" rel="noopener">http://www.proxy360.cn</a></li><li><a href="http://www.kuaidaili.com" target="_blank" rel="noopener">http://www.kuaidaili.com</a></li><li><a href="http://cn-proxy.com/" target="_blank" rel="noopener">http://cn-proxy.com/</a></li></ul><p>以 <a href="http://www.xicidaili.com" target="_blank" rel="noopener">http://www.xicidaili.com</a> 为例，如图所示为该网站 <strong>国内高匿代理</strong> 分类下的页面：</p><p><img src="https://dongfei.oss-cn-shanghai.aliyuncs.com/blog/proxy3.jpg" alt="proxy3"></p><p>接下来爬取 <strong>国内高匿代理</strong> 分类中前 3 页的所有代理服务器信息。并验证每个代理是否可用。</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy genspider xici_proxy www<span class="selector-class">.xicidaili</span><span class="selector-class">.com</span></span><br></pre></td></tr></table></figure><p>该网站会监测用户发送的 HTTP 请求头部中的 User-Agent 字段，因此我们需要伪装成某种常规浏览器，在配置文件添加如下代码：<br></p><figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">custom_settings = &#123;</span><br><span class="line">    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.<span class="number">3112.11</span>3 Safari/537.36',</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>实现 XiciProxySpider 爬取代理服务器信息，并过滤不可用代理，代码如下：</p><figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding:utf<span class="number">-8</span> -*-</span><br><span class="line">__author__ = <span class="string">"東飛"</span></span><br><span class="line">__date__ = <span class="string">"2017-11-29"</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> Request, FormRequest</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> XiciProxySpider(scrapy.Spider):</span><br><span class="line">    name = <span class="string">'xici_proxy'</span></span><br><span class="line">    allowed_domains = [<span class="string">'www.xicidaili.com'</span>]</span><br><span class="line">    start_urls = [<span class="string">'http://www.xicidaili.com/nn/'</span>]</span><br><span class="line"></span><br><span class="line">    custom_settings = &#123;</span><br><span class="line">        <span class="string">"DOWNLOADER_MIDDLEWARES"</span>: &#123;</span><br><span class="line">            # 置于 HttpProxyMiddleware(<span class="number">750</span>) 之前</span><br><span class="line">            # <span class="string">'ArticleSpider.middlewares.RandomUserAgentMiddleware'</span>: <span class="number">745</span>,</span><br><span class="line">            # <span class="string">'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware'</span>: None,</span><br><span class="line">            <span class="string">'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware'</span>: None,</span><br><span class="line">            <span class="string">'scrapy_fake_useragent.middleware.RandomUserAgentMiddleware'</span>: <span class="number">400</span>,</span><br><span class="line">        &#125;,</span><br><span class="line">        # <span class="string">'DOWNLOAD_DELAY'</span>: <span class="number">2</span>,</span><br><span class="line">        <span class="string">'COOKIES_ENABLED'</span>: <span class="literal">False</span>,</span><br><span class="line">        # <span class="string">'DOWNLOAD_TIMEOUT'</span>: <span class="number">180</span>,</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    def start_requests(self):</span><br><span class="line">        # 爬取 http:<span class="comment">//www.xicidaili.com/nn/ 前 3 页</span></span><br><span class="line">        for i <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">3</span>):</span><br><span class="line">            yield Request(<span class="string">'http://www.xicidaili.com/nn/%s'</span> % i)</span><br><span class="line"></span><br><span class="line">    def parse(self, response):</span><br><span class="line"></span><br><span class="line">        for sel <span class="keyword">in</span> response.xpath(<span class="string">'//table[@id="ip_list"]/tr[position()&gt;1]'</span>):</span><br><span class="line">            # 提取代理 IP、port、scheme(http or https)</span><br><span class="line">            ip = sel.css(<span class="string">'td:nth-child(2)::text'</span>).extract_first()</span><br><span class="line">            port = sel.css(<span class="string">'td:nth-child(3)::text'</span>).extract_first()</span><br><span class="line">            scheme = sel.css(<span class="string">'td:nth-child(6)::text'</span>).extract_first().lower()</span><br><span class="line"></span><br><span class="line">            # 使用爬取到的代理再次发送请求到 http(s):<span class="comment">//httpbin.org/ip ，验证代理是否可用</span></span><br><span class="line"></span><br><span class="line">            url = <span class="string">'%s://httpbin.org/ip'</span> % scheme</span><br><span class="line">            # url = <span class="string">'%s://ip.taobao.com/service/getIpInfo.php'</span> % scheme</span><br><span class="line">            # url = <span class="string">'%s://fp.ip-api.com/json'</span> % scheme</span><br><span class="line">            # url = <span class="string">'%s://ip-api.com/json'</span> % scheme</span><br><span class="line"></span><br><span class="line">            proxy = <span class="string">'%s://%s:%s'</span> % (scheme, ip, port)</span><br><span class="line"></span><br><span class="line">            # formdata = &#123;</span><br><span class="line">            #     <span class="string">'ip'</span>: <span class="string">'myip'</span>,</span><br><span class="line">            # &#125;</span><br><span class="line"></span><br><span class="line">            meta = &#123;</span><br><span class="line">                <span class="string">'proxy'</span>: proxy,</span><br><span class="line">                <span class="string">'dont_retry'</span>: <span class="literal">True</span>,</span><br><span class="line">                <span class="string">'download_timeout'</span>: <span class="number">10</span>,</span><br><span class="line"></span><br><span class="line">                # 以下两个字段是传递给 check_available 方法的信息，方便检测</span><br><span class="line">                <span class="string">'_proxy_scheme'</span>: scheme,</span><br><span class="line">                <span class="string">'_proxy_ip'</span>: ip,</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            yield Request(url, callback=self.check_available, meta=meta, dont_filter=<span class="literal">True</span>)</span><br><span class="line">            # yield FormRequest(url, callback=self.check_available, formdata=formdata, meta=meta, dont_filter=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    def check_available(self, response):</span><br><span class="line">        proxy_ip = response.meta[<span class="string">'_proxy_ip'</span>]</span><br><span class="line"></span><br><span class="line">        # <span class="keyword">if</span> proxy_ip == json.loads(response.text)[<span class="string">'data'</span>][<span class="string">'ip'</span>]:</span><br><span class="line">            # <span class="keyword">from</span> pprint <span class="keyword">import</span> pprint</span><br><span class="line">            # pprint(&#123;</span><br><span class="line">            #         <span class="string">'ip'</span>: json.loads(response.text)[<span class="string">'query'</span>],</span><br><span class="line">            #         <span class="string">'user_agent'</span>: json.loads(response.text)[<span class="string">'user_agent'</span>],</span><br><span class="line">            #     &#125;)</span><br><span class="line">            # <span class="keyword">if</span> proxy_ip == json.loads(response.text)[<span class="string">'query'</span>]:</span><br><span class="line">        </span><br><span class="line">        # 判断代理是否具有隐藏 IP 功能</span><br><span class="line">        <span class="keyword">if</span> proxy_ip == json.loads(response.text)[<span class="string">'origin'</span>]:</span><br><span class="line">            yield &#123;</span><br><span class="line">                <span class="string">'proxy_scheme'</span>: response.meta[<span class="string">'_proxy_scheme'</span>],</span><br><span class="line">                <span class="string">'proxy'</span>: response.meta[<span class="string">'proxy'</span>],</span><br><span class="line">            &#125;</span><br></pre></td></tr></table></figure><p>解释上述代码如下：</p><ul><li><p>在 start_requests 方法中请求 <a href="http://www.xicidaili.com/nn/" target="_blank" rel="noopener">http://www.xicidaili.com/nn/</a> 下的前 3 页，以 parse 方法作为页面解析函数。</p></li><li><p>在 parse 方法中提取一个页面中所有的代理服务器信息，这些代理未必都是可用的，因此使用爬取到的代理发送请求到 <code>http(s)://httpbin.org/ip</code> 验证其是否可用。以 check_available 方法作为页面解析函数。</p></li><li><p>能执行到 check_available 方法，意味着 response 对应请求所使用的代理是可用的。在 check_available 方法中，通过响应 json 串中的 origin 字段可以判断代理是否是匿名的（隐藏 ip），返回匿名代理。</p></li></ul><p>运行爬虫，将可用的代理服务器保存到 json 文件中，供其他程序使用：</p><figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">F:\VirtualEnv\jobboleArticle\ArticleSpider\ArticleSpider</span><br><span class="line">(jobboleArticle) $ scrapy crawl xici_proxy -o .\data\proxy_list.json</span><br><span class="line"></span><br><span class="line">....................</span><br><span class="line"></span><br><span class="line">(jobboleArticle) $ cat -n .\data\proxy_list.json</span><br><span class="line">     <span class="number">1</span>  [</span><br><span class="line">     <span class="number">2</span>  &#123;<span class="string">"proxy_scheme"</span>: <span class="string">"http"</span>, <span class="string">"proxy"</span>: <span class="string">"http://183.159.87.96:18118"</span>&#125;,</span><br><span class="line">     <span class="number">3</span>  &#123;<span class="string">"proxy_scheme"</span>: <span class="string">"https"</span>, <span class="string">"proxy"</span>: <span class="string">"https://60.177.228.86:18118"</span>&#125;,</span><br><span class="line">     <span class="number">4</span>  &#123;<span class="string">"proxy_scheme"</span>: <span class="string">"http"</span>, <span class="string">"proxy"</span>: <span class="string">"http://60.177.229.113:18118"</span>&#125;,</span><br><span class="line">     <span class="number">5</span>  &#123;<span class="string">"proxy_scheme"</span>: <span class="string">"http"</span>, <span class="string">"proxy"</span>: <span class="string">"http://183.159.93.180:18118"</span>&#125;,</span><br><span class="line">     <span class="number">6</span>  &#123;<span class="string">"proxy_scheme"</span>: <span class="string">"https"</span>, <span class="string">"proxy"</span>: <span class="string">"https://183.159.83.153:18118"</span>&#125;,</span><br><span class="line">     <span class="number">7</span>  &#123;<span class="string">"proxy_scheme"</span>: <span class="string">"https"</span>, <span class="string">"proxy"</span>: <span class="string">"https://114.99.29.251:18118"</span>&#125;,</span><br><span class="line">     <span class="number">8</span>  &#123;<span class="string">"proxy_scheme"</span>: <span class="string">"http"</span>, <span class="string">"proxy"</span>: <span class="string">"http://183.159.93.180:18118"</span>&#125;,</span><br><span class="line">     <span class="number">9</span>  &#123;<span class="string">"proxy_scheme"</span>: <span class="string">"https"</span>, <span class="string">"proxy"</span>: <span class="string">"https://183.159.94.136:18118"</span>&#125;,</span><br><span class="line">    <span class="number">10</span>  &#123;<span class="string">"proxy_scheme"</span>: <span class="string">"https"</span>, <span class="string">"proxy"</span>: <span class="string">"https://183.159.93.27:18118"</span>&#125;</span><br><span class="line">    <span class="number">11</span>  ]</span><br></pre></td></tr></table></figure><h5 id="实现随机代理"><a href="#实现随机代理" class="headerlink" title="实现随机代理"></a>实现随机代理</h5><p>某些网站为防止爬虫爬取会对接收到的请求进行监测，如果短时间内接收到了来自同一 IP 的大量请求，就判定该 IP 的主机在使用爬虫程序爬取网站，因而将该 IP 封禁（拒绝请求）。爬虫程序可以使用多个代理对此类网站进行爬取，此时单位时间的访问量会被多个代理分摊，从而避免封禁 IP 。</p><p>下面基于 HttpProxyMiddleware 实现一个随机代理下载中同件。</p><p>在 <code>middlewares.py</code> 中实现 <code>RandomHttpProxyMiddleware</code> 代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.downloadermiddlewares.httpproxy <span class="keyword">import</span> HttpProxyMiddleware</span><br><span class="line"><span class="keyword">from</span> scrapy.exceptions <span class="keyword">import</span> NotConfigured</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RandomHttpProxyMiddleware</span><span class="params">(HttpProxyMiddleware)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, auth_encoding=<span class="string">'latin-1'</span>, proxy_list_file=None)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> proxy_list_file:</span><br><span class="line">            <span class="keyword">raise</span> NotConfigured</span><br><span class="line"></span><br><span class="line">        self.auth_encoding = auth_encoding</span><br><span class="line">        <span class="comment"># 分别用两个列表维护 HTTP 和 HTTPS 代理，&#123;'http':[...],'https':[....]&#125;</span></span><br><span class="line">        self.proxies = defaultdict(list)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 从 json 文件中读取代理服务器信息，填入 self.proxies</span></span><br><span class="line">        <span class="keyword">with</span> open(proxy_list_file) <span class="keyword">as</span> f:</span><br><span class="line">            proxy_list = json.load(f)</span><br><span class="line">            <span class="keyword">for</span> proxy <span class="keyword">in</span> proxy_list:</span><br><span class="line">                scheme = proxy[<span class="string">'proxy_scheme'</span>]</span><br><span class="line">                url = proxy[<span class="string">'proxy'</span>]</span><br><span class="line">                self.proxies[scheme].append(self._get_proxy(url, scheme))</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_crawler</span><span class="params">(cls, crawler)</span>:</span></span><br><span class="line">        <span class="comment"># 从配置文件中读取用户验证信息的编码</span></span><br><span class="line">        auth_encoding = crawler.settings.get(<span class="string">'HTTPPROXY_AUTH_ENCODING'</span>,<span class="string">'latin-1'</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 从配置文件中读取代理服务器列表文件（json）的路径</span></span><br><span class="line">        proxy_list_file = crawler.settings.get(<span class="string">'HTTPPROXY_PROXY_LIST_FILE'</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> cls(auth_encoding,proxy_list_file)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_set_proxy</span><span class="params">(self, request, scheme)</span>:</span></span><br><span class="line">        <span class="comment"># 随机选择一个代理</span></span><br><span class="line">        creds, proxy = random.choice(self.proxies[scheme])</span><br><span class="line">        request.meta[<span class="string">'proxy'</span>] = proxy</span><br><span class="line">        <span class="keyword">if</span> creds:</span><br><span class="line">            request.headers[<span class="string">'Proxy-Authorization'</span>] = <span class="string">b'Basic '</span> + creds</span><br></pre></td></tr></table></figure><p>解释上述代码如下：</p><ul><li><p>仿照 HttpProxyMiddleware 构造器实现 RandomHttpProxyMiddleware 构造器，首先从代理服务器列表文件（配置文件中指定）中读取代理服务器信息。然后将它们按照协议 （ HTTP 或 HTTPS ）分别存入不同列表，由 self.proxies 字典维护。</p></li><li><p>_set_proxy 方法负责为每一个 Request 清单设置代理，覆写 _set_proxy 方法（覆盖基类方法），对于每一个 request ，根据请求协议获取 sdifproxis 中的代理服务器列表，然后从中随机抽取一个代理，赋值给 request.meta[‘proxy’]。</p></li></ul><p>在配置文件 <code>settings.py</code> 或者 <code>custom_settings</code> 中启用 RandomHttpProxyMiddleware ，并指定所要使用的代理服务器列表文件（json 文件），添加代码如下：</p><figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">custom_settings = &#123;</span><br><span class="line">    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.<span class="number">3112.11</span>3 Safari/537.36',</span><br><span class="line">    <span class="string">"DOWNLOADER_MIDDLEWARES"</span>: &#123;</span><br><span class="line">        <span class="meta"># 置于 HttpProxyMiddleware(750) 之前</span></span><br><span class="line">        'ArticleSpider.middlewares.RandomHttpProxyMiddleware': <span class="number">745</span>,</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="meta"># 使用之前在 http:<span class="comment">//www.xicidaili.com/ 网站爬取到的代理</span></span></span><br><span class="line">    'HTTPPROXY_PROXY_LIST_FILE': './data/proxy_list.json',</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>最后编写一个 TestRandomProxySpider 测试该中间件，重复向 发送请求，根据响应的请求源 IP 地址信息判断代理使用情况：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> Request</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TestRandomProxySpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">'test_random_proxy'</span></span><br><span class="line">    allowed_domains = [<span class="string">'httpbin.org'</span>]</span><br><span class="line">    start_urls = [<span class="string">'http://httpbin.org/'</span>]</span><br><span class="line">    custom_settings = &#123;</span><br><span class="line">        <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36'</span>,</span><br><span class="line">        <span class="string">"DOWNLOADER_MIDDLEWARES"</span>: &#123;</span><br><span class="line">            <span class="comment"># 置于 HttpProxyMiddleware(750) 之前</span></span><br><span class="line">            <span class="string">'ArticleSpider.middlewares.RandomHttpProxyMiddleware'</span>: <span class="number">745</span>,</span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="comment"># 使用之前在 http://www.xicidaili.com/ 网站爬取到的代理</span></span><br><span class="line">        <span class="string">'HTTPPROXY_PROXY_LIST_FILE'</span>: <span class="string">'ArticleSpider/data/proxy_list.json'</span>,</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">start_requests</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">            <span class="keyword">yield</span> Request(<span class="string">'http://httpbin.org/ip'</span>, dont_filter=<span class="keyword">True</span>)</span><br><span class="line">            <span class="keyword">yield</span> Request(<span class="string">'https://httpbin.org/ip'</span>, dont_filter=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        print(json.loads(response.text))</span><br></pre></td></tr></table></figure><p>运行爬虫，观察输出：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">F:\VirtualEnv\jobboleArticle\ArticleSpider\ArticleSpider</span><br><span class="line">(jobboleArticle) $ scrapy crawl test_random_proxy</span><br><span class="line"><span class="number">2018</span>-<span class="number">04</span>-<span class="number">09</span> <span class="number">11</span>:<span class="number">20</span>:<span class="number">11</span> [scrapy<span class="selector-class">.core</span><span class="selector-class">.engine</span>] DEBUG: Crawled (<span class="number">200</span>) &lt;GET http:<span class="comment">//httpbin.org/ip&gt; (referer: None)</span></span><br><span class="line">&#123;<span class="string">'origin'</span>: <span class="string">'60.177.229.113'</span>&#125;</span><br><span class="line"><span class="number">2018</span>-<span class="number">04</span>-<span class="number">09</span> <span class="number">11</span>:<span class="number">20</span>:<span class="number">13</span> [scrapy<span class="selector-class">.core</span><span class="selector-class">.engine</span>] DEBUG: Crawled (<span class="number">200</span>) &lt;GET https:<span class="comment">//httpbin.org/ip&gt; (referer: None)</span></span><br><span class="line">&#123;<span class="string">'origin'</span>: <span class="string">'183.159.94.136'</span>&#125;</span><br><span class="line"><span class="number">2018</span>-<span class="number">04</span>-<span class="number">09</span> <span class="number">11</span>:<span class="number">20</span>:<span class="number">14</span> [scrapy<span class="selector-class">.core</span><span class="selector-class">.engine</span>] DEBUG: Crawled (<span class="number">200</span>) &lt;GET https:<span class="comment">//httpbin.org/ip&gt; (referer: None)</span></span><br><span class="line">&#123;<span class="string">'origin'</span>: <span class="string">'60.177.229.113'</span>&#125;</span><br></pre></td></tr></table></figure></div><div></div><div><div style="padding:10px 0;margin:20px auto;width:90%;text-align:center"><div>坚持原创技术分享，您的支持将鼓励我继续创作！</div><button id="rewardButton" disable="enable" onclick='var qr=document.getElementById("QR");"none"===qr.style.display?qr.style.display="block":qr.style.display="none"'><span>赏</span></button><div id="QR" style="display:none"><div id="wechat" style="display:inline-block"><img id="wechat_qr" src="http://oqiflua2i.bkt.clouddn.com/微信支付.png" alt="那小子真帅 WeChat Pay"><p>微信打赏</p></div><div id="alipay" style="display:inline-block"><img id="alipay_qr" src="http://oqiflua2i.bkt.clouddn.com/支付宝支付.png" alt="那小子真帅 Alipay"><p>支付宝打赏</p></div></div></div></div><div></div><footer class="post-footer"><div class="post-tags"><a href="/tags/Scrapy/" rel="tag"># Scrapy</a></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/2018-04-06/精通-Scrapy-网络爬虫（数据库篇）/" rel="next" title="精通 Scrapy 网络爬虫（数据库篇）"><i class="fa fa-chevron-left"></i> 精通 Scrapy 网络爬虫（数据库篇）</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"><a href="/2018-04-11/Scrapinghub-的-Scrapy-技巧系列（一）/" rel="prev" title="Scrapinghub 的 Scrapy 技巧系列（一）">Scrapinghub 的 Scrapy 技巧系列（一） <i class="fa fa-chevron-right"></i></a></div></div></footer></article><div class="post-spread"><div class="jiathis_style"><a class="jiathis_button_tsina"></a> <a class="jiathis_button_tqq"></a> <a class="jiathis_button_weixin"></a> <a class="jiathis_button_cqq"></a> <a class="jiathis_button_douban"></a> <a class="jiathis_button_renren"></a> <a class="jiathis_button_qzone"></a> <a class="jiathis_button_kaixin001"></a> <a class="jiathis_button_copy"></a> <a href="http://www.jiathis.com/share" class="jiathis jiathis_txt jiathis_separator jtico jtico_jiathis" target="_blank"></a> <a class="jiathis_counter_style"></a></div><script type="text/javascript">var jiathis_config={hideMore:!1}</script><script type="text/javascript" src="http://v3.jiathis.com/code/jia.js" charset="utf-8"></script></div></div></div><div class="comments" id="comments"><div id="cloud-tie-wrapper" class="cloud-tie-wrapper"></div><script src="https://img1.cache.netease.com/f2e/tie/yun/sdk/loader.js"></script><script>var cloudTieConfig={url:document.location.href,sourceId:"",productKey:"f466c18e6eea4e24bc686a8ae4514dae",target:"cloud-tie-wrapper"},yunManualLoad=!0;Tie.loader("aHR0cHM6Ly9hcGkuZ2VudGllLjE2My5jb20vcGMvbGl2ZXNjcmlwdC5odG1s",!0)</script></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">文章目录</li><li class="sidebar-nav-overview" data-target="site-overview">站点概览</li></ul><section class="site-overview sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" src="http://oqiflua2i.bkt.clouddn.com/%E5%9B%BE%E7%89%878.png" alt="那小子真帅"><p class="site-author-name" itemprop="name">那小子真帅</p><p class="site-description motion-element" itemprop="description">有酒有肉有朋友，能贫能笑能干架</p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives"><span class="site-state-item-count">113</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/index.html"><span class="site-state-item-count">31</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/index.html"><span class="site-state-item-count">39</span> <span class="site-state-item-name">标签</span></a></div></nav><div class="feed-link motion-element"><a href="/atom.xml" rel="alternate"><i class="fa fa-rss"></i> RSS</a></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/GFigure" target="_blank" title="GitHub"><i class="fa fa-fw fa-globe"></i> GitHub </a></span><span class="links-of-author-item"><a href="https://www.zhihu.com/people/geng-dong-fei" target="_blank" title="知乎"><i class="fa fa-fw fa-bandcamp"></i> 知乎 </a></span><span class="links-of-author-item"><a href="http://www.jianshu.com/u/507e68ab9a5a" target="_blank" title="简书"><i class="fa fa-fw fa-book"></i> 简书 </a></span><span class="links-of-author-item"><a href="mailto:iamdongfei@foxmail.com" target="_blank" title="邮箱"><i class="fa fa-fw fa-envelope"></i> 邮箱 </a></span><span class="links-of-author-item"><a href="https://www.facebook.com/profile.php?id=100012742063569" target="_blank" title="Facebook"><i class="fa fa-fw fa-facebook-official"></i> Facebook </a></span><span class="links-of-author-item"><a href="https://plus.google.com/u/0/" target="_blank" title="Google+"><i class="fa fa-fw fa-google-plus-square"></i> Google+ </a></span><span class="links-of-author-item"><a title="嗨一下" style="underline:none;color:red" rel="alternate" class="mw-harlem_shake_slow wobble shake" href="javascript:shake()"><i class="fa fa-music"></i> &nbsp;&nbsp;High</a></span></div><div class="links-of-blogroll motion-element links-of-blogroll-inline"><div class="links-of-blogroll-title"><i class="fa fa-fw fa-globe"></i> 友情链接</div><ul class="links-of-blogroll-list"><li class="links-of-blogroll-item"><a href="http://www.csdn.net/" title="CSDN" target="_blank">CSDN</a></li><li class="links-of-blogroll-item"><a href="https://www.github.com" title="Github" target="_blank">Github</a></li><li class="links-of-blogroll-item"><a href="https://segmentfault.com/" title="Segmentfault" target="_blank">Segmentfault</a></li><li class="links-of-blogroll-item"><a href="https://www.oschina.net/" title="OsChina" target="_blank">OsChina</a></li></ul></div></section><section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class="post-toc"><div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-5"><a class="nav-link" href="#使用-HTTP-代理"><span class="nav-number">1.</span> <span class="nav-text">使用 HTTP 代理</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#HttpProxyMiddleware"><span class="nav-number">2.</span> <span class="nav-text">HttpProxyMiddleware</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#使用简介"><span class="nav-number">3.</span> <span class="nav-text">使用简介</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#源码分析"><span class="nav-number">4.</span> <span class="nav-text">源码分析</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#使用多个代理"><span class="nav-number">5.</span> <span class="nav-text">使用多个代理</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#获取免费代理"><span class="nav-number">6.</span> <span class="nav-text">获取免费代理</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#实现随机代理"><span class="nav-number">7.</span> <span class="nav-text">实现随机代理</span></a></li></ol></div></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2018</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">那小子真帅</span></div><div class="powered-by">由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动</div><div class="theme-info">主题 - <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a></div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script><script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script><script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script><script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script><script type="text/javascript" src="/lib/three/three.min.js"></script><script type="text/javascript" src="/lib/three/three-waves.min.js"></script><script type="text/javascript" src="/js/src/utils.js?v=5.1.1"></script><script type="text/javascript" src="/js/src/motion.js?v=5.1.1"></script><script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.1"></script><script type="text/javascript" src="/js/src/post-details.js?v=5.1.1"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.1"></script><script type="text/javascript">var isfetched=!1,search_path="search.xml";0==search_path.length&&(search_path="search.xml");var path="/"+search_path;function proceedsearch(){$("body").append('<div class="search-popup-overlay local-search-pop-overlay"></div>').css("overflow","hidden"),$(".popup").toggle()}var searchFunc=function(e,c,s){"use strict";$.ajax({url:e,dataType:"xml",async:!0,success:function(e){isfetched=!0,$(".popup").detach().appendTo(".header-inner");var t=$("entry",e).map(function(){return{title:$("title",this).text(),content:$("content",this).text(),url:$("url",this).text()}}).get(),a=document.getElementById(c),r=document.getElementById(s);a.addEventListener("input",function(){var u=0,d='<ul class="search-result-list">',f=this.value.trim().toLowerCase().split(/[\s\-]+/);r.innerHTML="",1<this.value.trim().length&&t.forEach(function(e){var a=!1,r=e.title.trim().toLowerCase(),c=e.content.trim().replace(/<[^>]+>/g,"").toLowerCase(),t=decodeURIComponent(e.url),s=-1,o=-1,n=-1;if(""!=r&&f.forEach(function(e,t){s=r.indexOf(e),o=c.indexOf(e),(0<=s||0<=o)&&(a=!0,0==t&&(n=o))}),a){u+=1,d+="<li><a href='"+t+"' class='search-result-title'>"+r+"</a>";var i=e.content.trim().replace(/<[^>]+>/g,"");if(0<=n){var l=n-20,p=n+80;l<0&&(l=0),0==l&&(p=50),p>i.length&&(p=i.length);var h=i.substring(l,p);f.forEach(function(e){var t=new RegExp(e,"gi");h=h.replace(t,'<b class="search-keyword">'+e+"</b>")}),d+='<p class="search-result">'+h+"...</p>"}d+="</li>"}}),d+="</ul>",0==u&&(d='<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'),""==f&&(d='<div id="no-result"><i class="fa fa-search fa-5x" /></div>'),r.innerHTML=d}),proceedsearch()}})};$(".popup-trigger").click(function(e){e.stopPropagation(),0==isfetched?searchFunc(path,"local-search-input","local-search-result"):proceedsearch()}),$(".popup-btn-close").click(function(e){$(".popup").hide(),$(".local-search-pop-overlay").remove(),$("body").css("overflow","")}),$(".popup").click(function(e){e.stopPropagation()})</script><script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.1.js"></script><script>AV.initialize("cmVXt6URwS0fSRTsfStnNLTi-gzGzoHsz","O8mac9LovG7JWC2ucYMbj7pM")</script><script>function showTime(e){var t=new AV.Query(e),c=[],u=$(".leancloud_visitors");u.each(function(){c.push($(this).attr("id").trim())}),t.containedIn("url",c),t.find().done(function(e){var t=".leancloud-visitors-count";if(0!==e.length){for(var n=0;n<e.length;n++){var o=e[n],i=o.get("url"),s=o.get("time"),r=document.getElementById(i);$(r).find(t).text(s)}for(n=0;n<c.length;n++){i=c[n],r=document.getElementById(i);var l=$(r).find(t);""==l.text()&&l.text(0)}}else u.find(t).text(0)}).fail(function(e,t){console.log("Error: "+t.code+" "+t.message)})}function addCount(i){var e=$(".leancloud_visitors"),s=e.attr("id").trim(),r=e.attr("data-flag-title").trim(),t=new AV.Query(i);t.equalTo("url",s),t.find({success:function(e){if(0<e.length){var t=e[0];t.fetchWhenSave(!0),t.increment("time"),t.save(null,{success:function(e){$(document.getElementById(s)).find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to save Visitor num, with error message: "+t.message)}})}else{var n=new i,o=new AV.ACL;o.setPublicReadAccess(!0),o.setPublicWriteAccess(!0),n.setACL(o),n.set("title",r),n.set("url",s),n.set("time",1),n.save(null,{success:function(e){$(document.getElementById(s)).find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to create")}})}},error:function(e){console.log("Error:"+e.code+" "+e.message)}})}$(function(){var e=AV.Object.extend("Counter");1==$(".leancloud_visitors").length?addCount(e):1<$(".post-title-link").length&&showTime(e)})</script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(t,s)}()</script><script type="text/javascript" src="/js/src/love.js"></script></body></html>