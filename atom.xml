<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>那小子真帅</title>
  
  <subtitle>码农，程序猿，技术控</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://blog.dongfei.xin/"/>
  <updated>2018-05-06T01:37:11.866Z</updated>
  <id>http://blog.dongfei.xin/</id>
  
  <author>
    <name>那小子真帅</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>遇见</title>
    <link href="http://blog.dongfei.xin/2018-05-06/%E9%81%87%E8%A7%81/"/>
    <id>http://blog.dongfei.xin/2018-05-06/遇见/</id>
    <published>2018-05-05T17:10:30.000Z</published>
    <updated>2018-05-06T01:37:11.866Z</updated>
    
    <content type="html"><![CDATA[<p><blockquote class="blockquote-center">每个人的心里都有一团火，路过的人只看到烟。<br>但是总有一个人，总有那么一个人能看到这火，然后走过来，陪我一起。<br></blockquote><br><img src="https://dongfei.oss-cn-shanghai.aliyuncs.com/blog/photo-1496660067708-010ebdd7ce72.jpeg?x-oss-process=style/regular-image-01" alt="photo-1496660067708-010ebdd7ce72.jpeg"><br>我在人群中，看到了她的火，我快步走过去，<br>生怕慢一点，她就会被淹没在岁月的尘埃里。<br>我带着，我的热情，我的冷漠，我的狂暴，我的温和，<br>以及对爱情毫无理由的相信，走的上气不接下气。<br>我倍紧张地问她，可以认识你吗？<br>从你微笑地那一刻开始。<br>后来，有了一切。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;blockquote class=&quot;blockquote-center&quot;&gt;每个人的心里都有一团火，路过的人只看到烟。&lt;br&gt;但是总有一个人，总有那么一个人能看到这火，然后走过来，陪我一起。&lt;br&gt;&lt;/blockquote&gt;&lt;br&gt;&lt;img src=&quot;https://don
      
    
    </summary>
    
      <category term="生活经历" scheme="http://blog.dongfei.xin/categories/%E7%94%9F%E6%B4%BB%E7%BB%8F%E5%8E%86/"/>
    
    
      <category term="她" scheme="http://blog.dongfei.xin/tags/%E5%A5%B9/"/>
    
  </entry>
  
  <entry>
    <title>Python3 人工智能之图片识别（tesseract-ocr）</title>
    <link href="http://blog.dongfei.xin/2018-05-02/Python3-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B9%8B%E5%9B%BE%E7%89%87%E8%AF%86%E5%88%AB/"/>
    <id>http://blog.dongfei.xin/2018-05-02/Python3-人工智能之图片识别/</id>
    <published>2018-05-02T03:16:45.000Z</published>
    <updated>2018-05-05T11:40:39.777Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://dongfei.oss-cn-shanghai.aliyuncs.com/blog/tesseract3.jpg" alt="tesseract3"></p><h5 id="安装识别引擎-tesseract-ocr"><a href="#安装识别引擎-tesseract-ocr" class="headerlink" title="安装识别引擎 tesseract-ocr"></a>安装识别引擎 <code>tesseract-ocr</code></h5><h6 id="下载安装"><a href="#下载安装" class="headerlink" title="下载安装"></a>下载安装</h6><p>安装包下载：<a href="https://zh.osdn.net/projects/sfnet_tesseract-ocr-alt/downloads/tesseract-ocr-setup-3.02.02.exe" target="_blank" rel="noopener">https://zh.osdn.net/projects/sfnet_tesseract-ocr-alt/downloads/tesseract-ocr-setup-3.02.02.exe</a></p><p>GitHub 地址：<a href="https://github.com/tesseract-ocr/" target="_blank" rel="noopener">https://github.com/tesseract-ocr/</a></p><h6 id="设置环境变量"><a href="#设置环境变量" class="headerlink" title="设置环境变量"></a>设置环境变量</h6><ul><li><p><code>PATH</code> </p><figure class="highlight fortran"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">D:\<span class="function"><span class="keyword">Program</span></span> Files (x86)\Tesseract-OCR\</span><br></pre></td></tr></table></figure></li><li><p><code>TESSDATA_PREFIX</code></p><figure class="highlight taggerscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">D:<span class="symbol">\P</span>rogram Files (x86)<span class="symbol">\T</span>esseract-OCR<span class="symbol">\t</span>essdata</span><br></pre></td></tr></table></figure></li></ul><a id="more"></a><h6 id="查看安装"><a href="#查看安装" class="headerlink" title="查看安装"></a>查看安装</h6><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">C:\Users\29485</span><br><span class="line">$ tesseract                                                                                 </span><br><span class="line">Usage:                                                                                      </span><br><span class="line">  tesseract --help | --help-psm | --help-oem | --version                                    </span><br><span class="line">  tesseract --list-langs [--tessdata-dir PATH]                                              </span><br><span class="line">  tesseract --print-parameters [options<span class="built_in">..</span>.] [configfile<span class="built_in">..</span>.]                                 </span><br><span class="line">  tesseract imagename|stdin outputbase|stdout [options<span class="built_in">..</span>.] [configfile<span class="built_in">..</span>.]                  </span><br><span class="line">                                                                                            </span><br><span class="line">OCR options:                                                                                </span><br><span class="line">  --tessdata-dir PATH   Specify the location of tessdata path.                              </span><br><span class="line">  --user-words PATH     Specify the location of<span class="built_in"> user </span>words file.                            </span><br><span class="line">  --user-patterns PATH  Specify the location of<span class="built_in"> user </span>patterns file.                         </span><br><span class="line">  -l LANG[+LANG]        Specify language(s) used <span class="keyword">for</span> OCR.                                   </span><br><span class="line">  -c <span class="attribute">VAR</span>=VALUE          <span class="builtin-name">Set</span> value <span class="keyword">for</span><span class="built_in"> config </span>variables.                                     </span><br><span class="line">                        Multiple -c arguments are allowed.                                  </span><br><span class="line">  --psm NUM             Specify<span class="built_in"> page </span>segmentation mode.                                     </span><br><span class="line">  --oem NUM             Specify OCR Engine mode.                                            </span><br><span class="line">NOTE: These options must occur before any configfile.                                       </span><br><span class="line">                                                                                            </span><br><span class="line">Page segmentation modes:                                                                    </span><br><span class="line">  0    Orientation <span class="keyword">and</span><span class="built_in"> script </span>detection (OSD) only.                                         </span><br><span class="line">  1    Automatic<span class="built_in"> page </span>segmentation with OSD.                                                </span><br><span class="line">  2    Automatic<span class="built_in"> page </span>segmentation, but <span class="literal">no</span> OSD, <span class="keyword">or</span> OCR.                                     </span><br><span class="line">  3    Fully automatic<span class="built_in"> page </span>segmentation, but <span class="literal">no</span> OSD. (Default)                             </span><br><span class="line">  4    Assume a single column of text of variable sizes.                                    </span><br><span class="line">  5    Assume a single uniform block of vertically aligned text.                            </span><br><span class="line">  6    Assume a single uniform block of text.                                               </span><br><span class="line">  7    Treat the image as a single text line.                                               </span><br><span class="line">  8    Treat the image as a single word.                                                    </span><br><span class="line">  9    Treat the image as a single word <span class="keyword">in</span> a circle.                                        </span><br><span class="line"> 10    Treat the image as a single character.                                               </span><br><span class="line"> 11    Sparse text. <span class="builtin-name">Find</span> as much text as possible <span class="keyword">in</span> <span class="literal">no</span> particular order.                   </span><br><span class="line"> 12    Sparse text with OSD.                                                                </span><br><span class="line"> 13   <span class="built_in"> Raw </span>line. Treat the image as a single text line,                                     </span><br><span class="line">                        bypassing hacks that are Tesseract-specific.                        </span><br><span class="line">OCR Engine modes:                                                                           </span><br><span class="line">  0    Original Tesseract only.                                                             </span><br><span class="line">  1    Neural nets LSTM only.                                                               </span><br><span class="line">  2    Tesseract + LSTM.                                                                    </span><br><span class="line">  3    Default, based on what is available.                                                 </span><br><span class="line">                                                                                            </span><br><span class="line">Single options:                                                                             </span><br><span class="line">  -h, --help            Show this help message.                                             </span><br><span class="line">  --help-psm            Show<span class="built_in"> page </span>segmentation modes.                                       </span><br><span class="line">  --help-oem            Show OCR Engine modes.                                              </span><br><span class="line">  -v, --version         Show version information.                                           </span><br><span class="line">  --list-langs          List available languages <span class="keyword">for</span> tesseract engine.                      </span><br><span class="line">  --print-parameters    <span class="builtin-name">Print</span> tesseract parameters.</span><br></pre></td></tr></table></figure><h6 id="tesseract-基本使用"><a href="#tesseract-基本使用" class="headerlink" title="tesseract 基本使用"></a><code>tesseract</code> 基本使用</h6><p>对下面的图像中微软雅黑字体的中文进行识别</p><p><img src="https://dongfei.oss-cn-shanghai.aliyuncs.com/blog/tesseract.png" alt="tesseract"></p><p><strong>-l</strong> ： 指定识别语言</p><p><strong>–psm</strong> ：指定识别对象属性，如果要识别的图像中文字的分布是只有一行，就是用<code>--psm 7</code></p><figure class="highlight taggerscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">C:<span class="symbol">\U</span>sers<span class="symbol">\2</span>9485<span class="symbol">\D</span>esktop</span><br><span class="line">$ tesseract tesseract.png output_sim -l chi_sim --psm 7</span><br><span class="line">Tesseract Open Source OCR Engine v4.00.00alpha with Leptonica</span><br><span class="line"></span><br><span class="line">C:<span class="symbol">\U</span>sers<span class="symbol">\2</span>9485<span class="symbol">\D</span>esktop</span><br><span class="line">$ cat output_sim.txt</span><br><span class="line">长 花 短 草 " 贴 河 而 立</span><br></pre></td></tr></table></figure><h5 id="Python-实现图片文字识别"><a href="#Python-实现图片文字识别" class="headerlink" title="Python 实现图片文字识别"></a>Python 实现图片文字识别</h5><h6 id="安装相应的包"><a href="#安装相应的包" class="headerlink" title="安装相应的包"></a>安装相应的包</h6><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip <span class="keyword">install</span> PIL</span><br><span class="line">pip <span class="keyword">install</span> pytesseract</span><br></pre></td></tr></table></figure><h6 id="基本使用"><a href="#基本使用" class="headerlink" title="基本使用"></a>基本使用</h6><p>识别下面古诗词：<br><img src="https://dongfei.oss-cn-shanghai.aliyuncs.com/blog/tesseract2.png" alt="tesseract2.png"></p><figure class="highlight sqf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">C:\Users\<span class="number">29485</span>\Desktop</span><br><span class="line">(py3) $ ipython</span><br><span class="line">Python <span class="number">3.6</span>.<span class="number">3</span> |Anaconda, Inc.| (<span class="keyword">default</span>, Oct <span class="number">15</span> <span class="number">2017</span>, <span class="number">03</span>:<span class="number">27</span>:<span class="number">45</span>) [MSC v.<span class="number">1900</span> <span class="number">64</span> bit (AMD64)]</span><br><span class="line"><span class="built_in">Type</span> <span class="string">'copyright'</span>, <span class="string">'credits'</span> <span class="built_in">or</span> <span class="string">'license'</span> <span class="keyword">for</span> more information</span><br><span class="line">IPython <span class="number">6.3</span>.<span class="number">1</span> -- An enhanced Interactive Python. <span class="built_in">Type</span> <span class="string">'?'</span> <span class="keyword">for</span> help.</span><br><span class="line"></span><br><span class="line"><span class="built_in">In</span> [<span class="number">1</span>]: <span class="keyword">from</span> PIL import <span class="built_in">Image</span></span><br><span class="line">   ...: import pytesseract</span><br><span class="line">   ...: <span class="built_in">text</span>=pytesseract.image_to_string(<span class="built_in">Image</span>.open(<span class="string">'./tesseract2.png'</span>),lang=<span class="string">'chi_sim'</span>)</span><br><span class="line">   ...: print(<span class="built_in">text</span>)</span><br><span class="line">   ...:</span><br><span class="line">李 教</span><br><span class="line"></span><br><span class="line">我 不 再 想 你</span><br><span class="line">我 要 忘 了 你</span><br><span class="line">E</span><br><span class="line">当 了 老 和 尚</span><br><span class="line"></span><br><span class="line">张 爱 玲</span><br><span class="line"></span><br><span class="line">海 上 月 是 天 上 月 ,</span><br><span class="line">眼 前 人 是 心 上 人 。</span><br><span class="line">向 来 心 是 看 客 心 ,</span><br><span class="line">奈 何 人 是 剧 中 人 。</span><br><span class="line"></span><br><span class="line">你 没 有 说 话</span><br><span class="line">你 也 没 有 哭</span><br><span class="line">你 跟 在 身 后</span><br><span class="line">当 了 小 尼 姑</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://dongfei.oss-cn-shanghai.aliyuncs.com/blog/tesseract3.jpg&quot; alt=&quot;tesseract3&quot;&gt;&lt;/p&gt;
&lt;h5 id=&quot;安装识别引擎-tesseract-ocr&quot;&gt;&lt;a href=&quot;#安装识别引擎-tesseract-ocr&quot; class=&quot;headerlink&quot; title=&quot;安装识别引擎 tesseract-ocr&quot;&gt;&lt;/a&gt;安装识别引擎 &lt;code&gt;tesseract-ocr&lt;/code&gt;&lt;/h5&gt;&lt;h6 id=&quot;下载安装&quot;&gt;&lt;a href=&quot;#下载安装&quot; class=&quot;headerlink&quot; title=&quot;下载安装&quot;&gt;&lt;/a&gt;下载安装&lt;/h6&gt;&lt;p&gt;安装包下载：&lt;a href=&quot;https://zh.osdn.net/projects/sfnet_tesseract-ocr-alt/downloads/tesseract-ocr-setup-3.02.02.exe&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://zh.osdn.net/projects/sfnet_tesseract-ocr-alt/downloads/tesseract-ocr-setup-3.02.02.exe&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;GitHub 地址：&lt;a href=&quot;https://github.com/tesseract-ocr/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/tesseract-ocr/&lt;/a&gt;&lt;/p&gt;
&lt;h6 id=&quot;设置环境变量&quot;&gt;&lt;a href=&quot;#设置环境变量&quot; class=&quot;headerlink&quot; title=&quot;设置环境变量&quot;&gt;&lt;/a&gt;设置环境变量&lt;/h6&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;PATH&lt;/code&gt; &lt;/p&gt;
&lt;figure class=&quot;highlight fortran&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;D:\&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;Program&lt;/span&gt;&lt;/span&gt; Files (x86)\Tesseract-OCR\&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;TESSDATA_PREFIX&lt;/code&gt;&lt;/p&gt;
&lt;figure class=&quot;highlight taggerscript&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;D:&lt;span class=&quot;symbol&quot;&gt;\P&lt;/span&gt;rogram Files (x86)&lt;span class=&quot;symbol&quot;&gt;\T&lt;/span&gt;esseract-OCR&lt;span class=&quot;symbol&quot;&gt;\t&lt;/span&gt;essdata&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Python" scheme="http://blog.dongfei.xin/categories/Python/"/>
    
      <category term="人工智能" scheme="http://blog.dongfei.xin/categories/Python/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
    
      <category term="Python" scheme="http://blog.dongfei.xin/tags/Python/"/>
    
      <category term="tesseract-ocr" scheme="http://blog.dongfei.xin/tags/tesseract-ocr/"/>
    
  </entry>
  
  <entry>
    <title>Python 图像处理库 Pillow</title>
    <link href="http://blog.dongfei.xin/2018-05-01/Python-%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E5%BA%93-Pillow/"/>
    <id>http://blog.dongfei.xin/2018-05-01/Python-图像处理库-Pillow/</id>
    <published>2018-05-01T02:22:41.000Z</published>
    <updated>2018-05-01T08:42:09.547Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://dongfei.oss-cn-shanghai.aliyuncs.com/blog/photo-1525033903830-ebcb102dbdab.jpeg?x-oss-process=style/regular-image-01" alt="photo-1525033903830-ebcb102dbdab.jpeg"></p><h5 id="安装第三方库"><a href="#安装第三方库" class="headerlink" title="安装第三方库"></a>安装第三方库</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> pip install imaging</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> pip install numpy</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> pip install numexpr</span></span><br></pre></td></tr></table></figure><h5 id="相关知识简介"><a href="#相关知识简介" class="headerlink" title="相关知识简介"></a>相关知识简介</h5><h6 id="图片模式"><a href="#图片模式" class="headerlink" title="图片模式"></a>图片模式</h6><p><img src="https://dongfei.oss-cn-shanghai.aliyuncs.com/blog/pohto-mode.jpg" alt="pohto-mode"></p><p>我们这里将用到 <code>RGB</code> 与 <code>RGBA</code> ；这两者的区别是 <code>RGBA</code> 有个透明通道，常见的照片格式中 <code>.png</code> 属于 <code>RGBA</code> ，而 <code>.jpg</code> 格式属于 <code>RGB</code> 模式。<br><a id="more"></a></p><h6 id="RGB"><a href="#RGB" class="headerlink" title="RGB"></a>RGB</h6><p><code>RGB</code> 即是代表红、绿、蓝三个通道的颜色，这个标准几乎包括了人类视力所能感知的所有颜色，是目前运用最广的颜色系统之一</p><h6 id="Numpy"><a href="#Numpy" class="headerlink" title="Numpy"></a>Numpy</h6><p><code>Numpy</code> 是 <code>Python</code> 很强大的开源数字扩展库，可以用来处理大型矩阵等等。</p><h6 id="PIL"><a href="#PIL" class="headerlink" title="PIL"></a>PIL</h6><p><code>PIL</code> 是 <code>Python</code> 与图片处理相关的第三方扩展库，可以完成图片的粘贴，拷贝，添加文字，层叠，合并，图片加强，操作像素点等等，非常强大。</p><h5 id="图片层叠的两种方式"><a href="#图片层叠的两种方式" class="headerlink" title="图片层叠的两种方式"></a>图片层叠的两种方式</h5><ul><li>方式一：</li></ul><p>两张照片同一点的像素按照一定比例叠加。假设两张照片同一点的像素分别为 A、B ，则层叠之后该点像素点（ alpha 取值在 0 和 1 之间）为： <code>Aalpha+B(1-alpha)</code></p><ul><li>方式二：</li></ul><p>正片叠底。<code>结果色 = 混合色 * 基色 / 255</code>，PS 中也采用这种方式；正片叠底的特点：明度变化、混合色不会大于 255 ，故结果色一定小于1。混合模式之后必定比基色暗。0 为黑色，若混合两色中有黑色，混合之后必定是黑色。若有白色，则混合色为另外一色的原色。故正片叠底可以改变非黑即白，处于灰度区间的明度，变黑。可以采用操作像素点，提高像素点的亮度。</p><h5 id="详细代码"><a href="#详细代码" class="headerlink" title="详细代码"></a>详细代码</h5><h6 id="常量设置"><a href="#常量设置" class="headerlink" title="常量设置"></a>常量设置</h6><p>首先导入相关文件，如下所示:<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> numpy</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> numexpr</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image, ImageFont, ImageDraw</span><br><span class="line"></span><br><span class="line">STAG = time.time()</span><br><span class="line"></span><br><span class="line">root = <span class="string">""</span>       <span class="comment"># 脚本的根目录</span></span><br><span class="line">W_num = <span class="number">15</span>      <span class="comment"># 一行放多少张照片</span></span><br><span class="line">H_num = <span class="number">15</span>      <span class="comment"># 一列放多少张照片</span></span><br><span class="line">W_size = <span class="number">320</span>    <span class="comment"># 照片宽为多少</span></span><br><span class="line">H_size = <span class="number">180</span>    <span class="comment"># 照片高为多少</span></span><br><span class="line"></span><br><span class="line">alpha = <span class="number">0.5</span> <span class="comment"># 图片透明度</span></span><br><span class="line">aval = []   <span class="comment"># 存放所有照片的路径</span></span><br></pre></td></tr></table></figure></p><h6 id="获取所有照片信息"><a href="#获取所有照片信息" class="headerlink" title="获取所有照片信息"></a>获取所有照片信息</h6><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># name: getAllPhotos</span></span><br><span class="line"><span class="comment"># <span class="doctag">todo:</span> 获得所有照片的路径</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getAllPhotos</span><span class="params">()</span>:</span></span><br><span class="line">    print(<span class="string">"正在获取图片路径..."</span>)</span><br><span class="line">    STA = time.time()</span><br><span class="line">    root = os.getcwd() + <span class="string">"/"</span></span><br><span class="line">    src = root + <span class="string">"/photos/"</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> os.listdir(src):</span><br><span class="line">        <span class="keyword">if</span> os.path.splitext(src + i)[<span class="number">-1</span>] == <span class="string">".jpg"</span> <span class="keyword">or</span> os.path.splitext(src + i)[<span class="number">-1</span>] == <span class="string">".png"</span>:</span><br><span class="line">            aval.append(src + i)</span><br><span class="line">    print(<span class="string">"getAllPhotos Func Time %s"</span> % (time.time() - STA))</span><br></pre></td></tr></table></figure><h6 id="重置图片大小"><a href="#重置图片大小" class="headerlink" title="重置图片大小"></a>重置图片大小</h6><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># name: transfer</span></span><br><span class="line"><span class="comment"># <span class="doctag">todo:</span> 将照片转为一样的大小</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">transfer</span><span class="params">(img_path, dst_width, dst_height)</span>:</span></span><br><span class="line">    print(<span class="string">"正在重置图片大小..."</span>)</span><br><span class="line">    STA = time.time()</span><br><span class="line">    im = Image.open(img_path)</span><br><span class="line">    <span class="keyword">if</span> im.mode != <span class="string">"RGBA"</span>:</span><br><span class="line">        im = im.convert(<span class="string">"RGBA"</span>)</span><br><span class="line">    resized_img = im.resize((dst_width, dst_height), Image.ANTIALIAS)</span><br><span class="line">    resized_img = resized_img.crop((<span class="number">0</span>, <span class="number">0</span>, dst_width, dst_height))</span><br><span class="line">    print(<span class="string">"transfer Func Time %s"</span> % (time.time() - STA))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> resized_img</span><br></pre></td></tr></table></figure><p>由于每张照片的大小是不一样的，这里主要是将照片转为一样的大小。</p><p>由于每张照片的模式既有 <code>RGB</code> ，也有 <code>RGBA</code> ，我们直接利用 <code>PIL</code> 中的 <code>convert</code> 方法，将所有照片的模式都转为 <code>RGBA</code> 。然后在调用 <code>resize</code> 方法，将照片放缩到一样的大小。</p><h6 id="方式一：照片拼接并进行层叠"><a href="#方式一：照片拼接并进行层叠" class="headerlink" title="方式一：照片拼接并进行层叠"></a>方式一：照片拼接并进行层叠</h6><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># name: createNevImg</span></span><br><span class="line"><span class="comment"># <span class="doctag">todo:</span> 创建一张新的照片并保存</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># @<span class="doctag">note:</span> 这里两张照片层叠选择的方法是：</span></span><br><span class="line"><span class="comment">#   假设两张照片的同一点的像素分别为 A，B，则层叠之后该点得像素为( alpha 取值在 0 和 1 之间)：</span></span><br><span class="line"><span class="comment">#   A * alpha + B * (1-alpha)</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createNevImg</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># 正在创建一张新的照片并保存...</span></span><br><span class="line">    iW_size = W_num * W_size</span><br><span class="line">    iH_size = H_num * H_size</span><br><span class="line">    new_img = root + <span class="string">"lyf.jpg"</span></span><br><span class="line">    I = numpy.array(transfer(new_img, iW_size, iH_size))</span><br><span class="line">    I = numexpr.evaluate(<span class="string">"""I*(1-alpha)"""</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(W_num):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(H_num):</span><br><span class="line">            SH = I[(j * H_size):((j + <span class="number">1</span>) * H_size), (i * W_size):((i + <span class="number">1</span>) * W_size)]</span><br><span class="line">            STA = time.time()</span><br><span class="line">            DA = transfer(random.choice(aval), W_size, H_size)</span><br><span class="line">            print(<span class="string">"Cal Func Time %s"</span> % (time.time() - STA))</span><br><span class="line">            res = numexpr.evaluate(<span class="string">"""SH+DA*alpha"""</span>)</span><br><span class="line">            I[(j * H_size):((j + <span class="number">1</span>) * H_size), (i * W_size):((i + <span class="number">1</span>) * W_size)] = res</span><br><span class="line"></span><br><span class="line">    img = Image.fromarray(I.astype(numpy.uint8))</span><br><span class="line">    img = img.convert(<span class="string">"RGB"</span>)</span><br><span class="line">    img.save(<span class="string">"createNevImg_%s_past3_1.jpg"</span> % alpha)</span><br></pre></td></tr></table></figure><p>这部分是本项目的核心，着重介绍一下这里。</p><ul><li><p><code>new_img</code> 是要与拼接之后的照片进行层叠的照片；并利用 <code>numpy.array</code> 的方法将其转换成矩阵 <strong>I</strong> ；注意：<strong>I</strong> 的大小为<code>(W_num*W_size, H_num*H_size)</code>，是重置图片生成矩阵的 <code>W_num*H_num</code> 倍</p></li><li><p><code>numexpr.evaluate</code> 方法，将矩阵中每个元素都乘以 <code>(1-alpha)</code>。</p></li><li><p>具体拼接的实现</p></li></ul><p>由于矩阵 <strong>I</strong> 的规模是伸缩照片生成矩阵的 <code>W_num*H_num</code> 倍，所以我们从左向右，从上向下依次取 <code>(W_size, H_size)</code> 大小的矩阵 <code>SH</code> 。</p><p>计算 <code>SH + DA*alpha</code> ，并将结果放回 <code>SH</code> 在 <strong>I</strong> 矩阵中位置。这里是讲两张照片中相同一点的像素分别乘以 <code>(1-alpha)</code> 、<code>alpha</code> ，然后相加，如此两个照片便层叠在一块。 <code>alpha</code> 的取值可以自己设置，这里设置的是<code>0.5</code> 。</p><p>调用 <code>fromarray</code> 方法将矩阵 <strong>I</strong> 转为图片对象，并保存为 <code>createNevImage_0.5_past3_1.jpg</code> ，图片如下所示：</p><p><img src="https://dongfei.oss-cn-shanghai.aliyuncs.com/blog/createNevImg_0.5_past3_1.png" alt="createNevImg_0.5_past3_1.png"></p><h6 id="方式二：照片拼接并进行层叠"><a href="#方式二：照片拼接并进行层叠" class="headerlink" title="方式二：照片拼接并进行层叠"></a>方式二：照片拼接并进行层叠</h6><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># name: createNevImg</span></span><br><span class="line"><span class="comment"># <span class="doctag">todo:</span> 创造一张新的图片，并保存</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># @<span class="doctag">note:</span> 这里两张照片层叠选择的方法是：正片叠底</span></span><br><span class="line"><span class="comment">#   结果色 = 混合色 * 基色 / 255</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># @description:</span></span><br><span class="line"><span class="comment">#   1 明度变化：混合色不会大于255，故结果色一定小于1，混合模式之后必定比基色暗。</span></span><br><span class="line"><span class="comment">#   0 为黑色，若混合两色中有黑色，混合之后必定是黑色。若有白色，则混合色为另外一色</span></span><br><span class="line"><span class="comment">#   的原色。</span></span><br><span class="line"><span class="comment">#   故正片叠底可以改变非黑即白，处于灰度区间的明度，变黑。</span></span><br><span class="line"><span class="comment">#   可以采用操作像素点，提高像素点的亮度</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createNevImg</span><span class="params">()</span>:</span></span><br><span class="line">    STAA = time.time()</span><br><span class="line">    iW_size = W_num * W_size</span><br><span class="line">    iH_size = H_num * H_size</span><br><span class="line">    I = numpy.array(transfer(root + <span class="string">"lyf.jpg"</span>, iW_size, iH_size)) * <span class="number">1.0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(W_num):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(H_num):</span><br><span class="line">            s = random.choice(aval)</span><br><span class="line">            res = I[j * H_size:(j + <span class="number">1</span>) * H_size, i * W_size:(i + <span class="number">1</span>) * W_size] * numpy.array(</span><br><span class="line">                    transfer(s, W_size, H_size)) / <span class="number">255</span></span><br><span class="line">            I[j * H_size:(j + <span class="number">1</span>) * H_size, i * W_size:(i + <span class="number">1</span>) * W_size] = res</span><br><span class="line"></span><br><span class="line">    img = Image.fromarray(I.astype(numpy.uint8))</span><br><span class="line">    img = img.point(<span class="keyword">lambda</span> i: i * <span class="number">1.5</span>).convert(<span class="string">'RGB'</span>)</span><br><span class="line">    img.save(<span class="string">"createNevImg_%s_past3_2.jpg"</span> % alpha)</span><br><span class="line">    print(<span class="string">"createNevImg Func time %s"</span> % (time.time() - STAA))</span><br></pre></td></tr></table></figure><ul><li>利用公式： <code>结果色 = 混合色 * 基色 / 255</code></li><li>利用 PIL 库对每个像素点进行操作。 <code>img.point(lambda i : i * 1.5)</code> </li><li>保存照片为 <code>createNevImage_0.5_past3_2.jpg</code></li></ul><p><img src="https://dongfei.oss-cn-shanghai.aliyuncs.com/blog/createNevImg_0.5_past3_2.png" alt="createNevImg_0.5_past3_2.png"></p><h6 id="旋转照片"><a href="#旋转照片" class="headerlink" title="旋转照片"></a>旋转照片</h6><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># name: newRotateImage</span></span><br><span class="line"><span class="comment"># <span class="doctag">todo:</span> 将 createnevimg 中得到的照片旋转，粘贴到另外一张照片中</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">newRotateImage</span><span class="params">()</span>:</span></span><br><span class="line">    imName = <span class="string">"createNevImg_%s_past3_1.jpg"</span> % alpha</span><br><span class="line">    print(<span class="string">"正在将图片旋转中..."</span>)</span><br><span class="line">    STA = time.time()</span><br><span class="line">    im = Image.open(imName)</span><br><span class="line">    <span class="comment"># 第一个参数是照片模式，第二个参数是照片的宽高，用元组表示。</span></span><br><span class="line">    im2 = Image.new(<span class="string">"RGB"</span>, (W_size * int(W_num + <span class="number">1</span>), H_size * (H_num + <span class="number">4</span>)))</span><br><span class="line">    <span class="comment"># 第二个参数是放置的中心点</span></span><br><span class="line">    im2.paste(im, (int(<span class="number">0.5</span> * W_size), int(<span class="number">0.8</span> * H_size)))</span><br><span class="line">    im2 = im2.rotate(<span class="number">359</span>)</span><br><span class="line">    im2.save(<span class="string">"newRotateImage_%s_past3.jpg"</span> % alpha)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"newRotateImage Func Time %s"</span> % (time.time() - STA))</span><br></pre></td></tr></table></figure><h6 id="照片中添加文字"><a href="#照片中添加文字" class="headerlink" title="照片中添加文字"></a>照片中添加文字</h6><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># name: writetoimage</span></span><br><span class="line"><span class="comment"># <span class="doctag">todo:</span> 在图片中写祝福语</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">writeToImage</span><span class="params">()</span>:</span></span><br><span class="line">    print(<span class="string">"正在向图片中添加祝福语..."</span>)</span><br><span class="line">    STA = time.time()</span><br><span class="line">    img = Image.open(<span class="string">"newRotateImage_%s_past3.jpg"</span> % alpha)</span><br><span class="line">    <span class="comment"># 第一个参数是字体文件，第二个参数是字体的大小</span></span><br><span class="line">    font = ImageFont.truetype(<span class="string">'xindexingcao57.ttf'</span>, <span class="number">300</span>)</span><br><span class="line">    <span class="comment"># 生成画笔，参数为要再图片中写字的图片对象。</span></span><br><span class="line">    draw = ImageDraw.Draw(img)</span><br><span class="line">    <span class="comment"># draw.ink = R + G256 + B256*256: 来设置画笔的颜色</span></span><br><span class="line">    draw.ink = <span class="number">21</span> + <span class="number">118</span> * <span class="number">256</span> + <span class="number">65</span> * <span class="number">256</span> * <span class="number">256</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># draw.text((0,H_size * 6),unicode("happy every day",'utf-8'),(0,0,0),font=font)</span></span><br><span class="line"></span><br><span class="line">    tHeight = H_num + <span class="number">1</span></span><br><span class="line">    <span class="comment"># 向图片对象中写字，第一个参数为字体的位置，用元组表示，第二参数是内容，第三个参数是字体。</span></span><br><span class="line">    draw.text((W_size * <span class="number">0.5</span>, H_size * tHeight), <span class="string">"happy life written by python"</span>, font=font)</span><br><span class="line"></span><br><span class="line">    img.save(<span class="string">"final_%s_past3.jpg"</span> % alpha)</span><br><span class="line">    <span class="comment"># rgb_im = img.convert('RGB')</span></span><br><span class="line">    <span class="comment"># rgb_im.save('final_past.jpg')</span></span><br><span class="line">    print(<span class="string">"writeToImage Func Time %s"</span> % (time.time() - STA))</span><br></pre></td></tr></table></figure><p><a href="https://github.com/GFigure/Pyfun/tree/master/PIL/picture-cascading" target="_blank" rel="noopener">项目地址</a></p><h5 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h5><h6 id="基础学习"><a href="#基础学习" class="headerlink" title="基础学习"></a>基础学习</h6><ul><li><p><a href="https://pillow.readthedocs.io/en/4.2.x/index.html" target="_blank" rel="noopener">Pillow 官方文档</a></p></li><li><p><a href="http://www.hjqjk.com/2017/Image-processing-library-of-python-pillow.html" target="_blank" rel="noopener">Python 图像处理库: Pillow</a></p></li><li><p><a href="https://liam0205.me/2015/04/22/pil-tutorial-basic-usage/" target="_blank" rel="noopener">PIL 简明教程 - 基本用法</a></p></li><li><p><a href="http://python.jobbole.com/84956/" target="_blank" rel="noopener">Python 图像处理库 Pillow 入门</a></p></li><li><p><a href="https://liam0205.me/2015/05/05/pil-tutorial-imagedraw-and-imagefont/" target="_blank" rel="noopener">PIL 简明教程 - 在现有的图片上涂涂改改</a></p></li><li><p><a href="https://blog.csdn.net/icamera0/article/details/50843172" target="_blank" rel="noopener">Python 图像处理库 PIL 中图像格式转换（一）</a></p></li></ul><h5 id="项目参考"><a href="#项目参考" class="headerlink" title="项目参考"></a>项目参考</h5><h6 id="Python-多张图片拼接与层叠"><a href="#Python-多张图片拼接与层叠" class="headerlink" title="Python 多张图片拼接与层叠"></a>Python 多张图片拼接与层叠</h6><ul><li><p><a href="http://whuhan2013.github.io/blog/2016/09/19/serval-picture-mosaic/" target="_blank" rel="noopener">多张图片拼接与层叠</a></p></li><li><p><a href="https://github.com/suqingdong/Sources/tree/master/Learning/Python/PILDemo/photo_joint" target="_blank" rel="noopener">Github Suqingdong</a></p></li><li><p><a href="https://github.com/dby/photo_joint" target="_blank" rel="noopener">Github dby</a></p></li><li><p><a href="https://github.com/Caesar233/PhotoMixer" target="_blank" rel="noopener">PhotoMixer</a></p></li></ul><h6 id="Itchat-Pillow-实现微信好友头像爬取和拼接"><a href="#Itchat-Pillow-实现微信好友头像爬取和拼接" class="headerlink" title="Itchat + Pillow 实现微信好友头像爬取和拼接"></a>Itchat + Pillow 实现微信好友头像爬取和拼接</h6><ul><li><a href="https://github.com/gzm1997/wxImage" target="_blank" rel="noopener">wxImage</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://dongfei.oss-cn-shanghai.aliyuncs.com/blog/photo-1525033903830-ebcb102dbdab.jpeg?x-oss-process=style/regular-image-01&quot; alt=&quot;photo-1525033903830-ebcb102dbdab.jpeg&quot;&gt;&lt;/p&gt;
&lt;h5 id=&quot;安装第三方库&quot;&gt;&lt;a href=&quot;#安装第三方库&quot; class=&quot;headerlink&quot; title=&quot;安装第三方库&quot;&gt;&lt;/a&gt;安装第三方库&lt;/h5&gt;&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;bash&quot;&gt; pip install imaging&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;bash&quot;&gt; pip install numpy&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;bash&quot;&gt; pip install numexpr&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h5 id=&quot;相关知识简介&quot;&gt;&lt;a href=&quot;#相关知识简介&quot; class=&quot;headerlink&quot; title=&quot;相关知识简介&quot;&gt;&lt;/a&gt;相关知识简介&lt;/h5&gt;&lt;h6 id=&quot;图片模式&quot;&gt;&lt;a href=&quot;#图片模式&quot; class=&quot;headerlink&quot; title=&quot;图片模式&quot;&gt;&lt;/a&gt;图片模式&lt;/h6&gt;&lt;p&gt;&lt;img src=&quot;https://dongfei.oss-cn-shanghai.aliyuncs.com/blog/pohto-mode.jpg&quot; alt=&quot;pohto-mode&quot;&gt;&lt;/p&gt;
&lt;p&gt;我们这里将用到 &lt;code&gt;RGB&lt;/code&gt; 与 &lt;code&gt;RGBA&lt;/code&gt; ；这两者的区别是 &lt;code&gt;RGBA&lt;/code&gt; 有个透明通道，常见的照片格式中 &lt;code&gt;.png&lt;/code&gt; 属于 &lt;code&gt;RGBA&lt;/code&gt; ，而 &lt;code&gt;.jpg&lt;/code&gt; 格式属于 &lt;code&gt;RGB&lt;/code&gt; 模式。&lt;br&gt;
    
    </summary>
    
      <category term="Python" scheme="http://blog.dongfei.xin/categories/Python/"/>
    
      <category term="其他" scheme="http://blog.dongfei.xin/categories/Python/%E5%85%B6%E4%BB%96/"/>
    
    
      <category term="Python" scheme="http://blog.dongfei.xin/tags/Python/"/>
    
      <category term="Pillow" scheme="http://blog.dongfei.xin/tags/Pillow/"/>
    
  </entry>
  
  <entry>
    <title>精通 Scrapy 网络爬虫（动态网页篇）</title>
    <link href="http://blog.dongfei.xin/2018-04-25/%E7%B2%BE%E9%80%9A-Scrapy-%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%EF%BC%88%E5%8A%A8%E6%80%81%E7%BD%91%E9%A1%B5%E7%AF%87%EF%BC%89/"/>
    <id>http://blog.dongfei.xin/2018-04-25/精通-Scrapy-网络爬虫（动态网页篇）/</id>
    <published>2018-04-25T11:26:21.000Z</published>
    <updated>2018-04-25T00:15:39.220Z</updated>
    
    <content type="html"><![CDATA[<blockquote class="blockquote-center">自己不牛逼，认识的人再多也没用</blockquote><h5 id="爬取动态页面"><a href="#爬取动态页面" class="headerlink" title="爬取动态页面"></a>爬取动态页面</h5><p>静态页面的内容始终不变，爬取相对容易，但在现实中，日前绝大多数网站的页面都是动态页面。动态页面中的部分内容是浏览器运行页面中的 JavaScript 脚本动态生成的，爬取相对困难。<br>先来看一个简单的动态页面的例子，在浏览器中打开 <a href="http://quotes.toscrape.com/js/" target="_blank" rel="noopener">Quotes</a></p><p><img src="https://dongfei.oss-cn-shanghai.aliyuncs.com/blog/quotes.jpg" alt="quotes"></p><a id="more"></a><p>页面中有 10 条名人名言，每一条都包含在一个 <code>&lt;div class=&quot;quto&quot;&gt;</code>元素中。现在，我们在 scrapy shell 环境下尝试爬取页面中的名人名言：<br><figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; scrapy shell http:<span class="comment">//quotes.toscrape.com/js/</span></span><br><span class="line">..................</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; response.css(<span class="string">'div.quote'</span>)</span><br><span class="line">[]</span><br><span class="line">&gt;&gt;&gt;</span><br></pre></td></tr></table></figure></p><p>从结果看出。爬取失败了，在页面中没有找到任何包含名人名言的<code>&lt;div class=&quot;quto&quot;&gt;</code>元素，这些<code>&lt;div class=&quot;quto&quot;&gt;</code>就是动态内容，从服务器下载的页面中并不包含它们。浏览器执行了页面中的一段 JavasScipt 代码后，它们才被显示出来。查看源码：<br><img src="https://dongfei.oss-cn-shanghai.aliyuncs.com/blog/jsjsjs.jpg" alt="jsjsjs.jpg"></p><p>上面是动态网页中最简单的一个例子，数据被硬编码于 JavaScript 代码中，实际中更常见的是 JavaScript 通过 HTTP 请求跟网站动态交互获取数据（AJAX），然后使用数据更新 HTML 页面。爬取此类动态网页需要先执行页面中的 JavaScript 代码渲染页面，再进行爬取。下面介绍使用 JavaScript 渲染引擎渲染页面。</p><h5 id="Splash-渲染引擎"><a href="#Splash-渲染引擎" class="headerlink" title="Splash 渲染引擎"></a>Splash 渲染引擎</h5><p>Splash 是 Scrapy 官方推荐的 JavaScript 渲染引擎，它是使用 Webkit 开发的轻量级无界面浏览器，提供基于 HTTP 接口的 JavaScript 渲染服务，支持以下功能:</p><ul><li>为用户返回经过渲染的 HTML 页面或页面截图。</li><li>并发渲染多个页面。</li><li>关闭图片加载，加速渲染。</li><li>在页面中执行用户自定义的 JavaScript 代码。</li><li>执行用户自定义的渲染脚本（lua），功能类似于 PhantomJS。</li></ul><p><a href="https://github.com/scrapy-plugins/scrapy-splash#installation" target="_blank" rel="noopener">项目链接</a></p><p><a href="http://splash.readthedocs.io/en/latest/scripting-tutorial.html" target="_blank" rel="noopener">API 文档</a></p><p>安装 Splash，需要依赖容器 Docker</p><p>Linux 下安装 Docker 和 Splash ：<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-<span class="builtin-name">get</span> install docker</span><br><span class="line">sudo docker pull scrapinghub/splash</span><br></pre></td></tr></table></figure></p><p>Windows 下安装参考：</p><ul><li><a href="https://blog.csdn.net/sanyuedexuanlv/article/details/78759743" target="_blank" rel="noopener">安裝 Docker for Windows</a></li><li><a href="https://www.cnblogs.com/zhxshseu/p/5970a5a763c8fe2b01cd2eb63a8622b2.html" target="_blank" rel="noopener">Docker 使用阿里云docker镜像加速</a></li><li><a href="https://www.cnblogs.com/my8100/p/splash_install.html" target="_blank" rel="noopener">Scrapy相关：Splash 安装 A javascript rendering service 渲染</a></li></ul><p>安装完成后，在本机的 8050 和 8051 端口开启 Splash 服务:   </p><figure class="highlight dns"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">C:\Users\<span class="number">29485</span></span><br><span class="line">$ docker run -p <span class="number">8050:8050</span> -p <span class="number">8051:8051</span> scrapinghub/splash</span><br><span class="line"><span class="number">2018-04-02</span> <span class="number">08</span>:<span class="number">38:30+0000</span> [-] Log opened.</span><br><span class="line"><span class="number">2018-04-02</span> <span class="number">08</span>:<span class="number">38:30.228904</span> [-] Splash version: <span class="number">3</span>.<span class="number">2</span></span><br><span class="line"><span class="number">2018-04-02</span> <span class="number">08</span>:<span class="number">38:30.246529</span> [-] Qt <span class="number">5</span>.<span class="number">9</span>.<span class="number">1</span>, PyQt <span class="number">5</span>.<span class="number">9</span>, WebKit <span class="number">602</span>.<span class="number">1</span>, sip <span class="number">4</span>.<span class="number">19</span>.<span class="number">3</span>, Twisted <span class="number">16</span>.<span class="number">1</span>.<span class="number">1</span>, Lua <span class="number">5</span>.<span class="number">2</span></span><br><span class="line"><span class="number">2018-04-02</span> <span class="number">08</span>:<span class="number">38:30.248380</span> [-] Python <span class="number">3</span>.<span class="number">5</span>.<span class="number">2</span> (default, Nov <span class="number">23</span> <span class="number">2017</span>, <span class="number">16</span>:<span class="number">37</span>:<span class="number">01</span>) [GCC <span class="number">5</span>.<span class="number">4</span>.<span class="number">0 20160609</span>]</span><br><span class="line"><span class="number">2018-04-02</span> <span class="number">08</span>:<span class="number">38:30.249581</span> [-] Open files limit: <span class="number">1048576</span></span><br><span class="line"><span class="number">2018-04-02</span> <span class="number">08</span>:<span class="number">38:30.249750</span> [-] Can't bump open files limit</span><br><span class="line"><span class="number">2018-04-02</span> <span class="number">08</span>:<span class="number">38</span>:<span class="number">30.373421</span> [-] Xvfb is started: ['Xvfb', ':<span class="number">177507167</span>', '-screen', '<span class="number">0</span>', '<span class="number">1024</span>x768x24', '-nolisten', 'tcp']</span><br><span class="line">QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-root'</span><br><span class="line"><span class="number">2018-04-02</span> <span class="number">08</span>:<span class="number">38</span>:<span class="number">30.757496</span> [-] proxy profiles support is enabled, proxy profiles path: /etc/splash/proxy-profiles</span><br><span class="line"><span class="number">2018-04-02</span> <span class="number">08</span>:<span class="number">38</span>:<span class="number">31.028505</span> [-] verbosity=<span class="number">1</span></span><br><span class="line"><span class="number">2018-04-02</span> <span class="number">08</span>:<span class="number">38</span>:<span class="number">31.028650</span> [-] slots=<span class="number">50</span></span><br><span class="line"><span class="number">2018-04-02</span> <span class="number">08</span>:<span class="number">38</span>:<span class="number">31.028836</span> [-] argument_cache_max_entries=<span class="number">500</span></span><br><span class="line"><span class="number">2018-04-02</span> <span class="number">08</span>:<span class="number">38</span>:<span class="number">31.030539</span> [-] Web UI: enabled, Lua: enabled (sandbox: enabled)</span><br><span class="line"><span class="number">2018-04-02</span> <span class="number">08</span>:<span class="number">38</span>:<span class="number">31.030702</span> [-] Server listening on <span class="number">0.0.0.0</span>:<span class="number">8050</span></span><br><span class="line"><span class="number">2018-04-02</span> <span class="number">08</span>:<span class="number">38:31.032203</span> [-] Site starting on <span class="number">8050</span></span><br><span class="line"><span class="number">2018-04-02</span> <span class="number">08</span>:<span class="number">38</span>:<span class="number">31.032426</span> [-] Starting factory &lt;twisted.web.server.Site object at <span class="number">0</span>x7f<span class="number">4d140d57f0</span>&gt;</span><br></pre></td></tr></table></figure><p>Splash 功能丰富，包含多个服务端点，这里只介绍其中两个最常用的端点</p><ul><li><p>render.html<br>提供 JavaScript 页面渲染服务。</p></li><li><p>execute<br>执行用户自定义的渲染脚本（lua），利用该端点可在页面中执行 JavaScript 代码</p></li></ul><h6 id="render-html-端点"><a href="#render-html-端点" class="headerlink" title="render.html 端点"></a>render.html 端点</h6><p>JavaScript 页面渲染服务是 Splash 中最基础的服务，调用方式如下：<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">服务端点             render.html</span><br><span class="line">请求地址             http:<span class="comment">//:locallhost:8050/render.html</span></span><br><span class="line">请求方式             GET / POST</span><br><span class="line">返回类型             html</span><br></pre></td></tr></table></figure></p><p>render.html 端点支持的参数如下所示：<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">参数              是否必选        类型         描述</span><br><span class="line">url                 必选         string    需要渲染页面的 url</span><br><span class="line">timeout             可选         float    渲染页面超时时间     </span><br><span class="line">proxy               可选         string    代理服务器地址 </span><br><span class="line">wait                可选         float     等待页面渲染的时间</span><br><span class="line">images              可选         integer   是否下载图片，默认为 1</span><br><span class="line">js_source           可选         string    用户自定义的 JavaScript 代码的，在页面渲染前执行</span><br></pre></td></tr></table></figure></p><p>这里仅列出部分常用参数，详细内容参见官方文档。<br>下面是使用 requests 库调用 render.html 端点服务对页面 <code>http://quotes.toscrape.com/js/</code> 进行渲染的示例代码。</p><figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import requests</span><br><span class="line">&gt;&gt;&gt; <span class="keyword">from</span> scrapy.selector import Selector</span><br><span class="line">&gt;&gt;&gt; splash_url = <span class="string">"http://localhost:8050/render.html"</span></span><br><span class="line">&gt;&gt;&gt; args = &#123;'url':<span class="string">"http://quotes.toscrape.com/js/"</span>, '<span class="keyword">timeout</span>':<span class="number">5</span>, 'image':<span class="number">0</span>&#125;</span><br><span class="line">&gt;&gt;&gt; response = requests.<span class="keyword">get</span>(splash_url,params=args)</span><br><span class="line">&gt;&gt;&gt; sel = Selector(response)</span><br><span class="line">&gt;&gt;&gt; sel.css('<span class="keyword">div</span>.<span class="literal">quote</span> span.<span class="built_in">text</span>::<span class="built_in">text</span>').extract()</span><br><span class="line">['“The world <span class="keyword">as</span> we have created <span class="keyword">it</span> <span class="keyword">is</span> a process <span class="keyword">of</span> our thinking. It cannot be changed <span class="keyword">without</span> changing our thinking.”', '“It <span class="keyword">is</span> our c</span><br><span class="line">hoices, Harry, <span class="keyword">that</span> show what we truly are, far more than our abilities.”', '“There are only two ways <span class="keyword">to</span> live your life. One <span class="keyword">is</span> <span class="keyword">as</span> tho</span><br><span class="line">ugh nothing <span class="keyword">is</span> a miracle. The other <span class="keyword">is</span> <span class="keyword">as</span> though everything <span class="keyword">is</span> a miracle.”', '“The person, be <span class="keyword">it</span> gentleman <span class="keyword">or</span> lady, who has <span class="keyword">not</span> pleasu</span><br><span class="line">re <span class="keyword">in</span> a good novel, must be intolerably stupid.”', <span class="string">"“Imperfection is beauty, madness is genius and it's better to be absolutely ridicu</span></span><br><span class="line"><span class="string">lous than absolutely boring.”"</span>, '“Try <span class="keyword">not</span> <span class="keyword">to</span> become a man <span class="keyword">of</span> success. Rather become a man <span class="keyword">of</span> value.”', '“It <span class="keyword">is</span> better <span class="keyword">to</span> be hated fo</span><br><span class="line">r what you are than <span class="keyword">to</span> be loved <span class="keyword">for</span> what you are <span class="keyword">not</span>.”', <span class="string">"“I have not failed. I've just found 10,000 ways that won't work.”"</span>, <span class="string">"“A wo</span></span><br><span class="line"><span class="string">man is like a tea bag; you never know how strong it is until it's in hot water.”"</span>, '“A <span class="built_in">day</span> <span class="keyword">without</span> sunshine <span class="keyword">is</span> like, you know, night.</span><br><span class="line">”']</span><br><span class="line">&gt;&gt;&gt;</span><br></pre></td></tr></table></figure><p>在上述代码中，依据文档中的描述设置参数 url、timeout、images，然后发送 HTTP 请求服务接口地址。从运行结果看出，页面渲染成功，我们爬取到了页面中的 10 条名人名言。</p><h6 id="execute-端点"><a href="#execute-端点" class="headerlink" title="execute 端点"></a>execute 端点</h6><p>在爬取某些页面时，我们想在页面中执行一些用户自定义的 JavaScript 代码，例如用 JavaScript 模拟点击页面中的按钮，或调用页面中的 JavaScript 函数与服务器交互。利用 Splash 的 execute 端点提供的服务可以实现这样的功能。</p><figure class="highlight smali"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">服务端点            execute</span><br><span class="line">请求地址            http://localhost:8050/execute</span><br><span class="line">请求方式            POST</span><br><span class="line">返回类型            自定义</span><br></pre></td></tr></table></figure><p>execute 端点支持的参数如下：<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">参数          必选/可选       类型         描述</span><br><span class="line">lua_source      必选          string      用户自定义的 lua 脚本</span><br><span class="line">timeout         可选          float       渲染页面超时时间</span><br><span class="line">proxy           可选          string      代理服务器地址</span><br></pre></td></tr></table></figure></p><p>我们可以将 execute 端点的服务看作一个可用 lua 语言编程的浏览器，功能类似于 PhantomJS。使用时需传递一个用户自定义的 lua 脚本给 Splash,该 lua 脚本中包含用户<br>想要模拟的浏览器行为，例如:</p><ul><li>打开某 url 地址的页面</li><li>等待页面加载及渲染</li><li>执行 JavaScript 代码</li><li>获取 HTTP 响应头部</li><li>获取 Cookie</li></ul><p>下面使用 requests 库调用 execute 端点服务的示例代码：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> requests</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> json</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>lua_script = <span class="string">'''</span></span><br><span class="line"><span class="string"><span class="meta">... </span>function main(splash):</span></span><br><span class="line"><span class="string"><span class="meta">... </span>    splash:go("http://blog.dongfei.xin/")</span></span><br><span class="line"><span class="string"><span class="meta">... </span>    spalsh:wait(0.5)</span></span><br><span class="line"><span class="string"><span class="meta">... </span>    local title = splash:evaljs("document.title")</span></span><br><span class="line"><span class="string"><span class="meta">... </span>    return &#123;title = title&#125;</span></span><br><span class="line"><span class="string"><span class="meta">... </span>end</span></span><br><span class="line"><span class="string"><span class="meta">... </span>'''</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>splash_url = <span class="string">'http://localhost:8050/execute'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>data = json.dumps(&#123;<span class="string">'lua_source'</span>: lua_script&#125;)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>headers = &#123;<span class="string">'content-type'</span>:<span class="string">'application/json'</span>&#125;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>response= requests.post(splash_url,headers=headers,data=data)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>response.content</span><br><span class="line"><span class="string">b'&#123;"title":"那小子真帅"&#125;'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>response.json()</span><br><span class="line">&#123;<span class="string">'title'</span>:<span class="string">'那小子真帅'</span>&#125;</span><br></pre></td></tr></table></figure></p><p>用户自定义的 lua 脚本中必须包含一个 main 函数作为程序入口，main 函数被调用时会传入一个 splash 对象（lua 中的对象），用户可以调用该对象上的方法操纵 Splash 。例如，在上面的例子中，先调用 go 方法打开某页面，再调用 wait 方法等待页面渲染，然后调用 evaljs 方法执行一个 JavaScript 表达式，并将结果转化为相应的 lua 对象，最终 Splash 根据 main 函数的返回值构造 HTTP 响应返回给用户，main 函数的返回值可以是字符串，也可以是 lua 中的表（类似 Python 字典）表会被编码成 json 串。</p><p>splash 对象常用的属性和方法。</p><ul><li>splash.args 属性</li></ul><p>用户传入参数的表，通过该属性可以访问用户传入的参数，如 splash.args.url、<br>splash.args.wait。</p><ul><li>splash.js_enabled 属性</li></ul><p>用于开启/禁止 JavaScript 渲染，默认为 true。</p><ul><li>splash.images_enabled 属性</li></ul><p>用于开启/禁止图片加载，默认为 true。</p><ul><li>splash:go 方法</li></ul><p>splash:go{url,baseurl=nil,headers=nil,http_method=”GET”,body=nil,formdata=nil}类似于在浏览器中打开某 url 地址的页面，页面所需资源会被加载，并进行 JavaScript 渲染，可以通过参数指定 HTTP 请求头部、请求方法、表单数据等。</p><ul><li>splash:wait 方法</li></ul><p>splash:wait{time,cancel_on_redirect=false,cancel_on_error=true} 等待页面渲染，time 参数为等待的秒数。</p><ul><li>splash:evaljs 方法</li></ul><p>splash:evaljs(snippet)<br>在当前页面下，执行一段 JavaScript 代码，并返回最后一句表达式的值。</p><ul><li>splash:runjs 方法</li></ul><p>splash:runjs(snippet)<br>在当前页面下，执行一段 JavaScript 代码，与 evaljs 方法相比，该函数只执行 JavaScript 代码，不返回值。</p><ul><li>splash:url 方法</li></ul><p>splash:url()<br>获取当前页面的 url</p><ul><li>splash:html 方法</li></ul><p>splash:html()<br>获取当前页面的 HTML 文本。</p><ul><li>splash:get_cookies 方法</li></ul><p>splash:get_cookies()<br>获取全部 Cookie 信息。</p><h5 id="在-Scrapy-中使用Splash"><a href="#在-Scrapy-中使用Splash" class="headerlink" title="在 Scrapy 中使用Splash"></a>在 Scrapy 中使用Splash</h5><p>在 Scrapy 中调用 Splash 服务，需要安装 scrapy-splash：<code>pip install scrapy-splash</code></p><p>在项目配置文件 setting.py 中对 scrapy-plash 进行配置：<br><figure class="highlight vala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># Splash服务器地址 </span></span><br><span class="line">SPLASH_URL = <span class="string">'http://localhost:8050/'</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># 开启 Splash 的两个下载中间件并调整 HttpCompressionMiddleware 的次序</span></span><br><span class="line"></span><br><span class="line">DOWNLOADER_MIDDLEWARES = &#123;</span><br><span class="line">    <span class="string">'scrapy_splash.SplashCookiesMiddleware'</span>: <span class="number">723</span>,</span><br><span class="line">    <span class="string">'scrapy_splash.SplashMiddleware'</span>: <span class="number">725</span>,</span><br><span class="line">    <span class="string">'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware'</span>: <span class="number">810</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta"># 设置去重过滤器</span></span><br><span class="line">DUPEFILTER_CLASS = <span class="string">'scrapy_splash.SplashAwareDupeFilter'</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># 用来支持 cache_args （可选）</span></span><br><span class="line">SPIDER_MIDDLEWARES = &#123;</span><br><span class="line">   <span class="string">'scrapy_splash.SplashDeduplicateArgsMiddleware'</span>: <span class="number">100</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>使用 scrapy_splash 调用 Splash 服务非常筒单，scrapy_splash 中定义了一个 SplashRequest 类，用户只需使用 scrapy_splash.SplashRequest （替代scrapy .Request）提交请求即可。下面是 SplashRequest 构造器方法中的一些常用参数。</p><ul><li>url</li></ul><p>与 scrapy.Request 中的 url 相同，也就是待爬取页面的 url（注意，不是Splash 服务器地址）</p><ul><li>headers</li></ul><p>与 scrapy.Request 中的 headers 相同。</p><ul><li>cookies</li></ul><p>与 scrapy.Request 中的 cookies 相同。</p><ul><li>args</li></ul><p>传递给 Splash 的参数（除 url 以外），如 wait、timeout、images、js_source 等。</p><ul><li>cache_args</li></ul><p>如果 args 中的某些参数每次调用都重复传递并且数据量较大（例如一段 JavaScript 代码），此时可以把该参数名填入 cache_args 列表中，让 Splash 服务器缓存该参数，如 <code>SplashRequest(url,args={&#39;js_source&#39;: js,&#39;wait&#39;: 0.5},cache_args=[&#39;js_source&#39;])</code>。</p><ul><li>endpoint</li></ul><p>Splash 服务端点，默认为 <code>render.html</code>，即 JavaScript 页面渲染服务，该参数可以设置为 <code>render.json</code>、<code>render.har</code>、<code>render.png</code>、<code>render.jpeg</code>、<code>execute</code>等，更多服务端点可以查阅文档。</p><ul><li>splash_url</li></ul><p>Splash 服务器地址，默认为 None，即使用配置文件中 SPLASH_URL 的地址。</p><h5 id="项目实战-爬取-toscrape-中的名人名言"><a href="#项目实战-爬取-toscrape-中的名人名言" class="headerlink" title="项目实战: 爬取 toscrape 中的名人名言"></a>项目实战: 爬取 toscrape 中的名人名言</h5><ul><li>项目需求</li></ul><p>爬取网站 <a href="http://quotes.toscrape.com/js/" target="_blank" rel="noopener">http://quotes.toscrape.com/js/</a> 中的名人名言信息。</p><ul><li>编码实现</li></ul><p>项目目录下使用 scrapy genspider 命令创建 Spider：<br><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">scrapy</span> <span class="selector-tag">genspider</span> <span class="selector-tag">quotes</span> <span class="selector-tag">quotes</span><span class="selector-class">.toscrape</span><span class="selector-class">.com</span></span><br></pre></td></tr></table></figure></p><p>这个案例中。我们只使用 Splash 的 render.html 端点渲染页面，再进行爬取即<br>可实现 QuotesSpider，代码如下： </p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line">__author_<span class="number">_</span> = <span class="string">"東飛"</span></span><br><span class="line">__date_<span class="number">_</span> = <span class="string">"2017-11-29"</span></span><br><span class="line">import scrapy</span><br><span class="line">from scrapy_splash import SplashRequest</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">QuotesSpider</span>(<span class="title">scrapy</span>.<span class="title">Spider</span>):</span></span><br><span class="line">    name = <span class="string">'quotes'</span></span><br><span class="line">    allowed_domains = [<span class="string">'quotes.toscrape.com'</span>]</span><br><span class="line">    start_urls = [<span class="string">'http://quotes.toscrape.com/'</span>]</span><br><span class="line"></span><br><span class="line">    custom_settings = &#123;</span><br><span class="line">        <span class="comment"># Splash服务器地址</span></span><br><span class="line">        <span class="string">'SPLASH_URL'</span>: <span class="string">'http://localhost:8050/'</span>,</span><br><span class="line">        <span class="comment"># 开启 Splash 的两个下载中间件并调整 HttpCompressionMiddleware 的次序</span></span><br><span class="line">        <span class="string">'DOWNLOADER_MIDDLEWARES'</span>: &#123;</span><br><span class="line">            <span class="string">'scrapy_splash.SplashCookiesMiddleware'</span>: <span class="number">723</span>,</span><br><span class="line">            <span class="string">'scrapy_splash.SplashMiddleware'</span>: <span class="number">725</span>,</span><br><span class="line">            <span class="string">'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware'</span>: <span class="number">810</span>,</span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="comment"># 设置去重过滤器</span></span><br><span class="line">        <span class="string">'DUPEFILTER_CLASS'</span>: <span class="string">'scrapy_splash.SplashAwareDupeFilter'</span>,</span><br><span class="line">        <span class="comment"># 用来支持 cache_args （可选）</span></span><br><span class="line">        <span class="string">'SPIDER_MIDDLEWARES'</span>: &#123;</span><br><span class="line">            <span class="string">'scrapy_splash.SplashDeduplicateArgsMiddleware'</span>: <span class="number">100</span>,</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">start_requests</span><span class="params">(<span class="keyword">self</span>)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">for</span> url <span class="keyword">in</span> <span class="keyword">self</span>.<span class="symbol">start_urls:</span></span><br><span class="line">            <span class="keyword">yield</span> SplashRequest(url, args=&#123;<span class="string">'images'</span>: <span class="number">0</span>, <span class="string">'timeout'</span>: <span class="number">3</span>&#125;)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(<span class="keyword">self</span>, response)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">for</span> sel <span class="keyword">in</span> response.css(<span class="string">'div.quote'</span>)<span class="symbol">:</span></span><br><span class="line">            quote = <span class="keyword">self</span>.css(<span class="string">'span.text::text'</span>).extract_first()</span><br><span class="line">            author = <span class="keyword">self</span>.css(<span class="string">'small.author::text'</span>).extract_first()</span><br><span class="line">            <span class="keyword">yield</span> &#123;<span class="string">'quote'</span>: quote, <span class="string">'author'</span>: author&#125;</span><br><span class="line"></span><br><span class="line">        href = response.css(<span class="string">'li.next&gt;a::attr(href)'</span>).extract_first()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="symbol">href:</span></span><br><span class="line">            url = response.urljoin(href)</span><br><span class="line">            <span class="keyword">yield</span> SplashRequest(url, args=&#123;<span class="string">'images'</span>: <span class="number">0</span>, <span class="string">'timeout'</span>: <span class="number">3</span>&#125;)</span><br></pre></td></tr></table></figure><p>上述代码中。使用 SplashRequest 提交请求，SplashRequest 的构造器中无须传递<br>endpoint 参数，因为该参数默认值便是 <code>render.html</code> 。使用 args 参数禁止 Splash 加载图片，并设置渲染超时时间。<br>运行爬虫，观察结果：</p><figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"> scrapy crawl quotes -o .\data\quotes.csv</span><br><span class="line"></span><br><span class="line"> F:\VirtualEnv\jobboleArticle\ArticleSpider\ArticleSpider</span><br><span class="line">(jobboleArticle) $ cat -n .\data\quotes.csv</span><br><span class="line">     <span class="number">1</span>  <span class="literal">quote</span>,author</span><br><span class="line">     <span class="number">2</span>  “The world <span class="keyword">as</span> we have created <span class="keyword">it</span> <span class="keyword">is</span> a process <span class="keyword">of</span> our thinking. It cannot be changed <span class="keyword">without</span> changing our thinking.”,Albert Ein</span><br><span class="line">stein</span><br><span class="line">     <span class="number">3</span>  <span class="string">"“It is our choices, Harry, that show what we truly are, far more than our abilities.”"</span>,J.K. Rowling</span><br><span class="line">     <span class="number">4</span>  “There are only two ways <span class="keyword">to</span> live your life. One <span class="keyword">is</span> <span class="keyword">as</span> though nothing <span class="keyword">is</span> a miracle. The other <span class="keyword">is</span> <span class="keyword">as</span> though everything <span class="keyword">is</span> a mirac le.”,Albert Einstein</span><br></pre></td></tr></table></figure><h5 id="项目实战-爬取京东商城中的书籍信息"><a href="#项目实战-爬取京东商城中的书籍信息" class="headerlink" title="项目实战: 爬取京东商城中的书籍信息"></a>项目实战: 爬取京东商城中的书籍信息</h5><ul><li>项目需求</li></ul><p>爬取京东商城中所有 Python 书籍的名字和价格信息。</p><ul><li>页面分析</li></ul><p>在<a href="http://www.jd.com" target="_blank" rel="noopener">京东网站</a>的书籍分类下搜索 Python 关键字得到的页面<br><img src="https://dongfei.oss-cn-shanghai.aliyuncs.com/blog/jdd1.jpg" alt=""></p><p>结果有很多页，在每个书籍列表页面中可以数有 60 本书但在 scrapy shell<br>中爬取该页面时遇到了问题，仅在页面中找到了 30 本书，少了30本，代码如下:<br><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="symbol">F:</span>\VirtualEnv\jobboleArticle\ArticleSpider\ArticleSpider</span><br><span class="line">(jobboleArticle) $ scrapy shell</span><br><span class="line"></span><br><span class="line">.............</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; url = <span class="string">"https://search.jd.com/Search?keyword=python&amp;enc=utf-8&amp;wq=python"</span></span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; fetch(url)</span><br><span class="line"><span class="number">2018</span>-<span class="number">04</span>-<span class="number">06</span> <span class="number">16</span><span class="symbol">:</span><span class="number">52</span><span class="symbol">:</span><span class="number">03</span> [scrapy.core.engine] <span class="symbol">INFO:</span> Spider opened</span><br><span class="line"><span class="number">2018</span>-<span class="number">04</span>-<span class="number">06</span> <span class="number">16</span><span class="symbol">:</span><span class="number">52</span><span class="symbol">:</span><span class="number">04</span> [scrapy.core.engine] <span class="symbol">DEBUG:</span> Crawled (<span class="number">200</span>) &lt;GET <span class="symbol">https:</span>/<span class="regexp">/search.jd.com/</span>Search?keyword=python&amp;enc=utf-<span class="number">8</span>&amp;wq=python&gt; (<span class="symbol">referer:</span> None)</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; len(response.css(<span class="string">'ul.gl-warp&gt;li'</span>))</span><br><span class="line"><span class="number">30</span></span><br></pre></td></tr></table></figure></p><p>原来页面中的 60 本书不是同时加载的，开始只有 30 本书，当我们使用鼠标滚轮滚动到页面下方某位置时，后 30 本书才由 JavaScript 脚本加载，通过实验可以验证这个说法，实验绝视如下：</p><p>（1） 页面刚加载时，在 Chrome 开发者工具的 console 中用 jQuery 代码查看当前<br>有多少本书，此时为 30。</p><p>（2） 之后滚动鼠标滚轮到某一位置时，可以看到 JavaScript 发送 HTTP 请求和服务<br>器交互（XHR）。</p><p>（3） 然用 jQuery 代码查看当前有多少本书，已经变成了 60 ，如图所示，</p><p><img src="https://dongfei.oss-cn-shanghai.aliyuncs.com/blog/jdd2.jpg" alt=""></p><p>既然如此，爬取这个页面时，可以先执行一段 JavaScript 代码，将滚动条拖到页面下方某位置，触发加载后 30 本书的事件，在开发者工具的 console 中进行实验，用 document.getElementsByXXX 方法随意选中页面下方的某元素，比如<code>下一页</code> 按钮所在的<code>&lt;div&gt;</code>元素，然后在该元素对象上调用 scrollIntoView(true) 完成拖曳动作，此时查看书籍数量，变成了 60 。如图：</p><p><img src="https://dongfei.oss-cn-shanghai.aliyuncs.com/blog/jdd3.jpg" alt=""></p><p>爬取一个页面的问题解决了，再宋研究如何从页面中找到下一页的 url 地址。下一页链接的 href 属性并不是一个 url，而在其 onclick 属性中包含了一条 JavaScript 代码，单击<code>下一页</code>按钮时会调函数<code>SEARCH.page(n,true)</code>。虽然<br>可以用 Splash 执行函数来跳转到下一页，但还是很麻烦，经观察发现，每个页面 url 的差异仅在于 page 参数不同，第一页<code>page=1</code>、第二页 <code>page=3</code>、第三页 <code>page=5</code> …… 以 2 递增，并且页面中还包含商品总数信息。因此，我们可以推算出所有页面的 url 。</p><p><img src="https://dongfei.oss-cn-shanghai.aliyuncs.com/blog/jdd4.jpg" alt=""></p><ul><li>编码实现</li></ul><p>项目目录下使用 scrapy genspider 命令创建 Spider：<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy genspider jd_book search<span class="selector-class">.jd</span><span class="selector-class">.com</span></span><br></pre></td></tr></table></figure></p><p>经上述分析，在爬取每一个书籍列表页面时都需要执行一段 JavaScript 代码，以让全部书籍加载，因此选用 Splash 的 execute 端点渲染页面，再进行爬取即<br>可实现 JdBookSpider，代码如下： </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line">__author__ = <span class="string">"東飛"</span></span><br><span class="line">__date__ = <span class="string">"2017-11-29"</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> Request</span><br><span class="line"><span class="keyword">from</span> scrapy_splash <span class="keyword">import</span> SplashRequest</span><br><span class="line"></span><br><span class="line">lua_script = <span class="string">'''</span></span><br><span class="line"><span class="string">function main(splash)</span></span><br><span class="line"><span class="string">    splash:go(splash.args.url)</span></span><br><span class="line"><span class="string">    splash:wait(2)</span></span><br><span class="line"><span class="string">    splash:runjs("document.getElementsByClassName('page')[0].scrollIntoView(true)")</span></span><br><span class="line"><span class="string">    splash:wait(2)</span></span><br><span class="line"><span class="string">    return splash:html()</span></span><br><span class="line"><span class="string">end</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JdBookSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">'jd_book'</span></span><br><span class="line">    allowed_domains = [<span class="string">'search.jd.com'</span>]</span><br><span class="line">    base_url = <span class="string">'https://search.jd.com/Search?keyword=python&amp;enc=utf-8&amp;book=y&amp;wq=python'</span></span><br><span class="line"></span><br><span class="line">    custom_settings = &#123;</span><br><span class="line">        <span class="comment"># Splash服务器地址</span></span><br><span class="line">        <span class="string">'SPLASH_URL'</span>: <span class="string">'http://localhost:8050/'</span>,</span><br><span class="line">        <span class="comment"># 开启 Splash 的两个下载中间件并调整 HttpCompressionMiddleware 的次序</span></span><br><span class="line">        <span class="string">'DOWNLOADER_MIDDLEWARES'</span>: &#123;</span><br><span class="line">            <span class="string">'ArticleSpider.middlewares.RandomUserAgentMiddleware'</span>: <span class="number">722</span>,</span><br><span class="line">            <span class="string">'scrapy_splash.SplashCookiesMiddleware'</span>: <span class="number">723</span>,</span><br><span class="line">            <span class="string">'scrapy_splash.SplashMiddleware'</span>: <span class="number">725</span>,</span><br><span class="line">            <span class="string">'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware'</span>: <span class="number">810</span>,</span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="comment"># 设置去重过滤器</span></span><br><span class="line">        <span class="string">'DUPEFILTER_CLASS'</span>: <span class="string">'scrapy_splash.SplashAwareDupeFilter'</span>,</span><br><span class="line">        <span class="comment"># 用来支持 cache_args （可选）</span></span><br><span class="line">        <span class="string">'SPIDER_MIDDLEWARES'</span>: &#123;</span><br><span class="line">            <span class="string">'scrapy_splash.SplashDeduplicateArgsMiddleware'</span>: <span class="number">100</span>,</span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="comment"># 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.146 Safari/537.36',</span></span><br><span class="line">        <span class="comment"># 'HTTPERROR_ALLOWED_CODES': [400],</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">start_requests</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># 请求第一页，无须 js 渲染</span></span><br><span class="line">        <span class="keyword">yield</span> Request(self.base_url, callback=self.parse_urls, dont_filter=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="comment"># 获取一个页面中每本书的名字和价格</span></span><br><span class="line">        <span class="keyword">for</span> sel <span class="keyword">in</span> response.css(<span class="string">'ul.gl-warp.clearfix &gt; li.gl-item'</span>):</span><br><span class="line">            <span class="keyword">yield</span> &#123;</span><br><span class="line">                <span class="string">'name'</span>: sel.css(<span class="string">'div.p-name'</span>).xpath(<span class="string">'string(.//em)'</span>).extract_first(),</span><br><span class="line">                <span class="string">'price'</span>: sel.css(<span class="string">'div.p-price i::text'</span>).extract_first(),</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_urls</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="comment"># 获取商品总数，计算出总页数</span></span><br><span class="line">        total = int(response.css(<span class="string">'span#J_resCount::text'</span>).re_first(<span class="string">'\d+'</span>))</span><br><span class="line">        pageNum = total // <span class="number">60</span> + (<span class="number">1</span> <span class="keyword">if</span> total % <span class="number">60</span> <span class="keyword">else</span> <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 构造每页的 url，向 Splash 的 execute 端点发送请求</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(pageNum):</span><br><span class="line">            url = <span class="string">'%s&amp;page=%s'</span> % (self.base_url, <span class="number">2</span> * i + <span class="number">1</span>)</span><br><span class="line">            <span class="keyword">yield</span> SplashRequest(</span><br><span class="line">                    url,</span><br><span class="line">                    endpoint=<span class="string">'execute'</span>,</span><br><span class="line">                    args=&#123;<span class="string">'lua_source'</span>: lua_script&#125;,</span><br><span class="line">                    cache_args=[<span class="string">'lua_source'</span>]</span><br><span class="line">            )</span><br></pre></td></tr></table></figure><p>解释上述代码如下：</p><ul><li>start_requests 方法</li></ul><p>start_requests 提交对第一个页面的请求，这个页面不需要渲染，因为我们只想从中获取页面总数，使用 scrapy.Request 提交请求，并指定 parse_urls 作为解析函数。</p><ul><li>parse_urls 方法</li></ul><p>从第一个页面中提取商品总数，用其计算页面总数，之后按照前面分析出的页面 url 规律构造每一个页面的 url 。这些页面都是需要渲染的，使用 SplashRequest 提交请求，除了渲染页面以外，还需要执行一段 JavaScript 代码（为了加载后 30 本书），因此使用 Splash 的 execute 端点将 endpoint 参数置为 <code>execute</code> 。通过 args 参数的 lua_source 字段传递我们要执行的 lua 脚本，由于爬取每个页面时都要执行该脚本，因此可以使用 cache_args 参数将该脚本缓存到 Splash 服务器。</p><ul><li>parse 方法</li></ul><p>一个页面中提取 60 本书的名字和价格信息。 </p><ul><li>lua_script  </li></ul><p>自定义的 lua 脚 本，其中的逻辑很简单:<br>打开页面 ==》 等待渲染 ==》 执行 js 触发数据加载（后 30 本书） ==》 等待渲染 ==》 返四 html</p><p>编码和配置的完成，运行爬虫：</p><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">(<span class="keyword">jobboleArticle) </span>$ <span class="keyword">scrapy </span>crawl <span class="keyword">jd_book </span>-o .\data\<span class="keyword">books.csv</span></span><br><span class="line"><span class="keyword"></span></span><br><span class="line"><span class="keyword"></span></span><br><span class="line"><span class="keyword">(jobboleArticle) </span>$ cat -n .\data\<span class="keyword">books.csv</span></span><br><span class="line"><span class="keyword"></span></span><br><span class="line"><span class="keyword"></span></span><br><span class="line"><span class="keyword">5928 </span> 包邮 Python Web开发实战+Python Web开发 测试驱动方法  <span class="number">2</span>本,<span class="number">153</span>.<span class="number">00</span></span><br><span class="line"><span class="number">5929</span>  包邮 Python编程导论 <span class="number">2</span>版+ Python <span class="number">3</span>学习笔记  <span class="number">2</span>本 编程程序,<span class="number">118</span>.<span class="number">00</span></span><br><span class="line"><span class="number">5930</span>  包邮数据科学家养成手册+Python大战机器学习:数据科学家的第一个小目标  <span class="number">2</span>本,<span class="number">112</span>.<span class="number">00</span></span><br><span class="line"><span class="number">5931</span>  包邮 Python参考手册（第<span class="number">4</span>版修订版）+Python数据抓取技术与实战  <span class="number">2</span>本,<span class="number">104</span>.<span class="number">00</span></span><br><span class="line"><span class="number">5932</span>  包邮  Python数据处理+Python网络数据采集 <span class="number">2</span>本 python编程入门书籍,<span class="number">115</span>.<span class="number">00</span></span><br><span class="line"><span class="number">5933</span>  包邮 Python网络编程 第<span class="number">3</span>版+Python网络数据采集  <span class="number">2</span>本,<span class="number">103</span>.<span class="number">00</span></span><br><span class="line"><span class="number">5934</span>  包邮Metasploit渗透测试指南（修订版）+Python黑帽子:黑客与渗透测试编程之道,<span class="number">101</span>.<span class="number">00</span></span><br><span class="line"><span class="number">5935</span>  包邮 Python性能分析与优化+Python算法教程  <span class="number">2</span>本,<span class="number">85</span>.<span class="number">00</span></span><br><span class="line"><span class="number">5936</span>  包邮 数据科学实战手册（R+Python）+干净的数据:数据清洗入门与实践 <span class="number">2</span>本,<span class="number">80</span>.<span class="number">00</span></span><br><span class="line"><span class="number">5937</span>  包邮 Web接口开发与自动化测试——基于Python语言+软件自动化测试开发  <span class="number">2</span>本,<span class="number">88</span>.<span class="number">00</span></span><br><span class="line"><span class="number">5938</span>  零起点Python大数据与量化交易 何海群 <span class="number">9787121306594</span>,<span class="number">74</span>.<span class="number">30</span></span><br><span class="line"><span class="number">5939</span>  从Python开始学编程,<span class="number">29</span>.<span class="number">40</span></span><br><span class="line"><span class="number">5940</span>  区域包邮 Python游戏编程入门+Maya Python 游戏与影视编程指南  <span class="number">2</span>本,<span class="number">96</span>.<span class="number">00</span></span><br><span class="line"><span class="number">5941</span>  包邮  Python与机器学习实战+TensorFlow技术解析与实战 <span class="number">2</span>本,<span class="number">112</span>.<span class="number">00</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;blockquote class=&quot;blockquote-center&quot;&gt;自己不牛逼，认识的人再多也没用&lt;/blockquote&gt;

&lt;h5 id=&quot;爬取动态页面&quot;&gt;&lt;a href=&quot;#爬取动态页面&quot; class=&quot;headerlink&quot; title=&quot;爬取动态页面&quot;&gt;&lt;/a&gt;爬取动态页面&lt;/h5&gt;&lt;p&gt;静态页面的内容始终不变，爬取相对容易，但在现实中，日前绝大多数网站的页面都是动态页面。动态页面中的部分内容是浏览器运行页面中的 JavaScript 脚本动态生成的，爬取相对困难。&lt;br&gt;先来看一个简单的动态页面的例子，在浏览器中打开 &lt;a href=&quot;http://quotes.toscrape.com/js/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Quotes&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://dongfei.oss-cn-shanghai.aliyuncs.com/blog/quotes.jpg&quot; alt=&quot;quotes&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Python" scheme="http://blog.dongfei.xin/categories/Python/"/>
    
      <category term="Scrapy" scheme="http://blog.dongfei.xin/categories/Python/Scrapy/"/>
    
    
      <category term="Scrapy" scheme="http://blog.dongfei.xin/tags/Scrapy/"/>
    
  </entry>
  
  <entry>
    <title>Python3 中设置代理总结</title>
    <link href="http://blog.dongfei.xin/2018-04-24/Python3-%E4%B8%AD%E8%AE%BE%E7%BD%AE%E4%BB%A3%E7%90%86%E6%80%BB%E7%BB%93/"/>
    <id>http://blog.dongfei.xin/2018-04-24/Python3-中设置代理总结/</id>
    <published>2018-04-24T12:31:42.000Z</published>
    <updated>2018-04-25T00:15:27.621Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://dongfei.oss-cn-shanghai.aliyuncs.com/blog/python0.jpg?x-oss-process=style/regular-image-01" alt="https://dongfei.oss-cn-shanghai.aliyuncs.com/blog/python0.jpg"></p><h5 id="使用-urllib-库设置-HTTP-代理"><a href="#使用-urllib-库设置-HTTP-代理" class="headerlink" title="使用 urllib 库设置 HTTP 代理"></a>使用 urllib 库设置 HTTP 代理</h5><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.error <span class="keyword">import</span> URLError</span><br><span class="line"><span class="keyword">from</span> urllib.request <span class="keyword">import</span> ProxyHandler, build_opener</span><br><span class="line">proxy = <span class="string">'127.0.0.1:1080'</span></span><br><span class="line">proxy_handler = ProxyHandler(&#123;</span><br><span class="line">    <span class="string">'http'</span>: <span class="string">'http://'</span> + proxy,</span><br><span class="line">    <span class="string">'https'</span>: <span class="string">'https://'</span> + proxy</span><br><span class="line">&#125;)</span><br><span class="line">opener = build_opener(proxy_handler)</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    response = opener.open(<span class="string">'https://www.google.com/'</span>)</span><br><span class="line">    print(response.read().decode(<span class="string">'utf-8'</span>))</span><br><span class="line"><span class="keyword">except</span> URLError <span class="keyword">as</span> e:</span><br><span class="line">    print(e.reason)</span><br></pre></td></tr></table></figure><a id="more"></a><h5 id="使用-Requests-库设置-HTTP-代理"><a href="#使用-Requests-库设置-HTTP-代理" class="headerlink" title="使用 Requests 库设置 HTTP 代理"></a>使用 Requests 库设置 HTTP 代理</h5><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">proxies = &#123;</span><br><span class="line">  <span class="string">"http"</span>: <span class="string">"http://127.0.0.1:1080"</span>,</span><br><span class="line">  <span class="string">"https"</span>: <span class="string">"http://127.0.0.1:1080"</span>,</span><br><span class="line">&#125;</span><br><span class="line">requests.get(<span class="string">"https://www.google.com/"</span>, proxies=proxies)</span><br></pre></td></tr></table></figure><h5 id="使用-Scrapy-设置-HTTP-代理"><a href="#使用-Scrapy-设置-HTTP-代理" class="headerlink" title="使用 Scrapy 设置 HTTP 代理"></a>使用 Scrapy 设置 HTTP 代理</h5><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用 Shadowsock 可以将本机 socks 代理转为 HTTP 代理</span></span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> Request</span><br><span class="line">r = Request(<span class="string">'http://httpbin.org/ip'</span>, meta=&#123;<span class="string">'proxy'</span>: <span class="string">'http://127.0.0.1:1080'</span>&#125;)</span><br><span class="line">fetch(r)</span><br></pre></td></tr></table></figure><h5 id="使用-urllib-库设置-socks5-代理"><a href="#使用-urllib-库设置-socks5-代理" class="headerlink" title="使用 urllib 库设置 socks5 代理"></a>使用 urllib 库设置 socks5 代理</h5><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 免费获取 socks : https://31f.cn/socks-proxy/</span></span><br><span class="line"><span class="keyword">import</span> urllib</span><br><span class="line"><span class="keyword">import</span> socks</span><br><span class="line"><span class="keyword">from</span> sockshandler <span class="keyword">import</span> SocksiPyHandler</span><br><span class="line">opener = urllib.request.build_opener(SocksiPyHandler(socks.SOCKS5,<span class="string">"127.0.0.1"</span>, <span class="number">1080</span>))</span><br><span class="line">response = opener.open(<span class="string">"https://www.google.com/"</span>)</span><br><span class="line">print(response.getcode())</span><br><span class="line">print(response.read())</span><br></pre></td></tr></table></figure><h5 id="使用-Requests-库设置-socks5-代理"><a href="#使用-Requests-库设置-socks5-代理" class="headerlink" title="使用 Requests 库设置 socks5 代理"></a>使用 Requests 库设置 socks5 代理</h5><p><code>pip install requests[socks]</code><br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">proxies = &#123;</span><br><span class="line">    <span class="string">'http'</span>: <span class="string">'socks5://127.0.0.1:1080'</span>,</span><br><span class="line">    <span class="string">'https'</span>: <span class="string">'socks5://127.0.0.1:1080'</span></span><br><span class="line">&#125;</span><br><span class="line">requests.get(<span class="string">"http://www.google.com"</span>, proxies=proxies)</span><br></pre></td></tr></table></figure></p><h5 id="使用-PySocks-库设置-socks5-代理"><a href="#使用-PySocks-库设置-socks5-代理" class="headerlink" title="使用 PySocks 库设置 socks5 代理"></a>使用 PySocks 库设置 socks5 代理</h5><p><code>pip install PySocks</code></p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> socks</span><br><span class="line"><span class="keyword">import</span> socket</span><br><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request</span><br><span class="line"><span class="keyword">from</span> urllib.error <span class="keyword">import</span> URLError</span><br><span class="line">socks.set_default_proxy(socks.SOCKS5, <span class="string">'111.230.254.102'</span>, <span class="number">1080</span>)</span><br><span class="line">socket.socket = socks.socksocket</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    response = request.urlopen(<span class="string">'http://httpbin.org/get'</span>)</span><br><span class="line">    print(response.read().decode(<span class="string">'utf-8'</span>))</span><br><span class="line"><span class="keyword">except</span> URLError <span class="keyword">as</span> e:</span><br><span class="line">    print(e.reason)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://dongfei.oss-cn-shanghai.aliyuncs.com/blog/python0.jpg?x-oss-process=style/regular-image-01&quot; alt=&quot;https://dongfei.oss-cn-shanghai.aliyuncs.com/blog/python0.jpg&quot;&gt;&lt;/p&gt;
&lt;h5 id=&quot;使用-urllib-库设置-HTTP-代理&quot;&gt;&lt;a href=&quot;#使用-urllib-库设置-HTTP-代理&quot; class=&quot;headerlink&quot; title=&quot;使用 urllib 库设置 HTTP 代理&quot;&gt;&lt;/a&gt;使用 urllib 库设置 HTTP 代理&lt;/h5&gt;&lt;figure class=&quot;highlight py&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;from&lt;/span&gt; urllib.error &lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; URLError&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;from&lt;/span&gt; urllib.request &lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; ProxyHandler, build_opener&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;proxy = &lt;span class=&quot;string&quot;&gt;&#39;127.0.0.1:1080&#39;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;proxy_handler = ProxyHandler(&amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;string&quot;&gt;&#39;http&#39;&lt;/span&gt;: &lt;span class=&quot;string&quot;&gt;&#39;http://&#39;&lt;/span&gt; + proxy,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;string&quot;&gt;&#39;https&#39;&lt;/span&gt;: &lt;span class=&quot;string&quot;&gt;&#39;https://&#39;&lt;/span&gt; + proxy&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#125;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;opener = build_opener(proxy_handler)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;try&lt;/span&gt;:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    response = opener.open(&lt;span class=&quot;string&quot;&gt;&#39;https://www.google.com/&#39;&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    print(response.read().decode(&lt;span class=&quot;string&quot;&gt;&#39;utf-8&#39;&lt;/span&gt;))&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;except&lt;/span&gt; URLError &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; e:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    print(e.reason)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="Python" scheme="http://blog.dongfei.xin/categories/Python/"/>
    
      <category term="基础" scheme="http://blog.dongfei.xin/categories/Python/%E5%9F%BA%E7%A1%80/"/>
    
    
      <category term="Python" scheme="http://blog.dongfei.xin/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>Scrapy-Elasticsearch-Django 搭建搜索网站</title>
    <link href="http://blog.dongfei.xin/2018-04-22/Scrapy-Elasticsearch-Django-%E6%90%AD%E5%BB%BA%E6%90%9C%E7%B4%A2%E7%BD%91%E7%AB%99/"/>
    <id>http://blog.dongfei.xin/2018-04-22/Scrapy-Elasticsearch-Django-搭建搜索网站/</id>
    <published>2018-04-22T03:19:38.000Z</published>
    <updated>2018-04-25T00:17:23.568Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://dongfei.oss-cn-shanghai.aliyuncs.com/elasticsearch.jpg" alt="https://www.elastic.co"><br><strong>Elasticsearch官方</strong>：<a href="https://www.elastic.co/use-cases" target="_blank" rel="noopener">https://www.elastic.co/use-cases</a></p><p>其他类似产品：<strong>Elasticsearch solr 、sphinx、ELK</strong><br><a id="more"></a></p><font color="red">Elasticsearch 安装 </font><ul><li>安装：<strong>JavaSE JDK</strong> <a href="http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html" target="_blank" rel="noopener">http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html</a></li><li>安装：<strong>Elasticsearch</strong> <a href="https://www.elastic.co/downloads/elasticsearch" target="_blank" rel="noopener">https://www.elastic.co/downloads/elasticsearch</a></li><li>推荐安装：<strong>Elasticsearch-rtf</strong> <a href="https://github.com/medcl/elasticsearch-rtf" target="_blank" rel="noopener">https://github.com/medcl/elasticsearch-rtf</a></li></ul><p>检测安装是否成功：<br><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">切换至<span class="selector-tag">elasticsearch</span>安装目录、<span class="selector-tag">cmd</span>运行：<span class="selector-tag">elasticsearch</span></span><br><span class="line">127<span class="selector-class">.0</span><span class="selector-class">.0</span><span class="selector-class">.1</span><span class="selector-pseudo">:9200</span></span><br></pre></td></tr></table></figure></p><font color="red">head 插件和 kibana 的安装 </font><br><strong>head安装：</strong> 数据库管理、基于浏览器<br><strong>Github地址：</strong> <a href="https://github.com/mobz/elasticsearch-head" target="_blank" rel="noopener">https://github.com/mobz/elasticsearch-head</a><br><br>需要安装：<strong>node.js npm+cnpm</strong><br>测试：<br><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">cd</span> <span class="selector-tag">elasticsearch-head</span></span><br><span class="line"><span class="selector-tag">npm</span> <span class="selector-tag">install</span></span><br><span class="line"><span class="selector-tag">npm</span> <span class="selector-tag">run</span> <span class="selector-tag">start</span></span><br><span class="line">打开：127<span class="selector-class">.0</span><span class="selector-class">.0</span><span class="selector-class">.1</span><span class="selector-pseudo">:9100</span></span><br></pre></td></tr></table></figure><br><br>报错权限问题：<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">配置Elasticsearch.yml</span><br><span class="line">http<span class="selector-class">.cors</span><span class="selector-class">.enabled</span>: true</span><br><span class="line">http<span class="selector-class">.cors</span><span class="selector-class">.allow-origin</span>: <span class="string">"*"</span></span><br><span class="line">http<span class="selector-class">.cors</span><span class="selector-class">.allow-methods</span>: OPTIONS, HEAD, GET, POST, PUT, DELETE</span><br><span class="line">http<span class="selector-class">.cors</span><span class="selector-class">.allow-headers</span>: <span class="string">"X-Requested-With, Content-Type, Content-Length, X-User"</span></span><br></pre></td></tr></table></figure><br><br><strong>kibana安装、与elasticsearch版本对应：</strong><br><strong>下载地址：</strong><a href="https://www.elastic.co/downloads/past-releases/" target="_blank" rel="noopener">https://www.elastic.co/downloads/past-releases/</a><br>测试、切换至解压目录运行 <code>kibana.bat</code><br><br><font color="red">Elasticsearch 基本的索引和文档 CRUD 操作</font><ul><li>index（索引）==》数据库</li><li>type（类型）==》表</li><li>documents（文档）==》行</li><li>fields ==》列 </li></ul><p><strong>初始化数据库</strong><br><figure class="highlight xquery"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">PUT lagou</span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">"settings"</span>: &#123;</span><br><span class="line">      <span class="string">"index"</span> :&#123;</span><br><span class="line">        <span class="string">"number_of_shards"</span>:<span class="number">5</span>, 分片</span><br><span class="line">        <span class="string">"number_of_replicas"</span>:<span class="number">1</span> 副本</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p><strong>获取数据库信息</strong><br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="builtin-name">GET</span> lagou/_settings</span><br><span class="line"><span class="builtin-name">GET</span> _all/_settings</span><br><span class="line"><span class="builtin-name">GET</span> _all/_settings</span><br><span class="line"><span class="builtin-name">GET</span> .kibana,lagou/_settings</span><br></pre></td></tr></table></figure></p><p><strong>修改 _settings</strong><br><figure class="highlight dts"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">PUT lagou/_settings</span><br><span class="line">&#123;</span><br><span class="line">   <span class="string">"number_of_shards"</span>:<span class="number">3</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p><strong>获取索引信息</strong><br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="builtin-name">GET</span> _all</span><br><span class="line"><span class="builtin-name">GET</span> lagou</span><br></pre></td></tr></table></figure></p><p><strong>保存文档</strong><br><figure class="highlight xquery"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">PUT lagou/job/<span class="number">1</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"title"</span>:<span class="string">"python分布式爬虫开发 "</span>,</span><br><span class="line">  <span class="string">"salary_min"</span>:<span class="number">15000</span>,</span><br><span class="line">  <span class="string">"city"</span>:<span class="string">"北京"</span>,</span><br><span class="line">  <span class="string">"company"</span>:&#123;</span><br><span class="line">    <span class="string">"name"</span>:<span class="string">"百度"</span>,</span><br><span class="line">    <span class="string">"company_addr"</span>:<span class="string">"北京市软件园"</span></span><br><span class="line">    </span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="string">"publish_date"</span>:<span class="string">"2017-4-16"</span>,</span><br><span class="line">  <span class="string">"comments"</span>:<span class="number">15</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p><strong>添加数据</strong><br><figure class="highlight xquery"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">POST lagou/job/</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"title"</span>:<span class="string">"python django 开发工程师"</span>,</span><br><span class="line">  <span class="string">"salary_min"</span>:<span class="number">30000</span>,</span><br><span class="line">  <span class="string">"city"</span>:<span class="string">"上海"</span>,</span><br><span class="line">  <span class="string">"company"</span>:&#123;</span><br><span class="line">    <span class="string">"name"</span>:<span class="string">"美团科技"</span>,</span><br><span class="line">    <span class="string">"company_addr"</span>:<span class="string">"北京市软件园A区"</span></span><br><span class="line">    </span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="string">"publish_date"</span>:<span class="string">"2017-4-16"</span>,</span><br><span class="line">  <span class="string">"comments"</span>:<span class="number">20</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p><strong>查看数据</strong><br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="builtin-name">GET</span> lagou/job/1</span><br><span class="line"><span class="builtin-name">GET</span> lagou/job/1?<span class="attribute">_source</span>=title</span><br><span class="line"><span class="builtin-name">GET</span> lagou/job/1?<span class="attribute">_source</span>=title,city</span><br></pre></td></tr></table></figure></p><p><strong>修改数据</strong><br><figure class="highlight xquery"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">PUT lagou/job/<span class="number">1</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"title"</span>:<span class="string">"python分布式爬虫开发 "</span>,</span><br><span class="line">  <span class="string">"salary_min"</span>:<span class="number">15000</span>,</span><br><span class="line">  <span class="string">"company"</span>:&#123;</span><br><span class="line">    <span class="string">"name"</span>:<span class="string">"百度"</span>,</span><br><span class="line">    <span class="string">"company_addr"</span>:<span class="string">"北京市软件园"</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="string">"publish_date"</span>:<span class="string">"2017-4-16"</span>,</span><br><span class="line">  <span class="string">"comments"</span>:<span class="number">15</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">POST lagou/job/<span class="number">1</span>/_<span class="keyword">update</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"doc"</span>: &#123;</span><br><span class="line">    <span class="string">"comment"</span>:<span class="number">20</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p><strong>删除数据</strong><br><figure class="highlight gauss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">DELETE</span> lagou/job/<span class="number">1</span></span><br><span class="line"><span class="keyword">DELETE</span> lagou/job</span><br><span class="line"><span class="keyword">DELETE</span> lagou</span><br></pre></td></tr></table></figure></p><font color="red">Elasticsearch 的 mget 和 bulk 批量操作</font><p><strong>mget 操作</strong><br><figure class="highlight xquery"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line">PUT testdb</span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">"settings"</span>: &#123;</span><br><span class="line">      <span class="string">"index"</span> :&#123;</span><br><span class="line">        <span class="string">"number_of_shards"</span>:<span class="number">5</span>, </span><br><span class="line">        <span class="string">"number_of_replicas"</span>:<span class="number">1</span> </span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">PUT testdb/job1/<span class="number">1</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"title"</span>:<span class="string">"job1_1"</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">PUT testdb/job1/<span class="number">2</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"title"</span>:<span class="string">"job1_2"</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">PUT testdb/job2/<span class="number">1</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"title"</span>:<span class="string">"job2_1"</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">PUT testdb/job2/<span class="number">2</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"title"</span>:<span class="string">"job2_2"</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">GET _mget</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"docs"</span>:[</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="string">"_index"</span>:<span class="string">"testdb"</span>,</span><br><span class="line">      <span class="string">"_type"</span>:<span class="string">"job1"</span>,</span><br><span class="line">      <span class="string">"_id"</span>:<span class="number">1</span></span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="string">"_index"</span>:<span class="string">"testdb"</span>,</span><br><span class="line">      <span class="string">"_type"</span>:<span class="string">"job1"</span>,</span><br><span class="line">      <span class="string">"_id"</span>:<span class="number">2</span></span><br><span class="line">    &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">GET testdb/_mget</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"docs"</span>:[</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="string">"_type"</span>:<span class="string">"job1"</span>,</span><br><span class="line">      <span class="string">"_id"</span>:<span class="number">1</span></span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="string">"_type"</span>:<span class="string">"job1"</span>,</span><br><span class="line">      <span class="string">"_id"</span>:<span class="number">2</span></span><br><span class="line">    &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">GET testdb/job1/_mget</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"docs"</span>:[</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="string">"_id"</span>:<span class="number">1</span></span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="string">"_id"</span>:<span class="number">2</span></span><br><span class="line">    &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">GET testdb/job1/_mget</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"ids"</span>:[<span class="number">1</span>,<span class="number">2</span>]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p><strong>bulk批量操作</strong><br><figure class="highlight xquery"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">POST _bulk</span><br><span class="line">&#123;<span class="string">"index"</span>:&#123;<span class="string">"_index"</span>:<span class="string">"lagou"</span>,<span class="string">"_type"</span>:<span class="string">"job"</span>,<span class="string">"_id"</span>:<span class="string">"1"</span>&#125;&#125;</span><br><span class="line">&#123;<span class="string">"title"</span>:<span class="string">"python django 开发工程师"</span>,<span class="string">"salary_min"</span>:<span class="number">30000</span>,<span class="string">"city"</span>:<span class="string">"上海"</span>,<span class="string">"company"</span>:&#123;<span class="string">"name"</span>:<span class="string">"美团科技"</span>,<span class="string">"company_addr"</span>:<span class="string">"北京市软件园A区"</span>&#125;,<span class="string">"publish_date"</span>:<span class="string">"2017-4-16"</span>,<span class="string">"comments"</span>:<span class="number">20</span>&#125;</span><br><span class="line">&#123;<span class="string">"index"</span>:&#123;<span class="string">"_index"</span>:<span class="string">"lagou"</span>,<span class="string">"_type"</span>:<span class="string">"job2"</span>,<span class="string">"_id"</span>:<span class="string">"2"</span>&#125;&#125;</span><br><span class="line">&#123;<span class="string">"title"</span>:<span class="string">"python分布式爬虫开发 "</span>,<span class="string">"salary_min"</span>:<span class="number">15000</span>,<span class="string">"city"</span>:<span class="string">"北京"</span>,<span class="string">"company"</span>:&#123;<span class="string">"name"</span>:<span class="string">"百度"</span>,<span class="string">"company_addr"</span>:<span class="string">"北京市软件园"</span>&#125;,<span class="string">"publish_date"</span>:<span class="string">"2017-4-16"</span>,<span class="string">"comments"</span>:<span class="number">15</span>&#125;</span><br></pre></td></tr></table></figure></p><font color="red">Elasticsearch 的 mapping 映射管理</font><br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="builtin-name">GET</span> lagou/_mapping/job</span><br><span class="line"><span class="builtin-name">GET</span> _all/_mapping</span><br></pre></td></tr></table></figure><br><br><font color="red">Scrapy 写入数据到 Elasticsearch</font><p><strong>elasticsearch-Python</strong> 数据转换包 <strong>elasticsearch-dsl-py</strong><br>Github地址：<a href="https://github.com/elastic/elasticsearch-dsl-py" target="_blank" rel="noopener">https://github.com/elastic/elasticsearch-dsl-py</a><br>安装：<code>pip install elasticsearch-dsl</code></p><p>定义数据类型、添加<code>ElasticsearchPipeline</code></p><p><strong>ES完成搜索建议-搜索建议字段保存</strong><br>自动补全文档：<br><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/search-suggesters-completion.html" target="_blank" rel="noopener">https://www.elastic.co/guide/en/elasticsearch/reference/current/search-suggesters-completion.html</a></p><p><strong>查看分析器分析结果</strong><br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="builtin-name">GET</span> _analyze</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"analyzer"</span>: <span class="string">"ik_smart"</span>, </span><br><span class="line">  <span class="string">"text"</span>: <span class="string">"Python网络开发工程师 "</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p><strong>注意安装指定版本</strong><br><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip install elasticsearch-dsl==<span class="number">5.1</span><span class="number">.0</span></span><br><span class="line">pip install elasticsearch==<span class="number">5.1</span><span class="number">.0</span></span><br></pre></td></tr></table></figure></p><p><font color="red">Django 实现 elasticsearch 的搜索建议</font><br>创建开发环境：<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkvirtualenv<span class="selector-class">.bat</span> --python=D:\ProgramData\Anaconda3\python<span class="selector-class">.exe</span> lcv_search</span><br></pre></td></tr></table></figure></p><p>安装django：<br><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip <span class="keyword">install</span> django</span><br></pre></td></tr></table></figure></p><p>GitHub相关：<strong>Django scrapy</strong></p><p><font color="red">scrapyd 部署 scrapy 项目</font><br>Github地址：<a href="https://github.com/scrapy/scrapyd" target="_blank" rel="noopener">https://github.com/scrapy/scrapyd</a><br>scrapyd 相当于服务器<br>安装：<br><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip <span class="keyword">install </span><span class="keyword">scrapyd</span></span><br><span class="line"><span class="keyword">pip </span><span class="keyword">install </span><span class="keyword">scrapyd-client</span></span><br></pre></td></tr></table></figure></p><p>创建 scrapyd-deploy.bat 文件：<br><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">scrapyd-deploy </span>-l</span><br><span class="line"><span class="keyword">scrapy </span>list</span><br></pre></td></tr></table></figure></p><p>部署项目：<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scrapyd-deploy dongfei -<span class="selector-tag">p</span> ArticleSpider</span><br><span class="line"></span><br><span class="line">curl http:<span class="comment">//localhost:6800/daemonstatus.json</span></span><br></pre></td></tr></table></figure></p><p>部署某一个spider：<br><figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl <span class="string">http:</span><span class="comment">//localhost:6800/schedule.json -d project=ArticleSpider -d spider=jobbole</span></span><br></pre></td></tr></table></figure></p><p>删除：<br><figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl <span class="string">http:</span><span class="comment">//localhost:6800/delproject.json -d project=ArticleSpider</span></span><br></pre></td></tr></table></figure></p><p>官方API文档：<br><a href="http://scrapyd.readthedocs.io/en/stable/api.html#daemonstatus-json" target="_blank" rel="noopener">http://scrapyd.readthedocs.io/en/stable/api.html#daemonstatus-json</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://dongfei.oss-cn-shanghai.aliyuncs.com/elasticsearch.jpg&quot; alt=&quot;https://www.elastic.co&quot;&gt;&lt;br&gt;&lt;strong&gt;Elasticsearch官方&lt;/strong&gt;：&lt;a href=&quot;https://www.elastic.co/use-cases&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.elastic.co/use-cases&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;其他类似产品：&lt;strong&gt;Elasticsearch solr 、sphinx、ELK&lt;/strong&gt;&lt;br&gt;
    
    </summary>
    
      <category term="Python" scheme="http://blog.dongfei.xin/categories/Python/"/>
    
      <category term="Scrapy" scheme="http://blog.dongfei.xin/categories/Python/Scrapy/"/>
    
    
      <category term="Scrapy" scheme="http://blog.dongfei.xin/tags/Scrapy/"/>
    
      <category term="Elasticsearch" scheme="http://blog.dongfei.xin/tags/Elasticsearch/"/>
    
      <category term="Django" scheme="http://blog.dongfei.xin/tags/Django/"/>
    
  </entry>
  
  <entry>
    <title>Markdown LaTex 数学公式</title>
    <link href="http://blog.dongfei.xin/2018-04-22/Markdown-LaTex-%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F/"/>
    <id>http://blog.dongfei.xin/2018-04-22/Markdown-LaTex-数学公式/</id>
    <published>2018-04-21T16:35:22.000Z</published>
    <updated>2018-04-22T12:36:32.175Z</updated>
    
    <content type="html"><![CDATA[<p>$$ \color{red} {J_\alpha(x) = \sum_{m=0}^\infty \frac{(-1)^m}{m! \Gamma (m + \alpha + 1)} {\left({ \frac{x}{2} }\right)}^{2m + \alpha}} $$</p><p><img src="https://dongfei.oss-cn-shanghai.aliyuncs.com/blog/MathJax/logo.png" alt="MathJax"></p><p><a href="https://blog.csdn.net/zryxh1/article/details/53161011" target="_blank" rel="noopener">Md语法 | LaTex数学公式</a></p><p><a href="http://jzqt.github.io/2015/06/30/Markdown中写数学公式" target="_blank" rel="noopener">Markdown 中写数学公式</a></p><p><a href="https://www.zybuluo.com/codeep/note/163962" target="_blank" rel="noopener">Cmd Markdown 公式指导手册</a></p><p><a href="https://www.cnblogs.com/peaceWang/p/Markdown-tian-jia-Latex-shu-xue-gong-shi.html" target="_blank" rel="noopener">Markdown 添加 Latex 数学公式</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;$$ \color{red} {J_\alpha(x) = \sum_{m=0}^\infty \frac{(-1)^m}{m! \Gamma (m + \alpha + 1)} {\left({ \frac{x}{2} }\right)}^{2m + \alpha}} $
      
    
    </summary>
    
      <category term="Hexo" scheme="http://blog.dongfei.xin/categories/Hexo/"/>
    
    
      <category term="Hexo" scheme="http://blog.dongfei.xin/tags/Hexo/"/>
    
      <category term="Markdown" scheme="http://blog.dongfei.xin/tags/Markdown/"/>
    
  </entry>
  
  <entry>
    <title>MongoDB 导入导出以及数据库备份</title>
    <link href="http://blog.dongfei.xin/2018-04-20/MongoDB-%E5%AF%BC%E5%85%A5%E5%AF%BC%E5%87%BA%E4%BB%A5%E5%8F%8A%E6%95%B0%E6%8D%AE%E5%BA%93%E5%A4%87%E4%BB%BD/"/>
    <id>http://blog.dongfei.xin/2018-04-20/MongoDB-导入导出以及数据库备份/</id>
    <published>2018-04-20T08:53:58.000Z</published>
    <updated>2018-04-20T09:19:15.795Z</updated>
    
    <content type="html"><![CDATA[<h4 id="MongoDB-数据导入与导出"><a href="#MongoDB-数据导入与导出" class="headerlink" title="MongoDB 数据导入与导出"></a>MongoDB 数据导入与导出</h4><h5 id="数据导出工具：-mongoexport"><a href="#数据导出工具：-mongoexport" class="headerlink" title="数据导出工具： mongoexport"></a>数据导出工具： <code>mongoexport</code></h5><h6 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h6><p><code>MongoDB</code> 中的 <code>mongoexport</code> 工具可以把一个 <code>collection</code> 导出成 <code>JSON</code> 格式或 <code>CSV</code> 格式的文件。可以通过参数指定导出的数据项，也可以根据指定的条件导出数据。</p><h6 id="语法"><a href="#语法" class="headerlink" title="语法"></a>语法</h6><figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@izwz9ek33mwn4j4nw4vrmez ~]<span class="comment"># mongoexport --help</span></span><br><span class="line"></span><br><span class="line">mongoexport -d dbname -c collectionname -o <span class="built_in">file</span> <span class="comment">--type json/csv -f field</span></span><br></pre></td></tr></table></figure><a id="more"></a><h6 id="参数说明"><a href="#参数说明" class="headerlink" title="参数说明"></a>参数说明</h6><figure class="highlight haml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">-<span class="ruby">d ：        数据库名</span></span><br><span class="line"><span class="ruby">-c ：        collection 名</span></span><br><span class="line"><span class="ruby">-o ：        输出的文件名</span></span><br><span class="line"><span class="ruby">--type ：    输出的格式，默认为 json</span></span><br><span class="line"><span class="ruby">-f ：        输出的字段，如果 -type 为 csv ，则需要加上 -f  <span class="string">"字段名"</span></span></span><br></pre></td></tr></table></figure><h6 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h6><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo mongoexport -d mongotest -c users -o /home/python/Desktop/mongoDB/users.json --<span class="built_in">type</span> json -f <span class="string">"_id,user_id,user_name,age,status"</span></span><br></pre></td></tr></table></figure><h5 id="数据导入工具：-mongoimport"><a href="#数据导入工具：-mongoimport" class="headerlink" title="数据导入工具： mongoimport"></a>数据导入工具： <code>mongoimport</code></h5><h6 id="语法-1"><a href="#语法-1" class="headerlink" title="语法"></a>语法</h6><figure class="highlight brainfuck"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="title">[</span><span class="comment">root@izwz9ek33mwn4j4nw4vrmez</span> <span class="comment">~</span><span class="title">]</span><span class="comment">#</span> <span class="comment">mongoimport</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">help</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">mongoimport</span> <span class="literal">-</span><span class="comment">d</span> <span class="comment">dbname</span> <span class="literal">-</span><span class="comment">c</span> <span class="comment">collectionname</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">file</span> <span class="comment">filename</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">headerline</span> <span class="literal">-</span><span class="literal">-</span><span class="comment">type</span> <span class="comment">json/csv</span> <span class="literal">-</span><span class="comment">f</span> <span class="comment">field</span></span><br></pre></td></tr></table></figure><h6 id="参数说明-1"><a href="#参数说明-1" class="headerlink" title="参数说明"></a>参数说明</h6><figure class="highlight haml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">-<span class="ruby">d ：    数据库名</span></span><br><span class="line"><span class="ruby">-c ：    collection 名</span></span><br><span class="line"><span class="ruby">--type ：导入的格式默认 json</span></span><br><span class="line"><span class="ruby">-f ：    导入的字段名</span></span><br><span class="line"><span class="ruby">--headerline ：  如果导入的格式是 csv，则可以使用第一行的标题作为导入的字段</span></span><br><span class="line"><span class="ruby">--file ：        要导入的文件</span></span><br></pre></td></tr></table></figure><h6 id="示例-1"><a href="#示例-1" class="headerlink" title="示例"></a>示例</h6><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo mongoimport -d mongotest -c<span class="built_in"> users </span>--file /home/mongodump/articles.json --type json</span><br></pre></td></tr></table></figure><h4 id="MongoDB-备份与恢复"><a href="#MongoDB-备份与恢复" class="headerlink" title="MongoDB 备份与恢复"></a>MongoDB 备份与恢复</h4><h5 id="MongoDB-数据库备份"><a href="#MongoDB-数据库备份" class="headerlink" title="MongoDB 数据库备份"></a>MongoDB 数据库备份</h5><h6 id="语法-2"><a href="#语法-2" class="headerlink" title="语法"></a>语法</h6><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">mongodump -h dbhost -d dbname -o dbdirectory</span></span><br></pre></td></tr></table></figure><h6 id="参数说明-2"><a href="#参数说明-2" class="headerlink" title="参数说明"></a>参数说明</h6><figure class="highlight haml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">-<span class="ruby">h：     MongDB 所在服务器地址，例如：<span class="number">127.0</span>.<span class="number">0</span>.<span class="number">1</span>，当然也可以指定端口号：<span class="number">127.0</span>.<span class="number">0</span>.<span class="number">1</span><span class="symbol">:</span><span class="number">27017</span></span></span><br><span class="line"><span class="ruby">-d： 需要备份的数据库实例，例如：test</span></span><br><span class="line"><span class="ruby">-o： 备份的数据存放位置，例如：/home/mongodump/，当然该目录需要提前建立，这个目录里面存放该数据库实例的备份数据。</span></span><br></pre></td></tr></table></figure><h6 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h6><figure class="highlight arduino"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo rm -rf /<span class="built_in">home</span>/momgodump/</span><br><span class="line">sudo <span class="built_in">mkdir</span> -p /<span class="built_in">home</span>/momgodump</span><br><span class="line">sudo mongodump -h <span class="number">39.108</span><span class="number">.3</span><span class="number">.174</span>:<span class="number">27017</span> -d PornHub -o ./mongodump/</span><br></pre></td></tr></table></figure><h5 id="MongoDB-数据库恢复"><a href="#MongoDB-数据库恢复" class="headerlink" title="MongoDB 数据库恢复"></a>MongoDB 数据库恢复</h5><h6 id="语法-3"><a href="#语法-3" class="headerlink" title="语法"></a>语法</h6><figure class="highlight ada"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mongorestore -h dbhost -d dbname <span class="comment">--dir dbdirectory</span></span><br></pre></td></tr></table></figure><h6 id="参数说明-3"><a href="#参数说明-3" class="headerlink" title="参数说明"></a>参数说明</h6><figure class="highlight haml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">-<span class="ruby">h： MongoDB 所在服务器地址</span></span><br><span class="line"><span class="ruby">-d： 需要恢复的数据库实例，例如：test，当然这个名称也可以和备份时候的不一样，比如 test2</span></span><br><span class="line"><span class="ruby">--dir：  备份数据所在位置，例如：/home/mongodump/itcast/</span></span><br><span class="line"><span class="ruby">--drop： 恢复的时候，先删除当前数据，然后恢复备份的数据。就是说，恢复后，备份后添加修改的数据都会被删除，慎用！</span></span><br></pre></td></tr></table></figure><h6 id="实例-1"><a href="#实例-1" class="headerlink" title="实例"></a>实例</h6><figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mongorestore -h 127.0.0.1<span class="function">:27017</span> -d PornHub <span class="params">--dir</span> <span class="string">./mongodump/PornHub/</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;MongoDB-数据导入与导出&quot;&gt;&lt;a href=&quot;#MongoDB-数据导入与导出&quot; class=&quot;headerlink&quot; title=&quot;MongoDB 数据导入与导出&quot;&gt;&lt;/a&gt;MongoDB 数据导入与导出&lt;/h4&gt;&lt;h5 id=&quot;数据导出工具：-mongoexport&quot;&gt;&lt;a href=&quot;#数据导出工具：-mongoexport&quot; class=&quot;headerlink&quot; title=&quot;数据导出工具： mongoexport&quot;&gt;&lt;/a&gt;数据导出工具： &lt;code&gt;mongoexport&lt;/code&gt;&lt;/h5&gt;&lt;h6 id=&quot;概念&quot;&gt;&lt;a href=&quot;#概念&quot; class=&quot;headerlink&quot; title=&quot;概念&quot;&gt;&lt;/a&gt;概念&lt;/h6&gt;&lt;p&gt;&lt;code&gt;MongoDB&lt;/code&gt; 中的 &lt;code&gt;mongoexport&lt;/code&gt; 工具可以把一个 &lt;code&gt;collection&lt;/code&gt; 导出成 &lt;code&gt;JSON&lt;/code&gt; 格式或 &lt;code&gt;CSV&lt;/code&gt; 格式的文件。可以通过参数指定导出的数据项，也可以根据指定的条件导出数据。&lt;/p&gt;
&lt;h6 id=&quot;语法&quot;&gt;&lt;a href=&quot;#语法&quot; class=&quot;headerlink&quot; title=&quot;语法&quot;&gt;&lt;/a&gt;语法&lt;/h6&gt;&lt;figure class=&quot;highlight applescript&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;[root@izwz9ek33mwn4j4nw4vrmez ~]&lt;span class=&quot;comment&quot;&gt;# mongoexport --help&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;mongoexport -d dbname -c collectionname -o &lt;span class=&quot;built_in&quot;&gt;file&lt;/span&gt; &lt;span class=&quot;comment&quot;&gt;--type json/csv -f field&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="Linux" scheme="http://blog.dongfei.xin/categories/Linux/"/>
    
      <category term="CentOS" scheme="http://blog.dongfei.xin/categories/Linux/CentOS/"/>
    
    
      <category term="CentOS" scheme="http://blog.dongfei.xin/tags/CentOS/"/>
    
      <category term="MongoDB" scheme="http://blog.dongfei.xin/tags/MongoDB/"/>
    
      <category term="数据库" scheme="http://blog.dongfei.xin/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>Vim 文本替换</title>
    <link href="http://blog.dongfei.xin/2018-04-16/Vim-%E6%96%87%E6%9C%AC%E6%9B%BF%E6%8D%A2/"/>
    <id>http://blog.dongfei.xin/2018-04-16/Vim-文本替换/</id>
    <published>2018-04-16T09:04:05.000Z</published>
    <updated>2018-04-16T09:31:50.917Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://dongfei.oss-cn-shanghai.aliyuncs.com/blog/unplash.png?x-oss-process=style/regular-image-01" alt="unplash.png"></p><h5 id="替换命令的完整形式：-range-s-from-to-flags"><a href="#替换命令的完整形式：-range-s-from-to-flags" class="headerlink" title="替换命令的完整形式：:[range]s/from/to/[flags]"></a>替换命令的完整形式：<code>:[range]s/from/to/[flags]</code></h5><ul><li><code>s/from/to/</code>  把 from 指定的字符串替换成 to 指定的字符串，from 可以是正则表达式。<a id="more"></a></li><li><code>[range]</code>     有以下一些表示方法：</li></ul><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">不写 range      默认为光标所在的行。</span><br><span class="line">.               光标所在的行。</span><br><span class="line"><span class="number">1</span>               第一行。</span><br><span class="line">$               最后一行。</span><br><span class="line"><span class="number">33</span>              第<span class="number">33</span>行。</span><br><span class="line">'a              标记a所在的行（之前要使用ma做过标记）。</span><br><span class="line">.+<span class="number">1</span>             当前光标所在行的下面一行。</span><br><span class="line">$<span class="number">-1</span>             倒数第二行。（这里说明我们可以对某一行加减某个数值来</span><br><span class="line">                取得相对的行）。</span><br><span class="line"><span class="number">22</span>,<span class="number">33</span>           第<span class="number">22</span>～<span class="number">33</span>行。</span><br><span class="line"><span class="number">1</span>,$             第<span class="number">1</span>行 到 最后一行。</span><br><span class="line"><span class="number">1</span>               第<span class="number">1</span>行 到 当前行。</span><br><span class="line">.,$             当前行 到 最后一行。</span><br><span class="line">'a,'b           标记a所在的行 到 标记b所在的行。</span><br><span class="line">%               所有行（与 <span class="number">1</span>,$ 等价）。</span><br><span class="line">?chapter?       从当前位置向上搜索，找到的第一个chapter所在的行。（</span><br><span class="line">                其中chapter可以是任何字符串或者正则表达式。</span><br><span class="line">/chapter/       从当前位置向下搜索，找到的第一个chapter所在的行。（</span><br><span class="line">                其中chapter可以是任何字符串或者正则表达式。</span><br></pre></td></tr></table></figure><p>注意，上面的所有用于 <code>range</code> 的表示方法都可以通过 <code>+、-</code> 操作来设置相对偏<br>移量。</p><ul><li><code>[flags]</code>     这里可用的 flags 有：</li></ul><figure class="highlight llvm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">无      ：  只对指定范围内的第一个匹配项进行替换。</span><br><span class="line">g       ：  对指定范围内的所有匹配项进行替换。</span><br><span class="line"><span class="keyword">c</span>       ：  在替换前请求用户确认。</span><br><span class="line">e       ：  忽略执行过程中的错误。</span><br></pre></td></tr></table></figure><p>注意：上面的所有 <code>flags</code> 都可以组合起来使用，比如 <code>gc</code> 表示对指定范围内的<br>所有匹配项进行替换，并且在每一次替换之前都会请用户确认。</p><h5 id="替换当前行中的内容"><a href="#替换当前行中的内容" class="headerlink" title="替换当前行中的内容"></a>替换当前行中的内容</h5><figure class="highlight perl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">:<span class="regexp">s/from/to/</span>         将当前行中的第一个from，替换成to。如果当前行含有多个from，</span><br><span class="line">                    则只会替换其中的第一个。（<span class="keyword">s</span> 即 substitude）</span><br><span class="line">:<span class="regexp">s/from/to/g</span>        将当前行中的所有from都替换成to。</span><br><span class="line">:<span class="regexp">s/from/to/gc</span>       将当前行中的所有from都替换成to，但是每一次替换之前都</span><br><span class="line">                    会询问请求用户确认此操作。</span><br></pre></td></tr></table></figure><p>注意：这里的 <code>from</code> 和 <code>to</code> 都可以是任何字符串，其中 <code>from</code> 还可以是正则表达式。</p><h5 id="替换某一行的内容"><a href="#替换某一行的内容" class="headerlink" title="替换某一行的内容"></a>替换某一行的内容</h5><figure class="highlight pf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">:.s/<span class="keyword">from</span>/<span class="keyword">to</span>/g       在当前行进行替换操作。</span><br><span class="line">:<span class="number">33</span>s/<span class="keyword">from</span>/<span class="keyword">to</span>/g      在第<span class="number">33</span>行进行替换操作。</span><br><span class="line">:<span class="variable">$s</span>/<span class="keyword">from</span>/<span class="keyword">to</span>/g       在最后一行进行替换操作。</span><br></pre></td></tr></table></figure><h5 id="替换某些行的内容："><a href="#替换某些行的内容：" class="headerlink" title="替换某些行的内容："></a>替换某些行的内容：</h5><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">:<span class="number">10</span>,<span class="number">20</span>s/<span class="built_in">from</span>/<span class="built_in">to</span>/g       对第<span class="number">10</span>行到第<span class="number">20</span>行的内容进行替换。</span><br><span class="line">:<span class="number">1</span>,$s/<span class="built_in">from</span>/<span class="built_in">to</span>/g         对第一行到最后一行的内容进行替换（即全部文本）。</span><br><span class="line">:<span class="number">1</span>,.s/<span class="built_in">from</span>/<span class="built_in">to</span>/g         对第一行到当前行的内容进行替换。</span><br><span class="line">:.,$s/<span class="built_in">from</span>/<span class="built_in">to</span>/g         对当前行到最后一行的内容进行替换。</span><br><span class="line">:<span class="string">'a,'</span>bs/<span class="built_in">from</span>/<span class="built_in">to</span>/g       对标记<span class="keyword">a</span>和b之间的行（含<span class="keyword">a</span>和b所在的行）进行替换。</span><br><span class="line">                        其中<span class="keyword">a</span>和b是之前用m命令所做的标记。</span><br></pre></td></tr></table></figure><h5 id="替换所有行的内容："><a href="#替换所有行的内容：" class="headerlink" title="替换所有行的内容："></a>替换所有行的内容：</h5><figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="symbol">:%s/from/to/g</span>   对所有行的内容进行替换。等价于 <span class="symbol">:</span><span class="number">1</span>,<span class="variable">$s</span>/from/to/g</span><br></pre></td></tr></table></figure><h5 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h5><figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="symbol">:s/vivian/sky/</span>      替换当前行第一个 vivian 为 sky</span><br><span class="line"><span class="symbol">:s/vivian/sky/g</span>     替换当前行所有 vivian 为 sky</span><br><span class="line"><span class="symbol">:n</span>,<span class="variable">$s</span>/vivian/sky/   替换第 n 行开始到最后一行中每一行的第一个 vivian 为 sky</span><br><span class="line"><span class="symbol">:n</span>,<span class="variable">$s</span>/vivian/sky/g  替换第 n 行开始到最后一行中每一行所有 vivian 为 sky</span><br><span class="line"></span><br><span class="line">n 为数字，若 n 为 .，表示从当前行开始到最后一行</span><br><span class="line"></span><br><span class="line"><span class="symbol">:%s/vivian/sky/</span>     （等同于<span class="symbol">:g/vivian/s//sky/</span>）替换每一行的第一个 vivian 为 sky</span><br><span class="line"><span class="symbol">:%s/vivian/sky/g</span>    （等同于<span class="symbol">:g/vivian/s//sky/g</span>）替换每一行中所有 vivian 为 sky</span><br><span class="line"></span><br><span class="line">可以使用 <span class="comment"># 作为分隔符，此时中间出现的 / 不会作为分隔符</span></span><br><span class="line"></span><br><span class="line"><span class="symbol">:s</span><span class="comment">#vivian/#sky/#    替换当前行第一个 vivian/ 为 sky/</span></span><br><span class="line"><span class="symbol">:%s+/oradata/apras/+/user01/apras1+</span> （使用+ 来 替换 /）</span><br><span class="line"></span><br><span class="line">/oradata/apras/ 替换成 /user01/apras1/</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://dongfei.oss-cn-shanghai.aliyuncs.com/blog/unplash.png?x-oss-process=style/regular-image-01&quot; alt=&quot;unplash.png&quot;&gt;&lt;/p&gt;
&lt;h5 id=&quot;替换命令的完整形式：-range-s-from-to-flags&quot;&gt;&lt;a href=&quot;#替换命令的完整形式：-range-s-from-to-flags&quot; class=&quot;headerlink&quot; title=&quot;替换命令的完整形式：:[range]s/from/to/[flags]&quot;&gt;&lt;/a&gt;替换命令的完整形式：&lt;code&gt;:[range]s/from/to/[flags]&lt;/code&gt;&lt;/h5&gt;&lt;ul&gt;
&lt;li&gt;&lt;code&gt;s/from/to/&lt;/code&gt;  把 from 指定的字符串替换成 to 指定的字符串，from 可以是正则表达式。
    
    </summary>
    
      <category term="Linux" scheme="http://blog.dongfei.xin/categories/Linux/"/>
    
      <category term="Vim" scheme="http://blog.dongfei.xin/categories/Linux/Vim/"/>
    
    
      <category term="Linux" scheme="http://blog.dongfei.xin/tags/Linux/"/>
    
      <category term="Vim" scheme="http://blog.dongfei.xin/tags/Vim/"/>
    
  </entry>
  
  <entry>
    <title>Centos 安装 MongoDB </title>
    <link href="http://blog.dongfei.xin/2018-04-13/Centos-%E5%AE%89%E8%A3%85-MongoDB/"/>
    <id>http://blog.dongfei.xin/2018-04-13/Centos-安装-MongoDB/</id>
    <published>2018-04-13T07:39:20.000Z</published>
    <updated>2018-04-13T11:54:49.620Z</updated>
    
    <content type="html"><![CDATA[<h5 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h5><p>MongoDB 是一个介于关系数据库和非关系数据库之间的产品，是非关系数据库当中功能最丰富，最像关系数据库的。它支持的数据结构非常松散，是类似 json 的 bson 格式，因此可以存储比较复杂的数据类型。Mongo 最大的特点是它支持的查询语言非常强大，其语法有点类似于面向对象的查询语言，几乎可以实现类似关系数据库单表查询的绝大部分功能，而且还支持对数据建立索引。</p><p><img src="https://iamdongfei.oss-cn-shenzhen.aliyuncs.com/blog/MongoDB.png" alt="MongoDB"></p><a id="more"></a><p>Packages 包说明<br>MongoDB 官方源中包含以下几个依赖包：<br>mongodb-org: MongoDB 元数据包，安装时自动安装下面四个组件包：</p><ul><li>mongodb-org-server: 包含 MongoDB 守护进程和相关的配置和初始化脚本。</li><li>mongodb-org-mongos: 包含 mongos 的守护进程。</li><li>mongodb-org-shell: 包含 mongo shell。</li><li>mongodb-org-tools: 包含 MongoDB 的工具： mongoimport、 bsondump、 mongodump、 mongoexport、 mongofiles、 mongooplog、 mongoperf、 mongorestore、 mongostat、 and mongotop 。</li></ul><h5 id="安装步骤"><a href="#安装步骤" class="headerlink" title="安装步骤"></a>安装步骤</h5><h6 id="配置-MongoDB-的-yum-源"><a href="#配置-MongoDB-的-yum-源" class="headerlink" title="配置 MongoDB 的 yum 源"></a>配置 MongoDB 的 yum 源</h6><p>创建 yum 源文件：<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/yum<span class="selector-class">.repos</span><span class="selector-class">.d</span>/mongodb-org-<span class="number">3.4</span>.repo</span><br></pre></td></tr></table></figure></p><p>添加以下内容：<br><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">[mongodb-org-3.4]</span>  </span><br><span class="line"><span class="attr">name</span>=MongoDB Repository  </span><br><span class="line"><span class="attr">baseurl</span>=https://repo.mongodb.org/yum/redhat/<span class="variable">$releasever</span>/mongodb-org/<span class="number">3.4</span>/x<span class="number">86_64</span>/  </span><br><span class="line"><span class="attr">gpgcheck</span>=<span class="number">1</span>  </span><br><span class="line"><span class="attr">enabled</span>=<span class="number">1</span>  </span><br><span class="line"><span class="attr">gpgkey</span>=https://www.mongodb.org/static/pgp/server-<span class="number">3.4</span>.asc</span><br></pre></td></tr></table></figure></p><p>这里可以修改 <code>gpgcheck=0</code>、 省去 gpg 验证</p><p>安装之前先更新所有包 ：<code>yum update</code> （可选操作）</p><h6 id="安装-MongoDB-命令："><a href="#安装-MongoDB-命令：" class="headerlink" title="安装 MongoDB 命令："></a>安装 MongoDB 命令：</h6><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum -y <span class="keyword">install</span> mongodb-org</span><br></pre></td></tr></table></figure><p>安装完成后</p><p>查看 mongo 安装位置 <code>whereis mongod</code></p><p>查看修改配置文件 ：<code>vim /etc/mongod.conf</code></p><h5 id="启动-MongoDB-服务"><a href="#启动-MongoDB-服务" class="headerlink" title="启动 MongoDB 服务"></a>启动 MongoDB 服务</h5><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">启动 mongodb ：systemctl start mongod.service</span><br><span class="line">停止 mongodb ：systemctl stop mongod.service</span><br><span class="line">查看 mongodb 的状态：systemctl status mongod.service</span><br></pre></td></tr></table></figure><h5 id="设置开机启动"><a href="#设置开机启动" class="headerlink" title="设置开机启动"></a>设置开机启动</h5><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl <span class="builtin-name">enable</span> mongod.service</span><br></pre></td></tr></table></figure><h5 id="外网访问需要关闭防火墙："><a href="#外网访问需要关闭防火墙：" class="headerlink" title="外网访问需要关闭防火墙："></a>外网访问需要关闭防火墙：</h5><p>CentOS 7.0 默认使用的是 firewall 作为防火墙，这里改为 iptables 防火墙。</p><p>关闭 firewall：<br><figure class="highlight gauss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">systemctl <span class="keyword">stop</span> firewalld.service    <span class="meta"># 停止firewall</span></span><br><span class="line">systemctl <span class="keyword">disable</span> firewalld.service <span class="meta"># 禁止firewall开机启动</span></span><br></pre></td></tr></table></figure></p><h5 id="启动-Mongo-shell"><a href="#启动-Mongo-shell" class="headerlink" title="启动 Mongo shell"></a>启动 Mongo shell</h5><p>命令：<code>mongo</code> </p><p>查看数据库：<code>show dbs</code></p><h5 id="设置-mongodb-远程访问："><a href="#设置-mongodb-远程访问：" class="headerlink" title="设置 mongodb 远程访问："></a>设置 mongodb 远程访问：</h5><p>编辑 <code>mongod.conf</code> 注释 <code>bindIp</code>、并重启 mongodb.</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/mongod.conf</span><br><span class="line"></span><br><span class="line">------mongod.conf-----</span><br><span class="line">net:</span><br><span class="line">  port: 27017</span><br><span class="line">  # bindIp: 127.0.0.1  # Listen <span class="keyword">to</span> local<span class="built_in"> interface </span>only, comment <span class="keyword">to</span> listen on all interfaces.</span><br><span class="line"></span><br><span class="line">----------------</span><br><span class="line">重启 mongodb：systemctl restart mongod.service</span><br></pre></td></tr></table></figure><p>参考：</p><p><a href="https://www.cnblogs.com/web424/p/6928992.html" target="_blank" rel="noopener"> centos7 安装 MongoDB3.4 </a></p><p><a href="https://blog.csdn.net/dhfttkl123/article/details/53284238" target="_blank" rel="noopener"> Mongodb 3.2 开启密码认证 </a></p>]]></content>
    
    <summary type="html">
    
      &lt;h5 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h5&gt;&lt;p&gt;MongoDB 是一个介于关系数据库和非关系数据库之间的产品，是非关系数据库当中功能最丰富，最像关系数据库的。它支持的数据结构非常松散，是类似 json 的 bson 格式，因此可以存储比较复杂的数据类型。Mongo 最大的特点是它支持的查询语言非常强大，其语法有点类似于面向对象的查询语言，几乎可以实现类似关系数据库单表查询的绝大部分功能，而且还支持对数据建立索引。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://iamdongfei.oss-cn-shenzhen.aliyuncs.com/blog/MongoDB.png&quot; alt=&quot;MongoDB&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Linux" scheme="http://blog.dongfei.xin/categories/Linux/"/>
    
      <category term="CentOS" scheme="http://blog.dongfei.xin/categories/Linux/CentOS/"/>
    
    
      <category term="CentOS" scheme="http://blog.dongfei.xin/tags/CentOS/"/>
    
      <category term="MongoDB" scheme="http://blog.dongfei.xin/tags/MongoDB/"/>
    
  </entry>
  
  <entry>
    <title>Centos 配置 Shadowsocks 翻墙</title>
    <link href="http://blog.dongfei.xin/2018-04-13/Centos-%E9%85%8D%E7%BD%AE-Shadowsocks-%E7%BF%BB%E5%A2%99/"/>
    <id>http://blog.dongfei.xin/2018-04-13/Centos-配置-Shadowsocks-翻墙/</id>
    <published>2018-04-13T06:20:19.000Z</published>
    <updated>2018-04-14T15:24:10.761Z</updated>
    
    <content type="html"><![CDATA[<p><blockquote class="blockquote-center">再牛逼的梦想也抵不住傻逼似得坚持</blockquote><br><img src="https://iamdongfei.oss-cn-shenzhen.aliyuncs.com/blog/shadowsocks.png" alt="shadowsocks"></p><h5 id="Socks5-全局代理"><a href="#Socks5-全局代理" class="headerlink" title="Socks5 全局代理"></a>Socks5 全局代理</h5><h6 id="安装-sslocal"><a href="#安装-sslocal" class="headerlink" title="安装 sslocal"></a>安装 sslocal</h6><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pip <span class="keyword">install </span><span class="keyword">shadowsocks </span><span class="comment"># pip 安装 ss 客户端</span></span><br><span class="line">如果提示 -<span class="keyword">bash: </span>pip: command not found</span><br><span class="line">运行 yum -y <span class="keyword">install </span>python-pip</span><br></pre></td></tr></table></figure><a id="more"></a><h6 id="配置-shadowsocks-json"><a href="#配置-shadowsocks-json" class="headerlink" title="配置 shadowsocks.json"></a>配置 shadowsocks.json</h6><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">vim</span> <span class="string">/etc/shadowsocks.json</span></span><br><span class="line"><span class="bullet">-</span><span class="bullet">--</span> <span class="string">shadowsocks.json</span> <span class="meta">---</span></span><br><span class="line"><span class="string">&#123;</span></span><br><span class="line"><span class="attr">    "server":</span><span class="string">"SERVER-IP"</span><span class="string">,</span>   <span class="comment"># 你的服务器ip</span></span><br><span class="line"><span class="attr">    "server_port":</span><span class="string">PORT,</span>    <span class="comment"># 服务器端口</span></span><br><span class="line"><span class="attr">    "local_address":</span> <span class="string">"127.0.0.1"</span><span class="string">,</span></span><br><span class="line"><span class="attr">    "local_port":</span><span class="number">1080</span><span class="string">,</span></span><br><span class="line"><span class="attr">    "password":</span><span class="string">"PASSWORD"</span><span class="string">,</span>    <span class="comment"># 密码</span></span><br><span class="line"><span class="attr">    "timeout":</span><span class="number">300</span><span class="string">,</span></span><br><span class="line"><span class="attr">    "method":</span><span class="string">"aes-128-cfb"</span><span class="string">,</span> <span class="comment"># 加密方式</span></span><br><span class="line"><span class="attr">    "fast_open":</span> <span class="literal">false</span><span class="string">,</span></span><br><span class="line"><span class="attr">    "workers":</span> <span class="number">1</span></span><br><span class="line"><span class="string">&#125;</span></span><br><span class="line"><span class="bullet">-</span><span class="bullet">--</span> <span class="string">shadowsocks.json</span> <span class="meta">---</span></span><br></pre></td></tr></table></figure><h6 id="运行-sslocal"><a href="#运行-sslocal" class="headerlink" title="运行 sslocal"></a>运行 sslocal</h6><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup sslocal -c /etc/shadowsocks<span class="selector-class">.json</span> &amp;&gt;&gt; /var/log/sslocal<span class="selector-class">.log</span> &amp;</span><br></pre></td></tr></table></figure><h5 id="Privoxy-篇"><a href="#Privoxy-篇" class="headerlink" title="Privoxy 篇"></a>Privoxy 篇</h5><h6 id="安装-privoxy"><a href="#安装-privoxy" class="headerlink" title="安装 privoxy"></a>安装 privoxy</h6><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum -y <span class="keyword">install</span> privoxy</span><br></pre></td></tr></table></figure><h6 id="配置-socks5-全局代理"><a href="#配置-socks5-全局代理" class="headerlink" title="配置 socks5 全局代理"></a>配置 socks5 全局代理</h6><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">echo <span class="string">'forward-socks5 / 127.0.0.1:1080 .'</span> <span class="meta">&gt;&gt; </span>/etc/privoxy/config</span><br></pre></td></tr></table></figure><h6 id="设置-http-https-代理"><a href="#设置-http-https-代理" class="headerlink" title="设置 http/https 代理"></a>设置 http/https 代理</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export http_proxy=http://127.0.0.1:8118 # privoxy默认监听端口为8118</span><br><span class="line">export https_proxy=http://127.0.0.1:8118</span><br></pre></td></tr></table></figure><h6 id="运行-privoxy"><a href="#运行-privoxy" class="headerlink" title="运行 privoxy"></a>运行 privoxy</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">service privoxy start</span><br></pre></td></tr></table></figure><h6 id="测试-socks5-全局代理"><a href="#测试-socks5-全局代理" class="headerlink" title="测试 socks5 全局代理"></a>测试 socks5 全局代理</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">curl www.google.com</span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># 如果出现下面这段输出则代理成功！</span></span></span><br><span class="line">------------------------------------------------------------------------------</span><br><span class="line">&lt;HTML&gt;&lt;HEAD&gt;&lt;meta http-equiv="content-type" content="text/html;charset=utf-8"&gt;</span><br><span class="line">&lt;TITLE&gt;302 Moved&lt;/TITLE&gt;&lt;/HEAD&gt;&lt;BODY&gt;</span><br><span class="line">&lt;H1&gt;302 Moved&lt;/H1&gt;</span><br><span class="line">The document has moved</span><br><span class="line">&lt;A HREF="http://www.google.com.hk/url?sa=p&amp;amp;hl=zh-CN&amp;amp;pref=hkredirect&amp;amp;pval=yes&amp;amp;q=http://www.google.com.hk/%3Fgws_rd%3Dcr&amp;amp;ust=1480320257875871&amp;amp;usg=AFQjCNHg9F5zMg83aD2KKHHHf-yecq0nfQ"&gt;here&lt;/A&gt;.</span><br><span class="line">&lt;/BODY&gt;&lt;/HTML&gt;</span><br><span class="line">------------------------------------------------------------------------------</span><br></pre></td></tr></table></figure><h5 id="简化使用"><a href="#简化使用" class="headerlink" title="简化使用"></a>简化使用</h5><p>进过上面的步骤我们的确代理成功了。但是每次都要输入这么多命令太麻烦<br>这时我们可以利用 <strong>命令别名</strong> 来简化我们的操作<br><figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">alias</span> ssinit='nohup sslocal -c <span class="string">/etc/shadowsocks.json</span> &amp;&gt;&gt; <span class="string">/var/log/sslocal.log</span> &amp;'</span><br><span class="line"><span class="keyword">alias</span> sson='export http_proxy=http:<span class="string">//127.0.0.1</span><span class="function">:8118</span> &amp;&amp; export https_proxy=http:<span class="string">//127.0.0.1</span><span class="function">:8118</span> &amp;&amp; systemctl start privoxy'</span><br><span class="line"><span class="keyword">alias</span> ssoff='<span class="keyword">unset</span> http_proxy &amp;&amp; <span class="keyword">unset</span> https_proxy &amp;&amp; systemctl stop privoxy &amp;&amp; pkill sslocal'</span><br></pre></td></tr></table></figure></p><h6 id="使用方法"><a href="#使用方法" class="headerlink" title="使用方法"></a>使用方法</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">## 开启ss代理</span></span></span><br><span class="line">ssinit</span><br><span class="line">sson</span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># 关闭ss代理</span></span></span><br><span class="line">ssoff</span><br></pre></td></tr></table></figure><p>参考：</p><ul><li><p><a href="https://i.jakeyu.top/2017/03/16/centos%E4%BD%BF%E7%94%A8SS%E7%BF%BB%E5%A2%99/" target="_blank" rel="noopener">centos使用SS翻墙</a></p></li><li><p><a href="https://bodyno.com/tool/2017/09/03/centos-ss.html" target="_blank" rel="noopener">如何使用Shadowsocks让centos翻墙</a></p></li><li><p><a href="https://blog.csdn.net/xwydq/article/details/51274185" target="_blank" rel="noopener">linux下的ss+privoxy代理配置</a></p></li><li><p><a href="https://thief.one/2017/02/22/Shadowsocks%E6%8A%98%E8%85%BE%E8%AE%B0/" target="_blank" rel="noopener">Shadowsocks折腾记</a></p></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;blockquote class=&quot;blockquote-center&quot;&gt;再牛逼的梦想也抵不住傻逼似得坚持&lt;/blockquote&gt;&lt;br&gt;&lt;img src=&quot;https://iamdongfei.oss-cn-shenzhen.aliyuncs.com/blog/shadowsocks.png&quot; alt=&quot;shadowsocks&quot;&gt;&lt;/p&gt;
&lt;h5 id=&quot;Socks5-全局代理&quot;&gt;&lt;a href=&quot;#Socks5-全局代理&quot; class=&quot;headerlink&quot; title=&quot;Socks5 全局代理&quot;&gt;&lt;/a&gt;Socks5 全局代理&lt;/h5&gt;&lt;h6 id=&quot;安装-sslocal&quot;&gt;&lt;a href=&quot;#安装-sslocal&quot; class=&quot;headerlink&quot; title=&quot;安装 sslocal&quot;&gt;&lt;/a&gt;安装 sslocal&lt;/h6&gt;&lt;figure class=&quot;highlight mipsasm&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;pip &lt;span class=&quot;keyword&quot;&gt;install &lt;/span&gt;&lt;span class=&quot;keyword&quot;&gt;shadowsocks &lt;/span&gt;&lt;span class=&quot;comment&quot;&gt;# pip 安装 ss 客户端&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;如果提示 -&lt;span class=&quot;keyword&quot;&gt;bash: &lt;/span&gt;pip: command not found&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;运行 yum -y &lt;span class=&quot;keyword&quot;&gt;install &lt;/span&gt;python-pip&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="Linux" scheme="http://blog.dongfei.xin/categories/Linux/"/>
    
      <category term="CentOS" scheme="http://blog.dongfei.xin/categories/Linux/CentOS/"/>
    
    
      <category term="CentOS" scheme="http://blog.dongfei.xin/tags/CentOS/"/>
    
      <category term="翻墙" scheme="http://blog.dongfei.xin/tags/%E7%BF%BB%E5%A2%99/"/>
    
  </entry>
  
  <entry>
    <title>Scrapinghub 的 Scrapy 技巧系列（六）</title>
    <link href="http://blog.dongfei.xin/2018-04-12/Scrapinghub-%E7%9A%84-Scrapy-%E6%8A%80%E5%B7%A7%E7%B3%BB%E5%88%97%EF%BC%88%E5%85%AD%EF%BC%89/"/>
    <id>http://blog.dongfei.xin/2018-04-12/Scrapinghub-的-Scrapy-技巧系列（六）/</id>
    <published>2018-04-11T16:58:27.000Z</published>
    <updated>2018-04-13T07:20:37.845Z</updated>
    
    <content type="html"><![CDATA[<p>原文：<a href="https://blog.scrapinghub.com/2016/07/20/scrapy-tips-from-the-pros-july-2016/" target="_blank" rel="noopener"> Scrapy Tips from the Pros: July 2016 </a></p><hr><p><a href="http://scrapy.org" target="_blank" rel="noopener"> Scrapy </a>被设计成可扩展，并且组件之间松耦合。你可以轻松地使用自己的中间件或者 pipeline 扩展 Scrapy 的功能。</p><p>这使得 Scrapy 社区可以很容易地开发新的插件来改善现有功能，而不需改变 Scrapy 自身。在这篇文章中，我们将向你展示如何利用 DeltaFetch 插件来进行增量爬取。</p><p><img src="https://dongfei.oss-cn-shanghai.aliyuncs.com/blog/scrapy-tips.png" alt="Scrapy Tips"></p><a id="more"></a><h2 id="使用-Deltafetch-进行增量爬取"><a href="#使用-Deltafetch-进行增量爬取" class="headerlink" title="使用 Deltafetch 进行增量爬取"></a>使用 Deltafetch 进行增量爬取</h2><p>我们开发的一些爬虫设计成一次性爬取并抓取我们所需的数据。另一方面，许多爬虫需要周期性地爬取，以便让我们的数据集保持最新。</p><p>在这些周期爬虫中，我们只对最后一次爬取的最新页面感兴趣。例如，我们有一个从一堆网络媒体网点爬取文章的爬虫。该爬虫一天执行一次，并且它们首先从预定义的首页检索文章URL。然后，从每篇文章上提取标题、作者、日期和内容。这种方法通常会导致许多重复结果，并且使得每次我们运行爬虫时，爬取的数量越来越多。</p><p>幸运的是，我们并不是第一个有这个问题的人。社区已经有了解决方法：<a href="https://github.com/scrapy-plugins/scrapy-deltafetch" target="_blank" rel="noopener"> scrapy-deltafetch插件 </a>。你可以用这个插件进行增量爬取。 DeltaFetch 的主要目的是避免请求那些之前已经爬过的页面，即使它在之前的执行中已经出现了。它只会对那些之前没有提取任何项的页面、爬虫的 <code>start_urls</code> 属性中的URL、或者在爬虫的 <code>start_requests</code> 方法中生成的 Request 进行请求。</p><p>DeltaFetch 的工作原理是，对爬虫回调中生成的每一个 Item 和 Request 对象进行拦截。对于 Item ，它计算相关的 Request 标识符(又名，<a href="https://github.com/scrapy/scrapy/blob/master/scrapy/utils/request.py#L19" target="_blank" rel="noopener"> 指纹(fingerprint) </a>)，并将其存储到一个本地数据库中。对于 Request ，Deltafetch 计算 Request fingerprint ，并在在其已存在数据库的时候丢弃该 Request。</p><p>现在，看看如何为你的 Scrapy 爬虫设置 Deltafetch 。</p><h3 id="开始使用-DeltaFetch"><a href="#开始使用-DeltaFetch" class="headerlink" title="开始使用 DeltaFetch"></a>开始使用 DeltaFetch</h3><p>首先，用 pip 安装 DeltaFetch ：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ pip install scrapy-deltafetch</span><br></pre></td></tr></table></figure><p>然后，你必须在你的项目的 settings.py 文件中启用它：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">SPIDER_MIDDLEWARES = &#123;</span><br><span class="line">    <span class="string">'scrapy_deltafetch.DeltaFetch'</span>: <span class="number">100</span>,</span><br><span class="line">&#125;</span><br><span class="line">DELTAFETCH_ENABLED = <span class="keyword">True</span></span><br></pre></td></tr></table></figure><h3 id="使用-DeltaFetch"><a href="#使用-DeltaFetch" class="headerlink" title="使用 DeltaFetch"></a>使用 DeltaFetch</h3><p><a href="https://github.com/stummjr/books_crawler/" target="_blank" rel="noopener"> 这个爬虫 </a>有一个爬取<a href="http://books.toscrape.com" target="_blank" rel="noopener"> books.toscrape.com </a>的蜘蛛。它通过所有列出的页面进行导航，访问每本书的详细页面，获取一些数据，例如书标题、描述和目录。该爬虫每天执行一次，以捕获对应目录中包含的新书。无需访问那些已经爬过的书页面，因为由爬虫收集的数据通常不会改变。</p><p>想看看 Deltafetch 的使用，<a href="https://github.com/stummjr/books_crawler/" target="_blank" rel="noopener"> clone这个repo </a>，其中，已经在 settings.py 启用了 DeltaFetch ，然后运行：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ scrapy crawl toscrape</span><br></pre></td></tr></table></figure><p>等它结束，然后看看 Scrapy 在最后记录的统计数据：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="number">2016</span><span class="number">-07</span><span class="number">-19</span> <span class="number">10</span>:<span class="number">17</span>:<span class="number">53</span> [scrapy] INFO: Dumping Scrapy stats:</span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">'deltafetch/stored'</span>: <span class="number">1000</span>,</span><br><span class="line">    ...</span><br><span class="line">    <span class="string">'downloader/request_count'</span>: <span class="number">1051</span>,</span><br><span class="line">    ...</span><br><span class="line">    <span class="string">'item_scraped_count'</span>: <span class="number">1000</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>除此之外，你会看到爬虫进行了 1051 次请求来爬取 1000 个项，而 DeltaFetch 存储了 1000 个请求的 fingerprint 。这意味着，只有 51 个页面请求没有生成 item ，因此下次还会继续访问他们。</p><p>现在，再次运行该爬虫，你会看到许多像这样的日志信息：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="number">2016</span><span class="number">-07</span><span class="number">-19</span> <span class="number">10</span>:<span class="number">47</span>:<span class="number">10</span> [toscrape] INFO: Ignoring already visited: </span><br><span class="line">&lt;GET http://books.toscrape.com/....../index.html&gt;</span><br></pre></td></tr></table></figure><p>而在统计数据中，你会看到，跳过了 1000 个请求，因为在之前的爬取中，已经爬到了 item 。现在，该爬虫并未提取任何 item ，并且它只进行了 51 次请求，它们所有都是之前没有爬取到 item 的页面：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="number">2016</span><span class="number">-07</span><span class="number">-19</span> <span class="number">10</span>:<span class="number">47</span>:<span class="number">10</span> [scrapy] INFO: Dumping Scrapy stats:</span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">'deltafetch/skipped'</span>: <span class="number">1000</span>,</span><br><span class="line">    ...</span><br><span class="line">    <span class="string">'downloader/request_count'</span>: <span class="number">51</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="修改数据库键"><a href="#修改数据库键" class="headerlink" title="修改数据库键"></a>修改数据库键</h3><p>默认情况下，DeltaFetch 使用一个 Request fingerprint 来区分 Request 。该 fingerprint 是一个基于规范 URL、HTTP 方法和请求体计算的哈希值。</p><p>一些网站对于相同的数据会有多个 URL 。例如，一个电子商务网站可能有指向同一个产品的 URL ，如下所示：</p><ul><li><a href="http://www.example.com/product?id=123" target="_blank" rel="noopener">http://www.example.com/product?id=123</a></li><li><a href="http://www.example.com/deals?id=123" target="_blank" rel="noopener">http://www.example.com/deals?id=123</a></li><li><a href="http://www.example.com/category/keyboards?id=123" target="_blank" rel="noopener">http://www.example.com/category/keyboards?id=123</a></li><li><a href="http://www.example.com/category/gaming?id=123" target="_blank" rel="noopener">http://www.example.com/category/gaming?id=123</a></li></ul><p>在这些情况下，Request fingerprint 并不适用，因为规范的 URL 将会不同，即使 item 是相同的。在这个例子中，我们可以使用产品的 ID 作为 DeltaFetch 键。</p><p>DeltaFetch允许我们在初始化Request时，通过传递一个名为<code>deltafetch_key</code>的元参数来自定义键：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> w3lib.url <span class="keyword">import</span> url_query_parameter</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">for</span> product_url <span class="keyword">in</span> response.css(<span class="string">'a.product_listing'</span>):</span><br><span class="line">        <span class="keyword">yield</span> Request(</span><br><span class="line">            product_url,</span><br><span class="line">            meta=&#123;<span class="string">'deltafetch_key'</span>: url_query_parameter(product_url, <span class="string">'id'</span>)&#125;,</span><br><span class="line">            callback=self.parse_product_page</span><br><span class="line">        )</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure><p>通过这种方式， DeltaFetch 将会忽略对重复页面进行请求，即使它们有不同的 URL 。</p><h3 id="重置-DeltaFetch"><a href="#重置-DeltaFetch" class="headerlink" title="重置 DeltaFetch"></a>重置 DeltaFetch</h3><p>如果你想要重新爬取页面，可以通过传递一个 <code>deltafetch_reset</code> 参数给你的爬虫，来重置 DeltaFetch 缓存：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ scrapy crawl example -a deltafetch_reset=<span class="number">1</span></span><br></pre></td></tr></table></figure><h3 id="在-Scrapy-Cloud-上使用-DeltaFetch"><a href="#在-Scrapy-Cloud-上使用-DeltaFetch" class="headerlink" title="在 Scrapy Cloud 上使用 DeltaFetch"></a>在 Scrapy Cloud 上使用 DeltaFetch</h3><p>你也可以对运行在<a href="https://app.scrapinghub.com/account/signup/" target="_blank" rel="noopener"> Scrapy Cloud </a>之上的爬虫使用 DeltaFetch 。仅需在你项目的 Addons 页面启用 DeltaFetch 和 DotScrapy Persistence 插件。后者是用来允许你的爬虫访问 .scrapy 文件夹，该文件夹是 DeltaFetch 存储其数据库的地方。</p><p><img src="https://dongfei.oss-cn-shanghai.aliyuncs.com/blog/image00.png" alt="image00"></p><p>Deltafetch 在我们已经看到的那些情况下是非常方便的。<strong>请记住，Deltafetch 只是避免了发送请求到之前已经生成了 item 的页面，并且仅当这些请求尚未由爬虫的s tart_urls 或者 start_requests 生成。</strong>那些来自于没有直接爬取到 item 的页面，在每一次你运行你的爬虫的时候，将仍会抓取。</p><p>你可以看看 github 上该项目页以获取更多信息：<a href="http://github.com/scrapy-plugins/scrapy-deltafetch" target="_blank" rel="noopener">http://github.com/scrapy-plugins/scrapy-deltafetch</a></p><h2 id="总结一下"><a href="#总结一下" class="headerlink" title="总结一下"></a>总结一下</h2><p>你可以在 Github 的<a href="https://github.com/scrapy-plugins" target="_blank" rel="noopener"> scrapy-plugins </a>页面上找到许多有趣的 Scrapy 插件，你还可以在那里包含自己的插件来回馈社区。</p><p>如果你有问题，或者你想在这个每月专栏上看到某个主题，请在这里（ Ele 注，到原文留言哈）留下评论，让我们知道，或者通过在 Twitter 上<a href="http://twitter.com/scrapinghub" target="_blank" rel="noopener"> @scrapinghub </a>来找到我们。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;原文：&lt;a href=&quot;https://blog.scrapinghub.com/2016/07/20/scrapy-tips-from-the-pros-july-2016/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt; Scrapy Tips from the Pros: July 2016 &lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;a href=&quot;http://scrapy.org&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt; Scrapy &lt;/a&gt;被设计成可扩展，并且组件之间松耦合。你可以轻松地使用自己的中间件或者 pipeline 扩展 Scrapy 的功能。&lt;/p&gt;
&lt;p&gt;这使得 Scrapy 社区可以很容易地开发新的插件来改善现有功能，而不需改变 Scrapy 自身。在这篇文章中，我们将向你展示如何利用 DeltaFetch 插件来进行增量爬取。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://dongfei.oss-cn-shanghai.aliyuncs.com/blog/scrapy-tips.png&quot; alt=&quot;Scrapy Tips&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Python" scheme="http://blog.dongfei.xin/categories/Python/"/>
    
      <category term="Scrapy" scheme="http://blog.dongfei.xin/categories/Python/Scrapy/"/>
    
    
      <category term="Scrapy" scheme="http://blog.dongfei.xin/tags/Scrapy/"/>
    
  </entry>
  
  <entry>
    <title>Scrapinghub 的 Scrapy 技巧系列（五）</title>
    <link href="http://blog.dongfei.xin/2018-04-12/Scrapinghub-%E7%9A%84-Scrapy-%E6%8A%80%E5%B7%A7%E7%B3%BB%E5%88%97%EF%BC%88%E4%BA%94%EF%BC%89/"/>
    <id>http://blog.dongfei.xin/2018-04-12/Scrapinghub-的-Scrapy-技巧系列（五）/</id>
    <published>2018-04-11T16:57:39.000Z</published>
    <updated>2018-04-12T11:57:54.862Z</updated>
    
    <content type="html"><![CDATA[<p>原文：<a href="https://blog.scrapinghub.com/2016/06/22/scrapy-tips-from-the-pros-june-2016/" target="_blank" rel="noopener"> Scrapy Tips from the Pros June 2016 </a></p><hr><p>欢迎来到 Scrapy 技巧系列！每个月，我们会发布一些技巧和 hack ，来帮助你加快网页抓取和数据提取。作为牵头的 Scrapy 维护者，你可以想象的任何障碍我们都遇到过了，所以别担心，你能在这获益良多。随意到<a href="https://twitter.com/ScrapingHub" target="_blank" rel="noopener"> Twitter </a>或者<a href="https://www.facebook.com/ScrapingHub/" target="_blank" rel="noopener"> Facebook </a>访问我们，提出对未来主题的建议吧。</p><p><img src="https://dongfei.oss-cn-shanghai.aliyuncs.com/blog/scrapy-tips.png" alt="Scrapy Tips"></p><a id="more"></a><h2 id="抓取无限滚动页面"><a href="#抓取无限滚动页面" class="headerlink" title="抓取无限滚动页面"></a>抓取无限滚动页面</h2><p>在单页应用以及每页具有大量 AJAX 请求的时代，很多网站已经用花哨的无限滚动机制取代了 <strong>前一个/下一个</strong>分页按钮。使用这种技术的网站每当用户滚动到页面的底部的时候加载新项（想想微博，Facebook，谷歌图片）。虽然<a href="https://www.smashingmagazine.com/2013/05/infinite-scrolling-lets-get-to-the-bottom-of-this/" target="_blank" rel="noopener"> UX专家 </a>认为，无限滚动为用户提供了海量数据，但是我们看到越来越多的 web 页面诉诸于展示这种无休止的结果列表。</p><p>在开发 web 爬虫时，我们要做的第一件事就是找到能将我们引导到下一页结果的带有链接的 UI 组件。不幸的是，这些链接在无限滚动页面上不存在。</p><p>虽然这种场景可能看起来像诸如<a href="http://scrapinghub.com/splash/" target="_blank" rel="noopener"> Splash </a>或者<a href="http://www.seleniumhq.org/" target="_blank" rel="noopener"> Selenium </a>这样的 JavaScript 引擎的一个经典案例，但是它实际上是一个简单的修复。你所需要做的是在你滚动目标页面的时候检查浏览器的 AJAX 请求，然后在 Scrapy spider 中重新创建这些请求，而不是模拟用于与此类引擎的交互。</p><p>让我们以<a href="http://spidyquotes.herokuapp.com/scroll" target="_blank" rel="noopener"> Spidy Quotes </a>为例，构建一个爬虫来获取上面列出来的所有的项。</p><h2 id="审查页面"><a href="#审查页面" class="headerlink" title="审查页面"></a>审查页面</h2><p>先说重要的事，我们需要理解无限滚动是如何在这个页面工作的，我们可以通过<a href="https://developer.chrome.com/devtools#access" target="_blank" rel="noopener"> 浏览器的开发者工具 </a>中的Network面板来完成此项工作。打开该面板，然后滚动页面，看看浏览器发送了什么请求：</p><p><img src="https://scrapinghub.files.wordpress.com/2016/06/scrapy-tips-from-the-pros-june.png?w=648" alt="scrapy tips from the pros june"></p><p>点开一个请求仔细看看。浏览器发送了一个请求到<code>/api/quotes?page=x</code>，然后接收诸如以下的一个 JSON 对象作为响应：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">   <span class="string">"has_next"</span>:true,</span><br><span class="line">   <span class="string">"page"</span>:<span class="number">8</span>,</span><br><span class="line">   <span class="string">"quotes"</span>:[</span><br><span class="line">      &#123;</span><br><span class="line">         <span class="string">"author"</span>:&#123;</span><br><span class="line">            <span class="string">"goodreads_link"</span>:<span class="string">"/author/show/1244.Mark_Twain"</span>,</span><br><span class="line">            <span class="string">"name"</span>:<span class="string">"Mark Twain"</span></span><br><span class="line">         &#125;,</span><br><span class="line">         <span class="string">"tags"</span>:[<span class="string">"individuality"</span>, <span class="string">"majority"</span>, <span class="string">"minority"</span>, <span class="string">"wisdom"</span>],</span><br><span class="line">         <span class="string">"text"</span>:<span class="string">"Whenever you find yourself on the side of the ..."</span></span><br><span class="line">      &#125;,</span><br><span class="line">      &#123;</span><br><span class="line">         <span class="string">"author"</span>:&#123;</span><br><span class="line">            <span class="string">"goodreads_link"</span>:<span class="string">"/author/show/1244.Mark_Twain"</span>,</span><br><span class="line">            <span class="string">"name"</span>:<span class="string">"Mark Twain"</span></span><br><span class="line">         &#125;,</span><br><span class="line">         <span class="string">"tags"</span>:[<span class="string">"books"</span>, <span class="string">"contentment"</span>, <span class="string">"friends"</span>],</span><br><span class="line">         <span class="string">"text"</span>:<span class="string">"Good friends, good books, and a sleepy ..."</span></span><br><span class="line">      &#125;</span><br><span class="line">   ],</span><br><span class="line">   <span class="string">"tag"</span>:null,</span><br><span class="line">   <span class="string">"top_ten_tags"</span>:[[<span class="string">"love"</span>, <span class="number">49</span>], [<span class="string">"inspirational"</span>, <span class="number">43</span>], ...]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这就是我们的爬虫需要的信息了。它所需要做的仅是生成到 <code>/api/quotes?page=x</code>的请求，其中，<code>x</code> 的值不断增加，直到 <code>has_next</code> 字段为 false 。这样做最棒的是，我们甚至无需爬取HTML内容以获取所需数据。这些数据都在一个漂亮的机器可读的 JSON 中。</p><h2 id="构建Spider"><a href="#构建Spider" class="headerlink" title="构建Spider"></a>构建Spider</h2><p>下面是我们的 spider 。它从服务器返回的 JSON 内容提取目标数据。这种方法比挖掘页面的 HTML 树更容易并且更健壮，相信布局的改变不会搞挂我们的 spider 。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SpidyQuotesSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">'spidyquotes'</span></span><br><span class="line">    quotes_base_url = <span class="string">'http://spidyquotes.herokuapp.com/api/quotes?page=%s'</span></span><br><span class="line">    start_urls = [quotes_base_url % <span class="number">1</span>]</span><br><span class="line">    download_delay = <span class="number">1.5</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        data = json.loads(response.body)</span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> data.get(<span class="string">'quotes'</span>, []):</span><br><span class="line">            <span class="keyword">yield</span> &#123;</span><br><span class="line">                <span class="string">'text'</span>: item.get(<span class="string">'text'</span>),</span><br><span class="line">                <span class="string">'author'</span>: item.get(<span class="string">'author'</span>, &#123;&#125;).get(<span class="string">'name'</span>),</span><br><span class="line">                <span class="string">'tags'</span>: item.get(<span class="string">'tags'</span>),</span><br><span class="line">            &#125;</span><br><span class="line">        <span class="keyword">if</span> data[<span class="string">'has_next'</span>]:</span><br><span class="line">            next_page = data[<span class="string">'page'</span>] + <span class="number">1</span></span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(self.quotes_base_url % next_page)</span><br></pre></td></tr></table></figure><p>要进一步练习这个技巧，你可以做个实验，爬取我们的博客，因为它也是使用无限滚动来加载旧博文的。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>如果你被爬取无限滚动网站的前景吓到，那么希望现在你可以有点信心了。下一次你需要处理那种基于用户操作引发的 AJAX 调用的页面时，看一看你的浏览器发送的请求吧，然后将其重放到你的 spider 中。响应往往是 JSON 的格式，这使得你的 spider 甚至更简单了。</p><p>好啦，这就是六月份的！请在<a href="https://twitter.com/ScrapingHub" target="_blank" rel="noopener"> Twitter </a>上联系我们，让我们知道未来你希望看到什么技巧。最近，我们还发布了一个<a href="https://blog.scrapinghub.com/2016/06/09/introducing-the-new-open-data-catalog/" target="_blank" rel="noopener"> 数据集目录 </a>，所以，如果你还苦思要爬取什么，那么看看这个目录获取一些灵感吧。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;原文：&lt;a href=&quot;https://blog.scrapinghub.com/2016/06/22/scrapy-tips-from-the-pros-june-2016/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt; Scrapy Tips from the Pros June 2016 &lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;欢迎来到 Scrapy 技巧系列！每个月，我们会发布一些技巧和 hack ，来帮助你加快网页抓取和数据提取。作为牵头的 Scrapy 维护者，你可以想象的任何障碍我们都遇到过了，所以别担心，你能在这获益良多。随意到&lt;a href=&quot;https://twitter.com/ScrapingHub&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt; Twitter &lt;/a&gt;或者&lt;a href=&quot;https://www.facebook.com/ScrapingHub/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt; Facebook &lt;/a&gt;访问我们，提出对未来主题的建议吧。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://dongfei.oss-cn-shanghai.aliyuncs.com/blog/scrapy-tips.png&quot; alt=&quot;Scrapy Tips&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Python" scheme="http://blog.dongfei.xin/categories/Python/"/>
    
      <category term="Scrapy" scheme="http://blog.dongfei.xin/categories/Python/Scrapy/"/>
    
    
      <category term="Scrapy" scheme="http://blog.dongfei.xin/tags/Scrapy/"/>
    
  </entry>
  
  <entry>
    <title>Scrapinghub 的 Scrapy 技巧系列（四）</title>
    <link href="http://blog.dongfei.xin/2018-04-12/Scrapinghub-%E7%9A%84-Scrapy-%E6%8A%80%E5%B7%A7%E7%B3%BB%E5%88%97%EF%BC%88%E5%9B%9B%EF%BC%89/"/>
    <id>http://blog.dongfei.xin/2018-04-12/Scrapinghub-的-Scrapy-技巧系列（四）/</id>
    <published>2018-04-11T16:43:30.000Z</published>
    <updated>2018-04-12T11:58:09.763Z</updated>
    
    <content type="html"><![CDATA[<p>欢迎来到 Scrapy 技巧系列！每个月，我们会发布一些技巧和 hack ，来帮助你加快网页抓取和数据提取。作为牵头的 Scrapy 维护者，你可以想象的任何障碍我们都遇到过了，所以别担心，你能在这获益良多。随意到<a href="https://twitter.com/ScrapingHub" target="_blank" rel="noopener"> Twitter </a>或者<a href="https://www.facebook.com/ScrapingHub/" target="_blank" rel="noopener"> Facebook </a>访问我们，提出对未来主题的建议吧。</p><p><img src="https://dongfei.oss-cn-shanghai.aliyuncs.com/blog/scrapy-tips.png" alt="Scrapy Tips"></p><a id="more"></a><h1 id="如何调试你的爬虫"><a href="#如何调试你的爬虫" class="headerlink" title="如何调试你的爬虫"></a>如何调试你的爬虫</h1><p>你的爬虫不工作了，但是你想不明白为啥。一个快速识别潜在问题的方法是添加一些打印语句，以找出发生了什么。这通常是我的第一个步骤，而有时我所需要做的是发现那些妨碍我的爬虫正常运行的错误。如果这个方法对你有效，那就太棒了，但是如果这个方法还不够，那么读下去，学学如何处理那些需要更加彻底调查的令人讨厌的 bug 。在这篇文章中，我将向你介绍一些工具，当涉及到调试爬虫时，它们应该在每个 Scrapy 用户的工作区中。</p><h2 id="Scrapy-Shell-是你的好基友"><a href="#Scrapy-Shell-是你的好基友" class="headerlink" title="Scrapy Shell 是你的好基友"></a>Scrapy Shell 是你的好基友</h2><p>Scrapy shell 是一个全功能的 Python shell ，它加载了与你在你的爬虫的回调方法中得到的上下文相同的上下文。你只需要提供一个 URL ，Scrapy Shell 就会让你与那个你的爬虫在它的回调中处理的相同的对象进行交互，包括 response 对象。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ scrapy shell http://blog.scrapinghub.com</span><br><span class="line">[s] Available Scrapy objects:</span><br><span class="line">[s]   crawler    &lt;scrapy.crawler.Crawler object at <span class="number">0x7f0638a2cbd0</span>&gt;</span><br><span class="line">[s]   item       &#123;&#125;</span><br><span class="line">[s]   request    &lt;GET http://blog.scrapinghub.com&gt;</span><br><span class="line">[s]   response   &lt;<span class="number">200</span> https://blog.scrapinghub.com/&gt;</span><br><span class="line">[s]   settings   &lt;scrapy.settings.Settings object at <span class="number">0x7f0638a2cb50</span>&gt;</span><br><span class="line">[s]   spider     &lt;DefaultSpider <span class="string">'default'</span> at <span class="number">0x7f06371f3290</span>&gt;</span><br><span class="line">[s] Useful shortcuts:</span><br><span class="line">[s]   shelp()           Shell help (<span class="keyword">print</span> this help)</span><br><span class="line">[s]   fetch(req_or_url) Fetch request (<span class="keyword">or</span> URL) <span class="keyword">and</span> update local objects</span><br><span class="line">[s]   view(response)    View response <span class="keyword">in</span> a browser</span><br><span class="line">&gt;&gt;&gt;</span><br></pre></td></tr></table></figure><p>加载它之后，你可以开始玩玩 response ，以构建选择器来提取所需的数据。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>response.css(<span class="string">"div.post-header &gt; h2 ::text"</span>).extract()</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>如果你不熟悉 Scrapy Shell，那么不妨试一试。它与你的开发工作流程可以完美契合，它位于在浏览器中进行页面检查的动作之后。你可以创建并测试爬虫的抽取规则，而一旦你构建了所需的规则，就可以在爬虫代码中使用它们。</p><p>通过官方文档，了解更多关于<a href="http://doc.scrapy.org/en/latest/topics/shell.html" target="_blank" rel="noopener"> Scrapy Shell的细节 </a>。</p><h3 id="从你的-Spider-代码中启动-Scrapy-Shell"><a href="#从你的-Spider-代码中启动-Scrapy-Shell" class="headerlink" title="从你的 Spider 代码中启动 Scrapy Shell"></a>从你的 Spider 代码中启动 Scrapy Shell</h3><p>如果对于某些响应，你的爬虫表现异常，那么在爬虫代码中使用 <code>scrapy.shell.inspect_response</code> 方法，你可以很快地看到发生了什么事。这将打开一个 Scrapy shell 会话，以让你与当前的 response 对象进行交互。</p><p>例如，假设你的爬虫不从某些页面中提取所期望数量的项，而你想要看看网站返回的响应有啥问题：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> scrapy.shell <span class="keyword">import</span> inspect_response</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">BlogSpider</span><span class="params">(scrapy.Spider)</span></span></span><br><span class="line"><span class="function">    ...</span></span><br><span class="line"><span class="function">    <span class="title">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> len(response.css(<span class="string">'div.post-header &gt; h2 ::text'</span>)) &gt; EXPECTED:</span><br><span class="line">            <span class="comment"># generate the items</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            inspect_response(response, self)</span><br><span class="line">        ...</span><br></pre></td></tr></table></figure><p>一旦执行这个 inspect_response 调用，Scrapy Shell 就会被打开，而你就能与 response 进行交互，从而看看发生了啥事。</p><h2 id="快速绑定一个调试器到你的-Spider"><a href="#快速绑定一个调试器到你的-Spider" class="headerlink" title="快速绑定一个调试器到你的 Spider"></a>快速绑定一个调试器到你的 Spider</h2><p>另一个调试爬虫的方法是使用常规的 Python 调试器，例如 pdb 或者 PuDB 。我使用<a href="https://pypi.python.org/pypi/pudb" target="_blank" rel="noopener"> PuDB </a>，因为它是一个相当强大且易于使用的调试器，而要激活它，我所需要的只是将这行代码放在我想要断点的那一行：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> pudb;</span><br><span class="line">pudb.set_trace()</span><br></pre></td></tr></table></figure><p>当到达断点的时候，PuDB 在你的终端中打开一个很酷的文本模式的用户界面，它将带你回到使用 Turbo Pascal 调试器的那些美好的旧时光。</p><p>看一看：<img src="https://dongfei.oss-cn-shanghai.aliyuncs.com/blog/image002.png" alt="image00"></p><p>你可以使用 pip 安装 PuDB ：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ pip install pudb</span><br></pre></td></tr></table></figure><p>看看这个视频，其中，我们自己的<a href="https://twitter.com/eliasdorneles" target="_blank" rel="noopener"> @eliasdorneles </a>演示了使用 PuDB 的几个小技巧：<a href="https://vimeo.com/166584837" target="_blank" rel="noopener">https://vimeo.com/166584837</a></p><h2 id="Scrapy-解析-CLI-命令"><a href="#Scrapy-解析-CLI-命令" class="headerlink" title="Scrapy 解析 CLI 命令"></a>Scrapy 解析 CLI 命令</h2><p>有些情况下，你需要你的爬虫很长一段时间运行某些爬取项目。但是，在运行了几个小时后，你可能会悲催地在日志中看到，对于一些特​​定的 URL ，爬虫之一有爬取问题。你想要调试爬虫，但你肯定不希望再运行整个抓取过程，并且要等到为该特定的URL调用的具体的回调，这样你就可以启动你的调试器。</p><p>别担心，Scrapy CLI 的<a href="http://doc.scrapy.org/en/latest/topics/commands.html#std:command-parse" target="_blank" rel="noopener"> parse命令 </a>就是为了让你节约时间的！你只需要提供该爬虫的名字，应该使用的爬虫的回调，以及你想要解析的URL：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ scrapy parse https://blog.scrapinghub.com/comments/bla --spider blog -c parse_comments</span><br></pre></td></tr></table></figure><p>在这种情况下，Scrapy 将会调用 blog 爬虫的 parse_comments 方法来解析 <code>blog.scrapinghub.com/comments/bla</code> URL。如果你不指定爬虫，那么 Scrapy 将会在你的项目中，基于爬虫的 allowed_domains 设置，搜寻能够处理这个 URL 的爬虫。</p><p>然后，它将会向你显示回调的执行摘要：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>STATUS DEPTH LEVEL <span class="number">1</span> &lt;&lt;&lt;</span><br><span class="line"><span class="comment"># Scraped Items  ------------------------------------------------------------</span></span><br><span class="line">[&#123;<span class="string">'comments'</span>: [</span><br><span class="line">    &#123;<span class="string">'content'</span>: <span class="string">u"I've seen this language ..."</span>,</span><br><span class="line">     <span class="string">'username'</span>: <span class="string">u'forthemostpart'</span>&#125;,</span><br><span class="line">    &#123;<span class="string">'content'</span>: <span class="string">u"It's a ..."</span>,</span><br><span class="line">     <span class="string">'username'</span>: <span class="string">u'YellowAfterlife'</span>&#125;,</span><br><span class="line">    ...</span><br><span class="line">    &#123;<span class="string">'content'</span>: <span class="string">u"There is a macro for ..."</span>,</span><br><span class="line">    <span class="string">'username'</span>: <span class="string">u'mrcdk'</span>&#125;]&#125;]</span><br><span class="line"><span class="comment"># Requests  -----------------------------------------------------------------</span></span><br><span class="line">[]</span><br></pre></td></tr></table></figure><p>你也可以在方法里面附加一个调试器，以帮助你弄清楚发生了什么（见前面的提示）。</p><h2 id="Scrapy-fetch-和-view-命令"><a href="#Scrapy-fetch-和-view-命令" class="headerlink" title="Scrapy fetch 和 view 命令"></a>Scrapy fetch 和 view 命令</h2><p>在浏览器中检查页面内容可能会被欺骗，因为它们的 JavaScript 引擎可能渲染某些 Scrapy 下载器不会做的内容。如果你想快速检查当一个页面被 Scrapy 下载后，该页面会看起来是什么样的，那么你可以使用下面这些命令：</p><ul><li><strong>fetch</strong>: 使用 Scrapy 下载器下载 HTML ，然后打印到标准输出。</li><li><strong>view</strong>: 使用 Scrapy 下载器下载 HTML ，然后用你的默认浏览器打开它。</li></ul><p><strong>例如</strong>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ scrapy fetch http://blog.scrapinghub.com &gt; blog.html</span><br><span class="line">$ scrapy view http://scrapy.org</span><br></pre></td></tr></table></figure><h2 id="使用-–pdb-选项，对爬虫进行事后剖析侦错"><a href="#使用-–pdb-选项，对爬虫进行事后剖析侦错" class="headerlink" title="使用 –pdb 选项，对爬虫进行事后剖析侦错"></a>使用 –pdb 选项，对爬虫进行事后剖析侦错</h2><p>编写防故障软件几乎是不可能的。这种情况对于网络爬虫更加糟糕，因为它们处理的网页内容是经常变化的（和损坏的）。最好接受我们的爬虫最后将会失败，并确保我们有工具来快速了解为什么它挂了，并能尽快解决这个问题。</p><p>Python 的回溯是棒棒哒，但在某些情况下，它们不向我们提供关于在我们的代码中发生了什么的足够信息。这就是事后剖析侦错的用武之地。Scrapy 提供了–  <code>--pdb</code> 命令行选项，它在你的爬虫挂掉的地方打开一个 pdb 会话，这样你就可以检查它的上下文，从而明白发生了什么：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ scrapy crawl blog -o blog_items.jl --pdb</span><br></pre></td></tr></table></figure><p>如果你的爬虫由于致命异常而挂了，那么 pdb 调试器将会打开，这样你就可以仔细检查其死因。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>好啦，这就是Scrapy技巧的五月版。在<a href="http://scrapy.readthedocs.io/en/latest/topics/debug.html #debugging-spiders" target="_blank" rel="noopener"> Scrapy官方文档 </a>，你也可以看到其中一些调试技巧。</p><p>因为这里，我们是要帮助你更有效地爬取网页的，所以<a href="https://twitter.com/ScrapingHub" target="_blank" rel="noopener"> 请让我们知道 </a>你希望在将来看到什么。那就下个月再见啦！</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;欢迎来到 Scrapy 技巧系列！每个月，我们会发布一些技巧和 hack ，来帮助你加快网页抓取和数据提取。作为牵头的 Scrapy 维护者，你可以想象的任何障碍我们都遇到过了，所以别担心，你能在这获益良多。随意到&lt;a href=&quot;https://twitter.com/ScrapingHub&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt; Twitter &lt;/a&gt;或者&lt;a href=&quot;https://www.facebook.com/ScrapingHub/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt; Facebook &lt;/a&gt;访问我们，提出对未来主题的建议吧。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://dongfei.oss-cn-shanghai.aliyuncs.com/blog/scrapy-tips.png&quot; alt=&quot;Scrapy Tips&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Python" scheme="http://blog.dongfei.xin/categories/Python/"/>
    
      <category term="Scrapy" scheme="http://blog.dongfei.xin/categories/Python/Scrapy/"/>
    
    
      <category term="Scrapy" scheme="http://blog.dongfei.xin/tags/Scrapy/"/>
    
  </entry>
  
  <entry>
    <title>Scrapinghub 的 Scrapy 技巧系列（三）</title>
    <link href="http://blog.dongfei.xin/2018-04-12/Scrapinghub-%E7%9A%84-Scrapy-%E6%8A%80%E5%B7%A7%E7%B3%BB%E5%88%97%EF%BC%88%E4%B8%89%EF%BC%89/"/>
    <id>http://blog.dongfei.xin/2018-04-12/Scrapinghub-的-Scrapy-技巧系列（三）/</id>
    <published>2018-04-11T16:25:48.000Z</published>
    <updated>2018-04-12T11:58:01.437Z</updated>
    
    <content type="html"><![CDATA[<p>原文；<a href="https://blog.scrapinghub.com/2016/04/20/scrapy-tips-from-the-pros-april-2016-edition/" target="_blank" rel="noopener"> Scrapy Tips from the Pros: April 2016 Edition </a></p><hr><p><img src="https://dongfei.oss-cn-shanghai.aliyuncs.com/blog/scrapy-tips.png" alt="Scrapy Tips"></p><p>欢迎来到四月版本的<a href="https://blog.scrapinghub.com/category/scrapy-tips-from-the-pros/" target="_blank" rel="noopener"> Scrapy技巧 </a>。每个月我们都会发布一些我们发现的技巧和 hack ，以帮助你的 Scrapy 工作流更加顺利。</p><p>这个月，我们只给你带来了一个提示，但是，不是这样的哦！所以，如果你发现你在爬取一个需要通过表单提交数据的 ASP.Net 页面，那么，就回来看看这篇文章吧。</p><a id="more"></a><h1 id="处理-ASP-Net-页面，-PostBack-和视图状态"><a href="#处理-ASP-Net-页面，-PostBack-和视图状态" class="headerlink" title="处理 ASP.Net 页面， PostBack 和视图状态"></a>处理 ASP.Net 页面， PostBack 和视图状态</h1><p>使用 ASP.Net 技术构建的网站对于 web 爬虫开发者来说通常是一场噩梦，这主要是由于它们处理表单的方式。</p><p>这类网站通常在请求和响应中发送状态，以便跟踪客户端的 UI 状态。想想那些你浏览许多页面，在 HTML 表单中填写你的数据来注册的网站吧。一个 ASP.Net 网站通常存储那些在前一个页面填写的数据到一个名为 <code>__VIEWSTATE</code> 的隐藏字段中，这个字段包含了像下面显示的一个巨大的字符串：</p><p><a href="https://dongfei.oss-cn-shanghai.aliyuncs.com/blog/image032.png" target="_blank" rel="noopener"><img src="https://scrapinghub.files.wordpress.com/2016/04/image032.png?w=648&amp;h=408" alt="ViewState example"></a></p><p><em>我不是在开玩笑，它真的很大！ (有时是数十kB)</em></p><p>这是一个 Base64 编码字符串，它表示客户端 UI 状态，包括来自表单的值。这在表单中的用户动作触发 POST 请求返回给服务器以获取其他字段的数据的 web 应用中，这种设置尤为常见。</p><p>每次浏览器向服务器发起 POST 请求时，就会带着这个 <code>__VIEWSTATE</code> 字段。然后，服务器根据该数据解码并加载客户端的 UI 状态，执行一些处理，基于新值为新的视图状态计算值，然后将这个新的视图状态作为隐藏字段渲染结果页面。</p><p>如果 <code>__VIEWSTATE</code> 没有发回给服务器，那么你可能会看到一个空白表单，因为服务器完全失去了客户端 UI 状态。所以，为了爬取像这样的根据表单生成的页面，你必须确保你的爬虫在它发送的请求中带有这个状态，否则，页面将不会加载它应该加载的内容。</p><p>这里有一个具体的例子，你可以亲眼看到如何处理这类情况。</p><h1 id="抓取一个基于视图状态的网站"><a href="#抓取一个基于视图状态的网站" class="headerlink" title="抓取一个基于视图状态的网站"></a>抓取一个基于视图状态的网站</h1><p>今天抓取的小白鼠是<a href="http://spidyquotes.herokuapp.com/search.aspx" target="_blank" rel="noopener"> spidyquotes.herokuapp.com/search.aspx </a>。 SpidyQuotes 列出了来自名人的引言，而它的搜索页面允许你根据作者和标签过滤引言：</p><p><a href="https://scrapinghub.files.wordpress.com/2016/04/image052.png" target="_blank" rel="noopener"><img src="https://scrapinghub.files.wordpress.com/2016/04/image052.png?w=300&amp;h=246" alt="image05"></a></p><p><strong>Author</strong> 字段的改变触发了一个到服务器的 POST 请求，以使用与所选的用户相关的标签来填充 <strong>Tag</strong>选择框。点击 <strong>Search</strong>，显示与所选作者的标签相对应的引言：</p><p><a href="https://dongfei.oss-cn-shanghai.aliyuncs.com/blog/image041.png" target="_blank" rel="noopener"><img src="https://scrapinghub.files.wordpress.com/2016/04/image041.png?w=295&amp;h=300" alt="image04"></a></p><p>为了爬取这些引言，我们的爬虫必须模拟用户选择一个作者，一个标签并提交表单。通过使用<a href="https://developer.chrome.com/devtools" target="_blank" rel="noopener"> Network Panel </a>（你可以通过浏览器的开发者工具访问）来仔细看看这个流程的每一步。首先，访问<a href="http://spidyquotes.herokuapp.com/search.aspx" target="_blank" rel="noopener"> spidyquotes.herokuapp.com/search.aspx </a>，然后按下 F12 或 Ctrl+Shift+I (如果你使用的是 Chrome )来加载工具，接着点击 Network 选项卡。</p><p><a href="https://dongfei.oss-cn-shanghai.aliyuncs.com/blog/image001.png" target="_blank" rel="noopener"><img src="https://scrapinghub.files.wordpress.com/2016/04/image001.png?w=648&amp;h=430" alt="image00"></a></p><p>从列表中选择一个作者，然后你将看到生成了一个发往 <code>/filter.aspx</code>的请求。点击资源名 (filter.aspx) ，你就可以看到请求细节，其中包括你选择的作者，以及在来自于服务器的原始响应中的 <code>__VIEWSTATE</code> 数据。</p><p><a href="https://dongfei.oss-cn-shanghai.aliyuncs.com/blog/image022.png" target="_blank" rel="noopener"><img src="https://scrapinghub.files.wordpress.com/2016/04/image022.png?w=648&amp;h=209" alt="image02"></a></p><p>选择一个标签并点击 Search 。你会看到你的浏览器发送了在表单中选择的值，以及一个与前面不同的 <code>__VIEWSTATE</code> 值。这是因为，当你选择作者时，服务器包含了一些新的信息在视图状态中。</p><p><a href="https://dongfei.oss-cn-shanghai.aliyuncs.com/blog/image011.png" target="_blank" rel="noopener"><img src="https://scrapinghub.files.wordpress.com/2016/04/image011.png?w=648&amp;h=234" alt="image01"></a></p><p>现在，你只需要构建一个爬虫，这个爬虫完成与你的浏览器做的事情。</p><h1 id="构建爬虫"><a href="#构建爬虫" class="headerlink" title="构建爬虫"></a>构建爬虫</h1><p>这里是你的爬虫应该遵循的步骤：</p><ol><li>抽取 <code>spidyquotes.herokuapp.com/filter.aspx</code></li><li><p>对于每一个在表单作者列表中找到的 <strong>Author</strong>：</p><ul><li>创建一个到 <code>/filter.aspx</code>的 POST 请求，同时传递选择的 <strong>Author</strong> 和 <strong>__VIEWSTATE</strong> 值</li></ul></li><li><p>对于在结果页面中找到的每一个 <strong>Tag</strong>：</p><ul><li>发送一个到<code>/filter.aspx</code>的 POST 请求，同时传递选择的 <strong>Author</strong>，选择的 <strong>Tag</strong> 和视图状态</li></ul></li><li><p>抓取结果页面</p></li></ol><h2 id="爬虫编码"><a href="#爬虫编码" class="headerlink" title="爬虫编码"></a>爬虫编码</h2><p>这里是我开发的从该网站抓取引言的爬虫，遵循了刚刚描述的步骤：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SpidyQuotesViewStateSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">'spidyquotes-viewstate'</span></span><br><span class="line">    start_urls = [<span class="string">'http://spidyquotes.herokuapp.com/search.aspx'</span>]</span><br><span class="line">    download_delay = <span class="number">1.5</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> author <span class="keyword">in</span> response.css(<span class="string">'select#author &gt; option ::attr(value)'</span>).extract():</span><br><span class="line">            <span class="keyword">yield</span> scrapy.FormRequest(</span><br><span class="line">                <span class="string">'http://spidyquotes.herokuapp.com/filter.aspx'</span>,</span><br><span class="line">                formdata=&#123;</span><br><span class="line">                    <span class="string">'author'</span>: author,</span><br><span class="line">                    <span class="string">'__VIEWSTATE'</span>: response.css(<span class="string">'input#__VIEWSTATE::attr(value)'</span>).extract_first()</span><br><span class="line">                &#125;,</span><br><span class="line">                callback=self.parse_tags</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_tags</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> tag <span class="keyword">in</span> response.css(<span class="string">'select#tag &gt; option ::attr(value)'</span>).extract():</span><br><span class="line">            <span class="keyword">yield</span> scrapy.FormRequest(</span><br><span class="line">                <span class="string">'http://spidyquotes.herokuapp.com/filter.aspx'</span>,</span><br><span class="line">                formdata=&#123;</span><br><span class="line">                    <span class="string">'author'</span>: response.css(</span><br><span class="line">                        <span class="string">'select#author &gt; option[selected] ::attr(value)'</span></span><br><span class="line">                    ).extract_first(),</span><br><span class="line">                    <span class="string">'tag'</span>: tag,</span><br><span class="line">                    <span class="string">'__VIEWSTATE'</span>: response.css(<span class="string">'input#__VIEWSTATE::attr(value)'</span>).extract_first()</span><br><span class="line">                &#125;,</span><br><span class="line">                callback=self.parse_results,</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_results</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> quote <span class="keyword">in</span> response.css(<span class="string">"div.quote"</span>):</span><br><span class="line">            <span class="keyword">yield</span> &#123;</span><br><span class="line">                <span class="string">'quote'</span>: response.css(<span class="string">'span.content ::text'</span>).extract_first(),</span><br><span class="line">                <span class="string">'author'</span>: response.css(<span class="string">'span.author ::text'</span>).extract_first(),</span><br><span class="line">                <span class="string">'tag'</span>: response.css(<span class="string">'span.tag ::text'</span>).extract_first(),</span><br><span class="line">            &#125;</span><br></pre></td></tr></table></figure></p><p><strong>步骤1</strong>由 Scrapy 完成，它读取 start_urls ，然后生成一个到 <code>/search.aspx</code>的 GET 请求。</p><p>parse() 方法负责 <strong>步骤2</strong>。它遍历了在第一个选择框中找到的 <strong>Authors</strong>，然后为每一个 <strong>Author</strong>创建一个到 <code>/filter.aspx</code> 的<a href="http://doc.scrapy.org/en/latest/topics/request-response.html#formrequest-objects" target="_blank" rel="noopener"> FormRequest </a>，模拟用户点击了列表中的每一个元素。值得注意的是，parse() 方法从它所收到的表单中读取__VIEWSTATE字段，然后将其传回给服务器，所以服务器可以跟踪我们位于哪个页面流。</p><p><strong>步骤3</strong>由 parse_tags() 方法来处理。它与 parse() 方法非常类似，因为它提取了所列的 <strong>Tags</strong> ，然后创建 POST 请求来传递每一个 <strong>Tag</strong> ，在前一个步骤中选择的 <strong>Author</strong> 以及从服务器收到的 __VIEWSTATE。</p><p>最后，在 <strong>步骤4</strong>中，parse_results()方法解析页面展示的引言列表，然后从中生成项。</p><h2 id="使用-FormRequest-from-response-简化你的爬虫"><a href="#使用-FormRequest-from-response-简化你的爬虫" class="headerlink" title="使用 FormRequest.from_response() 简化你的爬虫"></a>使用 FormRequest.from_response() 简化你的爬虫</h2><p>你也许注意到，在发送 POST 请求到服务器之前，我们的爬虫抽取了那些它从服务器收到的表单中的预填值，并在它将创建的请求中包含了这些值。</p><p>我们不需要对其手工编码，因为<a href="http://scrapy.org/" target="_blank" rel="noopener"> Scrapy </a>提供了<a href="http://doc.scrapy.org/en/latest/topics/request-response.html#scrapy.http.FormRequest.from_response" target="_blank" rel="noopener"> FormRequest.from_response() </a>方法。该方法读取 response 对象，创建一个 <code>FormRequest</code>，它自动包含表单所有的预填值以及隐藏值。这是我们的爬虫的 parse_tags() 方法：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_tags</span><span class="params">(self, response)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> tag <span class="keyword">in</span> response.css(<span class="string">'select#tag &gt; option ::attr(value)'</span>).extract():</span><br><span class="line">        <span class="keyword">yield</span> scrapy.FormRequest.from_response(</span><br><span class="line">            response,</span><br><span class="line">            formdata=&#123;<span class="string">'tag'</span>: tag&#125;,</span><br><span class="line">            callback=self.parse_results,</span><br><span class="line">        )</span><br></pre></td></tr></table></figure></p><p>所以，无论何时你处理包含隐藏值和预填值的表单，使用 <code>from_response</code> 方法，因为这样你的代码会看起来干净得多。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>好了，这就是这个月的技巧了。你可以<a href="http://msdn.microsoft.com/en-us/library/ms972976.aspx" target="_blank" rel="noopener"> 在这里读取更多关于ViewStates </a>的信息。我们希望你觉得这个技巧有用，并且很高兴看到你用它来做点什么。我们一直在寻找新的hack，所以如果你在爬取web的时候遇到了什么困难，请告诉我们。</p><p>随意在<a href="https://twitter.com/scrapinghub" target="_blank" rel="noopener"> Twitter </a>或者<a href="https://www.facebook.com/ScrapingHub/" target="_blank" rel="noopener"> Facebook </a>上告诉我们，你未来想看到什么吧。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;原文；&lt;a href=&quot;https://blog.scrapinghub.com/2016/04/20/scrapy-tips-from-the-pros-april-2016-edition/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt; Scrapy Tips from the Pros: April 2016 Edition &lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;img src=&quot;https://dongfei.oss-cn-shanghai.aliyuncs.com/blog/scrapy-tips.png&quot; alt=&quot;Scrapy Tips&quot;&gt;&lt;/p&gt;
&lt;p&gt;欢迎来到四月版本的&lt;a href=&quot;https://blog.scrapinghub.com/category/scrapy-tips-from-the-pros/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt; Scrapy技巧 &lt;/a&gt;。每个月我们都会发布一些我们发现的技巧和 hack ，以帮助你的 Scrapy 工作流更加顺利。&lt;/p&gt;
&lt;p&gt;这个月，我们只给你带来了一个提示，但是，不是这样的哦！所以，如果你发现你在爬取一个需要通过表单提交数据的 ASP.Net 页面，那么，就回来看看这篇文章吧。&lt;/p&gt;
    
    </summary>
    
      <category term="Python" scheme="http://blog.dongfei.xin/categories/Python/"/>
    
      <category term="Scrapy" scheme="http://blog.dongfei.xin/categories/Python/Scrapy/"/>
    
    
      <category term="Scrapy" scheme="http://blog.dongfei.xin/tags/Scrapy/"/>
    
  </entry>
  
  <entry>
    <title>Scrapinghub 的 Scrapy 技巧系列（二）</title>
    <link href="http://blog.dongfei.xin/2018-04-12/Scrapinghub-%E7%9A%84-Scrapy-%E6%8A%80%E5%B7%A7%E7%B3%BB%E5%88%97%EF%BC%88%E4%BA%8C%EF%BC%89/"/>
    <id>http://blog.dongfei.xin/2018-04-12/Scrapinghub-的-Scrapy-技巧系列（二）/</id>
    <published>2018-04-11T16:02:45.000Z</published>
    <updated>2018-04-12T11:57:58.354Z</updated>
    
    <content type="html"><![CDATA[<p>原文：<a href="https://blog.scrapinghub.com/2016/03/23/scrapy-tips-from-the-pros-march-2016-edition/" target="_blank" rel="noopener">Scrapy Tips from the Pros: March 2016 Edition</a></p><hr><p><img src="https://dongfei.oss-cn-shanghai.aliyuncs.com/blog/scrapy-tips-march-2016.png" alt="Scrapy-Tips-March-2016"></p><p>欢迎来到三月份版本的<a href="https://blog.scrapinghub.com/category/scrapy-tips-from-the-pros/" target="_blank" rel="noopener"> Scrapy 技巧 </a>! 每个月，我们都会发布一些我们开发的技巧和 hack，来帮助你，使得你的 Scrapy 工作流更顺畅。</p><p>这个月，我们将涵盖如何和 CookiesMiddleware 一起使用<a href="http://doc.scrapy.org/en/latest/topics/downloader-middleware.html#std:reqmeta-cookiejar" target="_blank" rel="noopener"> cookiejar </a>来绕过那些不允许你使用相同的 cookie 同时爬取多个页面的网站。我们还将分享一个一个好用的技巧，这个技巧关于如何和<a href="http://doc.scrapy.org/en/latest/topics/loaders.html" target="_blank" rel="noopener"> item loader </a>一起使用多个备用的 XPath/CSS 表达式，来从网站上更可靠地获取数据。</p><a id="more"></a><p><strong>学生请阅读以下：我们正参与<a href="https://blog.scrapinghub.com/2016/03/14/join-scrapinghub-for-google-summer-of-code-2016/" target="_blank" rel="noopener"> 2016年Google编程之夏 </a>，而我们一部分的项目点子使用了Scrapy! 如果你感兴趣，那么看一看<a href="http://gsoc2016.scrapinghub.com/ideas/" target="_blank" rel="noopener"> 我们的点子 </a>，并记得<a href="https://wiki.python.org/moin/SummerOfCode/2016#How_do_I_Apply.3F" target="_blank" rel="noopener"> 在3.25，也就是周五之前申请 </a>!</strong></p><p><strong>如果你不是学生，那么请与你的学生朋友分享。他们会获得一份夏天津贴，甚至最后我们可能聘用他们。</strong></p><h1 id="使用-CookieJar-解决站点怪异会话行为"><a href="#使用-CookieJar-解决站点怪异会话行为" class="headerlink" title="使用 CookieJar 解决站点怪异会话行为"></a>使用 CookieJar 解决站点怪异会话行为</h1><p>那些将你的 UI 状态存储在自己的服务器的会话中的网站是难以导航的，更别说抓取。你有没有遇到过那些在同一个网站上打开的一个选项卡会影响其他选项卡的网站？那么，你可能会碰到这个问题。</p><p>虽然这是令人沮丧的，它甚至对于网络爬虫更糟糕。它会严重阻碍网络爬虫会话。不幸的是，这是 ASP.Net 和基于 J2EE 的网站的通用模式。而这正是<a href="http://doc.scrapy.org/en/latest/topics/downloader-middleware.html#std:reqmeta-cookiejar" target="_blank" rel="noopener"> cookiejars </a>的用处所在。虽然不是经常需要 cookiejar ，但是对于那些意想不到的情况，你会很高兴拥有它。</p><p>当你的爬取一个网站时，Scrapy 自动为你处理 cookie ，存储并在随后的请求到将其发送到同一站点。但是，正如你可能知道的， Scrapy 请求是异步的。这意味着，你可能有发到相同的网站上的多个请求被同时处理，同时共享相同的 cookie 。为避免在爬取这些类型的网站时，请求相互影响，你必须为不同的请求设置不同的 cookie 。</p><p>您可以通过使用一个<a href="http://doc.scrapy.org/en/latest/topics/downloader-middleware.html#std:reqmeta-cookiejar" target="_blank" rel="noopener"> cookiejar </a>为同一网站中的不同页面存储单独的 cookie 来做到这点。该 cookiejar 只是在 Scrapy 爬取会话期间保持的一个 cookie 键值集合。你只需要为每个你想要存储的 cookie 定义一个唯一标识符，然后当你想要使用特定的 cookie 时，使用它的标识符。</p><p>例如，假设你想抓取一个网站上的多个类别，但这个网站存储与你在服务器会话中爬行/浏览的类别相关的数据。要同时爬取这些类别，则需要通过将类别名称作为 cookiejar 元参数的标识符来为每个类别创建一个 cookie ：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ExampleSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    urls = [</span><br><span class="line">        <span class="string">'http://www.example.com/category/photo'</span>,</span><br><span class="line">        <span class="string">'http://www.example.com/category/videogames'</span>,</span><br><span class="line">        <span class="string">'http://www.example.com/category/tablets'</span></span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">start_requests</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> url <span class="keyword">in</span> urls:</span><br><span class="line">            category = url.split(<span class="string">'/'</span>)[<span class="number">-1</span>]</span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(url, meta=&#123;<span class="string">'cookiejar'</span>: category&#125;)</span><br></pre></td></tr></table></figure></p><p>在此情况下，将管理三种不同的 Cookie（<code>photo</code>、 <code>videogames</code> 和 <code>tablets</code>）。每当你传递一个不存在的键作为 cookiejar 元值（例如，当一个类别名称尚未访问）时，你可以创建一个新的 Cookie 。当我们传递的键已经存在时，<a href="http://scrapy.org/" target="_blank" rel="noopener"> Scrapy </a>使用该请求相应的 cookie 。</p><p>所以，例如，如果你想重新使用已被用来抓取 <code>videogames</code>页面的 cookie，那么你只需要将 <code>videogames</code> 作为唯一键传递给 cookiejar。它将使用先用的 cookie，而不是创建一个新的 cookie：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">yield</span> scrapy.Request(<span class="string">'http://www.example.com/atari2600'</span>, meta=&#123;<span class="string">'cookiejar'</span>: <span class="string">'videogames'</span>&#125;)</span><br></pre></td></tr></table></figure></p><h1 id="添加备用的-CSS-XPath-规则"><a href="#添加备用的-CSS-XPath-规则" class="headerlink" title="添加备用的 CSS/XPath 规则"></a>添加备用的 CSS/XPath 规则</h1><p>当你需要完成比简单地填充字典或带有你的 spider 收集的数据的 Item 对象更多的东西时，<a href="http://doc.scrapy.org/en/latest/topics/loaders.html" target="_blank" rel="noopener"> Item Loader </a>是有用的。例如，你可能需要将一些后处理逻辑添加到你刚刚收集的数据中。你可能对某些如将标题中的每个单词首字母大写一样简单的事，甚至是更复杂的操作有兴趣。使用 ItemLoader ，你可以从 spider 中解耦这种后处理逻辑，以便拥有一个更易于维护的设计。</p><p>这个技巧说明如何将额外的功能添加到一个 Item Loader 中。比方说，你正爬取 Amazon.com ，并且提取每个产品的价格。你可以使用 Item Loader 来为 ProductItem 对象填充产品数据：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ProductItem</span><span class="params">(scrapy.Item)</span>:</span></span><br><span class="line">    name = scrapy.Field()</span><br><span class="line">    url = scrapy.Field()</span><br><span class="line">    price = scrapy.Field()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AmazonSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">"amazon"</span></span><br><span class="line">    allowed_domains = [<span class="string">"amazon.com"</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">start_requests</span><span class="params">(self)</span>:</span></span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_product</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        loader = ItemLoader(item=ProductItem(), response=response)</span><br><span class="line">        loader.add_css(<span class="string">'price'</span>, <span class="string">'#priceblock_ourprice ::text'</span>)</span><br><span class="line">        loader.add_css(<span class="string">'name'</span>, <span class="string">'#productTitle ::text'</span>)</span><br><span class="line">        loader.add_value(<span class="string">'url'</span>, response.url)</span><br><span class="line">        <span class="keyword">yield</span> loader.load_item()</span><br></pre></td></tr></table></figure><p>这种方法工作得很好，除非被爬取的产品是一次交易。这是因为对比那些普通的价格， Amazon 以一种稍微不同的格式展示交易价格。而普通产品的价格是这样表示的：</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">span</span> <span class="attr">id</span>=<span class="string">"priceblock_ourprice"</span> <span class="attr">class</span>=<span class="string">"a-size-medium a-color-price"</span>&gt;</span></span><br><span class="line">    $699.99</span><br><span class="line"><span class="tag">&lt;/<span class="name">span</span>&gt;</span></span><br></pre></td></tr></table></figure><p>交易价格显示稍微有点不同：<br><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">span</span> <span class="attr">id</span>=<span class="string">"priceblock_dealprice"</span> <span class="attr">class</span>=<span class="string">"a-size-medium a-color-price"</span>&gt;</span></span><br><span class="line">    $649.99</span><br><span class="line"><span class="tag">&lt;/<span class="name">span</span>&gt;</span></span><br></pre></td></tr></table></figure></p><p>要处理这种情况的一个好方法是，为 Item loader 中的价格字段添加一个后备规则。这是一个只有当该字段的前一规则已经失败时才应用的规则。要用 Item Loader 做到这一点，你可以添加一个 <code>add_fallback_css</code> 方法：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AmazonItemLoader</span><span class="params">(ItemLoader)</span>:</span></span><br><span class="line">    default_output_processor = TakeFirst()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_collected_values</span><span class="params">(self, field_name)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> (self._values[field_name] <span class="keyword">if</span> field_name <span class="keyword">in</span> self._values <span class="keyword">else</span> self._values.default_factory())</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add_fallback_css</span><span class="params">(self, field_name, css, *processors, **kw)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> any(self.get_collected_values(field_name)):</span><br><span class="line">            self.add_css(field_name, css, *processors, **kw)</span><br></pre></td></tr></table></figure></p><p>正如你所看到的， 如果对于该字段，没有之前收集到的值，那么 <code>add_fallback_css</code> 方法将使用 CSS 规则。现在，我们可以改变我们的 spider 来使用 AmazonItemLoader ，然后添加后备 CSS 规则到我们的 loader 中：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_product</span><span class="params">(self, response)</span>:</span></span><br><span class="line">    loader = AmazonItemLoader(item=ProductItem(), response=response)</span><br><span class="line">    loader.add_css(<span class="string">'price'</span>, <span class="string">'#priceblock_ourprice ::text'</span>)</span><br><span class="line">    loader.add_fallback_css(<span class="string">'price'</span>, <span class="string">'#priceblock_dealprice ::text'</span>)</span><br><span class="line">    loader.add_css(<span class="string">'name'</span>, <span class="string">'#productTitle ::text'</span>)</span><br><span class="line">    loader.add_value(<span class="string">'url'</span>, response.url)</span><br><span class="line">    <span class="keyword">yield</span> loader.load_item()</span><br></pre></td></tr></table></figure></p><p>这个技巧可以节省你的时间，让你的 spider 更健壮。如果有一个 CSS 规则无法获取数据，那么可以应用其他跪在来提取所需的数据。</p><p>如果Item Loader对于你来说是新玩意，那么<a href="http://doc.scrapy.org/en/latest/topics/loaders.html" target="_blank" rel="noopener"> 看看这个文档 </a>。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>就是这样了！请跟我们分享你在网络抓取以及提取数据时碰到的任何问题。我们一直在寻找新的技巧和 hack ，并且在我们的每月专栏上分享我们的 Scrapy 技巧。在<a href="https://twitter.com/ScrapingHub" target="_blank" rel="noopener"> Twitter </a>或者<a href="https://www.facebook.com/ScrapingHub/" target="_blank" rel="noopener"> Facebook  </a>上联系我们，并且让我们知道我们是否帮到了你。</p><p>如果你还没有，试试<a href="http://doc.scrapinghub.com/portia.html" target="_blank" rel="noopener"> Portia </a>，我们的开源可视化 Web 抓取工具。我们知道你喜欢<a href="http://scrapy.org/" target="_blank" rel="noopener"> Scrapy </a>，但是体验从来就不是一种令人痛苦的事 ;)</p><p>请<a href="http://gsoc2016.scrapinghub.com/ideas/" target="_blank" rel="noopener"> 申请加入我们的2016年Google编程之夏 </a>，截止如期是 3.25，周五！</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;原文：&lt;a href=&quot;https://blog.scrapinghub.com/2016/03/23/scrapy-tips-from-the-pros-march-2016-edition/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Scrapy Tips from the Pros: March 2016 Edition&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;img src=&quot;https://dongfei.oss-cn-shanghai.aliyuncs.com/blog/scrapy-tips-march-2016.png&quot; alt=&quot;Scrapy-Tips-March-2016&quot;&gt;&lt;/p&gt;
&lt;p&gt;欢迎来到三月份版本的&lt;a href=&quot;https://blog.scrapinghub.com/category/scrapy-tips-from-the-pros/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt; Scrapy 技巧 &lt;/a&gt;! 每个月，我们都会发布一些我们开发的技巧和 hack，来帮助你，使得你的 Scrapy 工作流更顺畅。&lt;/p&gt;
&lt;p&gt;这个月，我们将涵盖如何和 CookiesMiddleware 一起使用&lt;a href=&quot;http://doc.scrapy.org/en/latest/topics/downloader-middleware.html#std:reqmeta-cookiejar&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt; cookiejar &lt;/a&gt;来绕过那些不允许你使用相同的 cookie 同时爬取多个页面的网站。我们还将分享一个一个好用的技巧，这个技巧关于如何和&lt;a href=&quot;http://doc.scrapy.org/en/latest/topics/loaders.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt; item loader &lt;/a&gt;一起使用多个备用的 XPath/CSS 表达式，来从网站上更可靠地获取数据。&lt;/p&gt;
    
    </summary>
    
      <category term="Python" scheme="http://blog.dongfei.xin/categories/Python/"/>
    
      <category term="Scrapy" scheme="http://blog.dongfei.xin/categories/Python/Scrapy/"/>
    
    
      <category term="Scrapy" scheme="http://blog.dongfei.xin/tags/Scrapy/"/>
    
  </entry>
  
  <entry>
    <title>Scrapinghub 的 Scrapy 技巧系列（一）</title>
    <link href="http://blog.dongfei.xin/2018-04-11/Scrapinghub-%E7%9A%84-Scrapy-%E6%8A%80%E5%B7%A7%E7%B3%BB%E5%88%97%EF%BC%88%E4%B8%80%EF%BC%89/"/>
    <id>http://blog.dongfei.xin/2018-04-11/Scrapinghub-的-Scrapy-技巧系列（一）/</id>
    <published>2018-04-11T15:22:14.000Z</published>
    <updated>2018-04-12T12:10:32.643Z</updated>
    
    <content type="html"><![CDATA[<p>原文：<a href="http://blog.scrapinghub.com/2016/01/19/scrapy-tips-from-the-pros-part-1/" target="_blank" rel="noopener"> 跟着高手学习Scrapy技巧：第一部分 </a></p><hr><p><a href="http://scrapy.org" target="_blank" rel="noopener">Scrapy </a> 是<a href="http://scrapinghub.com" target="_blank" rel="noopener"> Scrapinghub </a>  的关键部分。我们广泛地采用此框架，并已积累了许多各种不同的快捷方法来解决常见问题。我们推出了一个系列来与大家分享这些 Scrapy 的技巧，这样，你就可以在你的日常工作流程中最有效的使用它。每一个博文将给出两到三个提示，敬请关注。</p><p><img src="https://dongfei.oss-cn-shanghai.aliyuncs.com/blog/scrapylogo.png" alt="scrapylogo"></p><a id="more"></a><h2 id="使用-Extruct-从网站中提取微观数据-Microdata"><a href="#使用-Extruct-从网站中提取微观数据-Microdata" class="headerlink" title="使用 Extruct 从网站中提取微观数据 ( Microdata )"></a>使用 Extruct 从网站中提取微观数据 ( Microdata )</h2><p>我相信网络爬虫的每一个开发者都会有理由来咒骂那些对他们的网站使用凌乱的布局的 Web 开发者。没有语义标记的网站，特别是那些基于 HTML 表格的网站，绝对是槽糕透顶。这些类型的网站使得爬取更加困难，因为几乎没有关于每一个元素代表什么的提示。有时候，你甚至不得不相信每个页面上的元素顺序将保持不变，从而抓取你需要的数据。</p><p>这就是为什么我们如此感激<a href="https://schema.org/" target="_blank" rel="noopener"> Schema.org </a> ，共同努力来使得语义标记在网页上。该项目为 Web 开发者提供了在他们的网站上展示一定范围的不同对象（包括 Person、 Product 和 Review）的架构，并使用例如<a href="http://www.w3.org/TR/microdata/" target="_blank" rel="noopener"> Microdata </a>，<a href="https://rdfa.info/" target="_blank" rel="noopener"> RDFa </a> ，<a href="http://json-ld.org/" target="_blank" rel="noopener"> JSON-LD </a> 等的任何元数据格式。这使得搜索引擎工作更加容易，因为它们可以从网站上提取有用信息，而不必深入到他们所抓取网站的 HTML 结构中。</p><p>例如，<a href="https://schema.org/AggregateRating" target="_blank" rel="noopener"> AggregateRating </a> 是网上零售商用来展示他们产品的用户评级的架构。下面是描述一个使用<a href="http://www.w3.org/TR/microdata/" target="_blank" rel="noopener"> Microdata format </a> 的网上商店中的一个产品的用户评级的标记：</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">itemprop</span>=<span class="string">"aggregateRating"</span> <span class="attr">itemscope</span>=<span class="string">""</span> <span class="attr">itemtype</span>=<span class="string">"http://schema.org/AggregateRating"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">meta</span> <span class="attr">itemprop</span>=<span class="string">"worstRating"</span> <span class="attr">content</span>=<span class="string">"1"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">meta</span> <span class="attr">itemprop</span>=<span class="string">"bestRating"</span> <span class="attr">content</span>=<span class="string">"5"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"bbystars-small-yellow"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"fill"</span> <span class="attr">style</span>=<span class="string">"width: 88%"</span>&gt;</span><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">span</span> <span class="attr">itemprop</span>=<span class="string">"ratingValue"</span> <span class="attr">aria-label</span>=<span class="string">"4.4 out of 5 stars"</span>&gt;</span>4.4<span class="tag">&lt;/<span class="name">span</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">meta</span> <span class="attr">itemprop</span>=<span class="string">"reviewCount"</span> <span class="attr">content</span>=<span class="string">"305733"</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br></pre></td></tr></table></figure><p>通过这种方式，搜索引擎可以在搜索结果中同时展示一个产品的评级及其 URL ，而不需要为每一个网站编写特定的爬虫：</p><p><img src="https://dongfei.oss-cn-shanghai.aliyuncs.com/blog/selection_096.png" alt="Example of a google search showing ratings for a product "></p><p>你还可以受益于一些网站使用的语义标记。我们推荐使用<a href="https://github.com/scrapinghub/extruct" target="_blank" rel="noopener"> Extruct </a>，一个从 HTML 文档中中提取 <a href="http://blog.scrapinghub.com/2014/06/18/extracting-schema-org-microdata-using-scrapy-selectors-and-xpath/" target="_blank" rel="noopener"> 嵌入式元数据 </a>  的库。它分析整个 HTML 并返回一个包含微观数据（ microdata ）的 Python 字典。看看我们是如何用它来提取展示用户评级的微观数据的：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> extruct.w3cmicrodata <span class="keyword">import</span> MicrodataExtractor</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>mde = MicrodataExtractor()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>data = mde.extract(html_content)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>data</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">'items'</span>: [</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="string">'type'</span>: <span class="string">'http://schema.org/AggregateRating'</span>,</span><br><span class="line">      <span class="string">'properties'</span>: &#123;</span><br><span class="line">        <span class="string">'reviewCount'</span>: <span class="string">'305733'</span>,</span><br><span class="line">        <span class="string">'bestRating'</span>: <span class="string">'5'</span>,</span><br><span class="line">        <span class="string">'ratingValue'</span>: <span class="string">u'4.4'</span>,</span><br><span class="line">        <span class="string">'worstRating'</span>: <span class="string">'1'</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>data[<span class="string">'items'</span>][<span class="number">0</span>][<span class="string">'properties'</span>][<span class="string">'ratingValue'</span>]</span><br><span class="line"><span class="string">u'4.4'</span></span><br></pre></td></tr></table></figure></p><p>现在，让我们建立一个使用 Extruct 的爬虫，它从<a href="http://www.apple.com/shop/mac/mac-accessories" target="_blank" rel="noopener"> 苹果产品网站 </a> 上提取价格和评级。该网站使用了微观数据来存储所列出的产品信息。它为每个产品使用这个结构 T ：<br><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">itemtype</span>=<span class="string">"http://schema.org/Product"</span> <span class="attr">itemscope</span>=<span class="string">"itemscope"</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">img</span> <span class="attr">src</span>=<span class="string">"/images/MLA02.jpg"</span> <span class="attr">itemprop</span>=<span class="string">"image"</span> /&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"/shop/product/MLA02/magic-mouse-2?"</span> <span class="attr">itemprop</span>=<span class="string">"url"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">span</span> <span class="attr">itemprop</span>=<span class="string">"name"</span>&gt;</span>Magic Mouse 2<span class="tag">&lt;/<span class="name">span</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"as-pinwheel-info"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">div</span> <span class="attr">itemprop</span>=<span class="string">"offers"</span> <span class="attr">itemtype</span>=<span class="string">"http://schema.org/Offer"</span> <span class="attr">itemscope</span>=<span class="string">"itemscope"</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">meta</span> <span class="attr">itemprop</span>=<span class="string">"priceCurrency"</span> <span class="attr">content</span>=<span class="string">"USD"</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">span</span> <span class="attr">class</span>=<span class="string">"as-pinwheel-pricecurrent"</span> <span class="attr">itemprop</span>=<span class="string">"price"</span>&gt;</span></span><br><span class="line">        $79.00</span><br><span class="line">      <span class="tag">&lt;/<span class="name">span</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br></pre></td></tr></table></figure></p><p>有了这个设置，你不需要使用 XPath 或者 CSS 选择器来提取所需数据。你只需要在你的爬虫中使用 Extruct 的 MicrodataExtractor ：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> extruct.w3cmicrodata <span class="keyword">import</span> MicrodataExtractor</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AppleSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">"apple"</span></span><br><span class="line">    allowed_domains = [<span class="string">"apple.com"</span>]</span><br><span class="line">    start_urls = (</span><br><span class="line">        <span class="string">'http://www.apple.com/shop/mac/mac-accessories'</span>,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        extractor = MicrodataExtractor()</span><br><span class="line">        items = extractor.extract(response.body_as_unicode(), response.url)[<span class="string">'items'</span>]</span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> items:</span><br><span class="line">            <span class="keyword">if</span> item.get(<span class="string">'properties'</span>, &#123;&#125;).get(<span class="string">'name'</span>):</span><br><span class="line">                properties = item[<span class="string">'properties'</span>]</span><br><span class="line">                <span class="keyword">yield</span> &#123;</span><br><span class="line">                    <span class="string">'name'</span>: properties[<span class="string">'name'</span>],</span><br><span class="line">                    <span class="string">'price'</span>: properties[<span class="string">'offers'</span>][<span class="string">'properties'</span>][<span class="string">'price'</span>],</span><br><span class="line">                    <span class="string">'url'</span>: properties[<span class="string">'url'</span>]</span><br><span class="line">                &#125;</span><br></pre></td></tr></table></figure></p><p>此爬虫会生成这样的项：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="string">"url"</span>: <span class="string">"http://www.apple.com/shop/product/MJ2R2/magic-trackpad-2?fnode=4c"</span>,</span><br><span class="line">    <span class="string">"price"</span>: <span class="string">u"$129.00"</span>,</span><br><span class="line">    <span class="string">"name"</span>: <span class="string">u"Magic Trackpad 2"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>所以，当你爬取的网站使用微观数据来将语义信息添加到它的内容中时，使用<a href="https://github.com/scrapinghub/extruct" target="_blank" rel="noopener"> Extruct </a>。这是一个比依赖统一的页面布局或者浪费时间分析 HTML 源代码更健壮的解决方案。</p><h2 id="使用-js2xml-抓取嵌入在-JavaScript-代码段中的数据"><a href="#使用-js2xml-抓取嵌入在-JavaScript-代码段中的数据" class="headerlink" title="使用 js2xml 抓取嵌入在 JavaScript 代码段中的数据"></a>使用 js2xml 抓取嵌入在 JavaScript 代码段中的数据</h2><p>你是否曾经受挫于你的浏览器呈现的网页与Scrapy下载的网页之间的差距？这可能是因为该网页中的一些内容并不在服务器发送给你的响应中。相反，它们是由你的浏览器通过JavaScript代码生成的。</p><p>你可以通过将此请求传递给一个例如<a href="http://scrapinghub.com/splash/" target="_blank" rel="noopener"> Splash </a>的 JavaScript 渲染服务来解决此问题。Splash 运行页面上的 JavaScript ，然后返回最终的页面结构以供你的爬虫使用。</p><p>Splash 专门为此设计，并<a href="http://blog.scrapinghub.com/2015/03/02/handling-javascript-in-scrapy-with-splash/" target="_blank" rel="noopener"> 与Scrapy很好的整合在一起 </a>。然而，在某些情况下，你需要的是一些简单功能，例如从一个 JavaScript 段中获取一个变量的值，所以使用这种强大的工具将大材小用。而这恰恰是 <a href="https://github.com/redapple/js2xml" target="_blank" rel="noopener"> js2xml </a> 的用武之地。它是一个将 JavaScript 代码转换成 XML 数据的库。</p><p>例如，假设一个在线零售商网站通过 JavaScript 加载产品评级。混合在该 HTML 中有这样一段 JavaScript 代码：<br><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">script</span> <span class="attr">type</span>=<span class="string">"text/javascript"</span>&gt;</span><span class="undefined"></span></span><br><span class="line"><span class="actionscript">    <span class="keyword">var</span> totalReviewsValue = <span class="number">32</span>;</span></span><br><span class="line"><span class="actionscript">    <span class="keyword">var</span> averageRating = <span class="number">4.5</span>;</span></span><br><span class="line"><span class="actionscript">    <span class="keyword">if</span>(totalReviewsValue != <span class="number">0</span>)&#123;</span></span><br><span class="line"><span class="actionscript">        events = <span class="string">"..."</span>;</span></span><br><span class="line"><span class="undefined">    &#125;</span></span><br><span class="line"><span class="undefined">    ...</span></span><br><span class="line"><span class="undefined"></span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br></pre></td></tr></table></figure></p><p>要使用 js2xml 提取 <code>averageRating</code> 的值，我们首选需要提取 <code>&lt;script&gt;</code>块，然后使用 js2xml 将此代码转换成 XML ：<br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>js_code = response.xpath(<span class="string">"//script[contains(., 'averageRating')]/text()"</span>).extract_first()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> js2xml</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>parsed_js = js2xml.parse(js_code)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">print</span> js2xml.pretty_print(parsed_js)</span><br><span class="line">&lt;program&gt;</span><br><span class="line">  &lt;var name=<span class="string">"totalReviewsValue"</span>&gt;</span><br><span class="line">    &lt;number value=<span class="string">"32"</span>/&gt;</span><br><span class="line">  &lt;/var&gt;</span><br><span class="line">  &lt;var name=<span class="string">"averageRating"</span>&gt;</span><br><span class="line">    &lt;number value=<span class="string">"4.5"</span>/&gt;</span><br><span class="line">  &lt;/var&gt;</span><br><span class="line">  &lt;<span class="keyword">if</span>&gt;</span><br><span class="line">    &lt;predicate&gt;</span><br><span class="line">      &lt;binaryoperation operation=<span class="string">"!="</span>&gt;</span><br><span class="line">        &lt;left&gt;&lt;identifier name="totalReviewsValue"/&gt;&lt;/left&gt;</span><br><span class="line">        &lt;right&gt;&lt;number value="0"/&gt;&lt;/right&gt;</span><br><span class="line">      &lt;/binaryoperation&gt;</span><br><span class="line">    &lt;/predicate&gt;</span><br><span class="line">    &lt;then&gt;</span><br><span class="line">      &lt;block&gt;</span><br><span class="line">        &lt;assign operator=<span class="string">"="</span>&gt;</span><br><span class="line">          &lt;left&gt;&lt;identifier name="events"/&gt;&lt;/left&gt;</span><br><span class="line">          &lt;right&gt;&lt;string&gt;...&lt;/string&gt;&lt;/right&gt;</span><br><span class="line">        &lt;/assign&gt;</span><br><span class="line">      &lt;/block&gt;</span><br><span class="line">    &lt;/then&gt;</span><br><span class="line">  &lt;/if&gt;</span><br><span class="line">&lt;/program&gt;</span><br></pre></td></tr></table></figure></p><p>现在，只需要建立一个 Scrapy 的 Selector ，然后使用 XPath 获取我们想要的值：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>js_sel = scrapy.Selector(_root=parsed_js)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>js_sel.xpath(<span class="string">"//program/var[@name='averageRating']/number/@value"</span>).extract_first()</span><br><span class="line"><span class="string">u'4.5'</span></span><br></pre></td></tr></table></figure></p><p>虽然你可能用思考的速度就可以编写一个正则表达式来解决这个问题，但是，一个 JavaScript 解析器将会更可靠。我们这里使用的例子是非常简单的，但在一些更复杂的例子中，正则表达式可能更难以维护得多。</p><h2 id="使用-w3lib-url-中的函数来从-URL-中抓取数据"><a href="#使用-w3lib-url-中的函数来从-URL-中抓取数据" class="headerlink" title="使用 w3lib.url 中的函数来从 URL 中抓取数据"></a>使用 w3lib.url 中的函数来从 URL 中抓取数据</h2><p>有时候，你感兴趣的数据段并不单独在一个 HTML 标签内。通常，你需要从页面上列出的 URL 中获取一些参数的值。例如，你可能对获取在 HTML 中列出的 URL 中的 <code>username</code> 的值感兴趣：<br><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"users"</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">ul</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">li</span>&gt;</span><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"/users?username=johndoe23"</span>&gt;</span>John Doe<span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">li</span>&gt;</span><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"/users?active=0&amp;username=the_jan"</span>&gt;</span>Jan Roe<span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line">     …</span><br><span class="line">    <span class="tag">&lt;<span class="name">li</span>&gt;</span><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"/users?active=1&amp;username=janie&amp;ref=b1946ac9249&amp;gas=_ga=1.234.567"</span>&gt;</span>Janie Doe<span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">ul</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br></pre></td></tr></table></figure></p><p>也许你会试图使用<a href="https://xkcd.com/208/" target="_blank" rel="noopener"> 正则表达式的超能力 </a>, 但是，请淡定，这里的<a href="https://github.com/scrapy/w3lib" target="_blank" rel="noopener"> w3lib </a>有一个更可靠的解决方案可以挽救局面：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> w3lib.url <span class="keyword">import</span> url_query_parameter</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>url_query_parameter(<span class="string">'/users?active=0&amp;username=the_jan'</span>, <span class="string">'username'</span>)</span><br><span class="line">    <span class="string">'the_jan'</span></span><br></pre></td></tr></table></figure></p><p>假如你对<a href="https://github.com/scrapy/w3lib" target="_blank" rel="noopener"> w3lib </a>感到陌生，那么看一看<a href="http://w3lib.readthedocs.org/en/latest/w3lib.html" target="_blank" rel="noopener"> 文档 </a>。稍后，我们将在我们的 <strong>跟着高手学习Scrapy技巧</strong> 系列中覆盖此Python库的一些其他功能。</p><h2 id="用-Scrapy-SitemapSpider-抓取网站"><a href="#用-Scrapy-SitemapSpider-抓取网站" class="headerlink" title="用 Scrapy SitemapSpider 抓取网站"></a>用 Scrapy SitemapSpider 抓取网站</h2><p>网络抓取工具以 URL 为基础。他们拥有的越多，他们生活的时间越长。为任何特定网站找到一个好的网址来源非常重要，因为它为抓取工具提供了一个强有力的起点。</p><p>站点地图是种子网址的绝佳来源。网站开发人员使用它们来指示哪些URL可用于以机器可读格式进行爬网。站点地图也是发现网页的好方法，否则无法访问网页，因为有些网页可能未链接到站点地图以外的任何其他页面。</p><p>站点地图通常可在<code>/sitemap.xml</code>，<code>robots.txt</code>文件中指定的不同位置或位于不同位置。</p><p>使用 Scrapy ，您无需担心解析 XML 和发出请求。它包含一个 SitemapSpider 类，您可以继承它以处理所有这些问题。</p><p><strong>SitemapSpider in Action</strong>：假设您想抓取 Apple 的网站来检查不同的产品。您希望访问尽可能多的网页，以便尽可能多地抓取数据。幸运的是，Apple 的网站在 <a href="http://apple.com/sitemap.xml" target="_blank" rel="noopener"> apple.com/sitemap.xml </a> 上提供了一个网站地图，如下所示：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">urlset</span> <span class="attr">xmlns</span>=<span class="string">"http://www.sitemaps.org/schemas/sitemap/0.9"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">url</span>&gt;</span><span class="tag">&lt;<span class="name">loc</span>&gt;</span>http://www.apple.com/<span class="tag">&lt;/<span class="name">loc</span>&gt;</span><span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">url</span>&gt;</span><span class="tag">&lt;<span class="name">loc</span>&gt;</span>http://www.apple.com/about/<span class="tag">&lt;/<span class="name">loc</span>&gt;</span><span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="tag">&lt;<span class="name">url</span>&gt;</span><span class="tag">&lt;<span class="name">loc</span>&gt;</span>http://www.apple.com/about/workingwithapple.html<span class="tag">&lt;/<span class="name">loc</span>&gt;</span><span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">url</span>&gt;</span><span class="tag">&lt;<span class="name">loc</span>&gt;</span>http://www.apple.com/accessibility/<span class="tag">&lt;/<span class="name">loc</span>&gt;</span><span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line">    ...</span><br><span class="line"><span class="tag">&lt;/<span class="name">urlset</span>&gt;</span></span><br></pre></td></tr></table></figure><p>Scrapy 的通用 <a href="http://doc.scrapy.org/en/latest/topics/spiders.html?_ga=2.191979183.427721090.1523448002-1918622539.1516814412#sitemapspider" target="_blank" rel="noopener"> SitemapSpider </a> 类实现了解析和分派处理站点地图所需的所有请求的逻辑。它从站点地图中读取和提取 URL ，并会为它找到的每个 URL 分派一个请求。这是一个蜘蛛，它会使用网站地图作为种子来刮掉 Apple 的网站：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.spiders <span class="keyword">import</span> SitemapSpider</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AppleSpider</span><span class="params">(SitemapSpider)</span>:</span></span><br><span class="line">    name = <span class="string">'apple-spider'</span></span><br><span class="line">    sitemap_urls = [<span class="string">'http://www.apple.com/sitemap.xml'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="keyword">yield</span> &#123;</span><br><span class="line">            <span class="string">'title'</span>: response.css(<span class="string">"title ::text"</span>).extract_first(),</span><br><span class="line">            <span class="string">'url'</span>: response.url</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment"># ...</span></span><br></pre></td></tr></table></figure><p>正如您所看到的，您只需要 SiteSiteider 的子类并将该站点地图的 URL 添加到该 sitemap_urls 属性。</p><p>现在，运行 spider 并检查结果：<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> scrapy runspider apple_spider.py -o items.jl --nolog</span></span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> head -n 5 items.jl </span></span><br><span class="line">&#123;"url": "http://www.apple.com/", "title": "Apple"&#125;</span><br><span class="line">&#123;"url": "http://www.apple.com/ae/support/products/iphone.html", "title": "Support - AppleCare+ - iPhone - Apple (AE)"&#125;</span><br><span class="line">&#123;"url": "http://www.apple.com/ae/support/products/ipad.html", "title": "Support - AppleCare+ - iPad - Apple (AE)"&#125;</span><br><span class="line">&#123;"url": "http://www.apple.com/ae/support/products/", "title": "Support - AppleCare - Apple (AE)"&#125;</span><br><span class="line">&#123;"url": "http://www.apple.com/ae/support/ipod/", "title": "iPod - Apple Support"&#125;</span><br></pre></td></tr></table></figure></p><p>Scrapy 会在站点地图中为 SitemapSpider 找到的每个网址发送一个请求，然后它会调用 parse 处理它获取的每个响应的方法。但是，网站中的某些页面在结构上可能会有所不同，因此您可能希望对不同类型的页面使用多个回调。</p><p>例如，您可以定义一个特定的回调来处理 Mac 页面，另一个用于 iTunes 页面和 parse 所有其他页面的默认方法：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.spiders <span class="keyword">import</span> SitemapSpider</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AppleSpider</span><span class="params">(SitemapSpider)</span>:</span></span><br><span class="line">    name = <span class="string">'apple-spider'</span></span><br><span class="line">    sitemap_urls = [<span class="string">'http://www.apple.com/sitemap.xml'</span>]</span><br><span class="line">    sitemap_rules = [</span><br><span class="line">        (<span class="string">'/mac/'</span>, <span class="string">'parse_mac'</span>),</span><br><span class="line">        (<span class="string">'/itunes/'</span>, <span class="string">'parse_itunes'</span>),</span><br><span class="line">        (<span class="string">''</span>, <span class="string">'parse'</span>)</span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        self.log(<span class="string">"default parsing method for &#123;&#125;"</span>.format(response.url))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_mac</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        self.log(<span class="string">"parse_mac method for &#123;&#125;"</span>.format(response.url))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_itunes</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        self.log(<span class="string">"parse_itunes method for &#123;&#125;"</span>.format(response.url))</span><br></pre></td></tr></table></figure><p>要做到这一点，您必须为 <code>sitemap_rules</code> 您的类添加一个属性，将 URL 模式映射到回调。例如，与 <code>/mac/</code> 模式匹配的 URL 将通过该 <code>parse_mac</code> 方法处理其响应。</p><p>因此，下次您编写抓取工具时，如果您想全面抓取网站，请务必使用 SitemapSpider 。</p><p>有关更多功能，请查看<a href="http://doc.scrapy.org/en/latest/topics/spiders.html?_ga=2.229931289.427721090.1523448002-1918622539.1516814412#sitemapspider" target="_blank" rel="noopener"> SitemapSpider </a>的文档。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;原文：&lt;a href=&quot;http://blog.scrapinghub.com/2016/01/19/scrapy-tips-from-the-pros-part-1/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt; 跟着高手学习Scrapy技巧：第一部分 &lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;a href=&quot;http://scrapy.org&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Scrapy &lt;/a&gt; 是&lt;a href=&quot;http://scrapinghub.com&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt; Scrapinghub &lt;/a&gt;  的关键部分。我们广泛地采用此框架，并已积累了许多各种不同的快捷方法来解决常见问题。我们推出了一个系列来与大家分享这些 Scrapy 的技巧，这样，你就可以在你的日常工作流程中最有效的使用它。每一个博文将给出两到三个提示，敬请关注。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://dongfei.oss-cn-shanghai.aliyuncs.com/blog/scrapylogo.png&quot; alt=&quot;scrapylogo&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Python" scheme="http://blog.dongfei.xin/categories/Python/"/>
    
      <category term="Scrapy" scheme="http://blog.dongfei.xin/categories/Python/Scrapy/"/>
    
    
      <category term="Scrapy" scheme="http://blog.dongfei.xin/tags/Scrapy/"/>
    
  </entry>
  
  <entry>
    <title>精通 Scrapy 网络爬虫（代理篇）</title>
    <link href="http://blog.dongfei.xin/2018-04-08/%E7%B2%BE%E9%80%9A-Scrapy-%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%EF%BC%88%E4%BB%A3%E7%90%86%E7%AF%87%EF%BC%89/"/>
    <id>http://blog.dongfei.xin/2018-04-08/精通-Scrapy-网络爬虫（代理篇）/</id>
    <published>2018-04-08T06:44:23.000Z</published>
    <updated>2018-04-15T11:56:11.030Z</updated>
    
    <content type="html"><![CDATA[<h5 id="使用-HTTP-代理"><a href="#使用-HTTP-代理" class="headerlink" title="使用 HTTP 代理"></a>使用 HTTP 代理</h5><p>HTTP 代理服务器可以比作客户端与 Web 服务器（网站）之间的一个信息中转站，客户端发送的 HTTP 请求和 Web 服务器返回的 HTTP 响应通过代理服务器转发给对方，如图所示：</p><p><img src="https://dongfei.oss-cn-shanghai.aliyuncs.com/blog/proxy1.jpg" alt="proxy1"></p><a id="more"></a><p>爬虫程序在爬取某些网站时也需要使用代理。例如：</p><ul><li><p>由于网络环境园素，直接爬取速度太慢，使用代理提高爬取速度。</p></li><li><p>某些网站对用户的访问速度进行限制，爬取过快会被封禁 ip ，使用代理防止被封禁。</p></li><li><p>由于地方法律或政治原因，某些网站无法直接访问，使用代理绕过访问限制。</p></li></ul><h5 id="HttpProxyMiddleware"><a href="#HttpProxyMiddleware" class="headerlink" title="HttpProxyMiddleware"></a>HttpProxyMiddleware</h5><p>Scrapy 内部提供了一个下载中间件 HttpProxyMiddleware ，专门用于给 Scrapy 爬虫设置代理。</p><h5 id="使用简介"><a href="#使用简介" class="headerlink" title="使用简介"></a>使用简介</h5><p>HttpProxyMiddleware 默认便是启用的，它会在系统环境变量中搜索当前系统代理（名字格式为 XXX_proxy 的环境变量），作为 scrapy 爬虫使用的代理。 </p><p><a href="http://cn-proxy.com/" target="_blank" rel="noopener">最新中国 ip 地址代理服务器</a></p><p>为本机的 Scrapy 爬虫分别设置发送 HTTP 和 HTTPS 请求时所使用的代理，只需要 bash 中添加加环境变量：<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">Linux 系统设置环境变量：</span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">export</span>  http_proxy=<span class="string">"http://120.92.118.64:10010"</span>  <span class="comment"># 为 HTTP 请求设置代理</span></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">export</span>  https_proxy=<span class="string">"http://118.114.77.47:8080"</span>  <span class="comment"># 为 HTTPS 请求设置代理</span></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">export</span>  <span class="comment"># 查看变量</span></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">unset</span> http_proxy  <span class="comment"># 删除变量</span></span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Windows 系统设置环境变量：</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">set</span> http_proxy=<span class="string">"http://120.92.118.64:10010"</span> <span class="comment"># 为 HTTP 请求设置代理</span></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">set</span> https_proxy=<span class="string">"http://118.114.77.47:8080"</span> <span class="comment"># 为 HTTPS 请求设置代理</span></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">set</span> <span class="comment"># 查看变量</span></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">set</span> http_proxy= <span class="comment"># 删除变量</span></span></span><br></pre></td></tr></table></figure></p><p>配置完成后，Scrapy 爬虫将会使用上面指定的代理下载页面。</p><p>示例，利用网站 <a href="http://httpbin.org" target="_blank" rel="noopener">http://httpbin.org</a> 提供的服务可以窥视我们所发送的 HTTP(S) 请求，如请求源 IP 地址、请求头部、Cookie 信息等。如图展示了该网站各种服务的 API 地址：</p><p><img src="https://dongfei.oss-cn-shanghai.aliyuncs.com/blog/proxy2.jpg" alt="proxy2"></p><p>访问 <a href="http://httpbin.org/ip" target="_blank" rel="noopener">http://httpbin.org/ip</a> 将返回一个包含请求源 IP 地址信息的 json 串，在 scrapy shell 中访问该 url ，查看请求源 IP 地址：</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">(py3) [root@izuf6g6v8wminmgpw6vd89z py3]<span class="comment"># scrapy shell </span></span><br><span class="line"></span><br><span class="line">。。。。。。。。。。。。</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; import json</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; fetch(scrapy.Request(<span class="string">'http://httpbin.org/ip'</span>))</span><br><span class="line"><span class="number">2018</span>-<span class="number">04</span>-08 <span class="number">22</span><span class="symbol">:</span><span class="number">44</span><span class="symbol">:</span><span class="number">26</span> [scrapy.core.engine] <span class="symbol">INFO:</span> Spider opened</span><br><span class="line"><span class="number">2018</span>-<span class="number">04</span>-08 <span class="number">22</span><span class="symbol">:</span><span class="number">44</span><span class="symbol">:</span><span class="number">26</span> [scrapy.core.engine] <span class="symbol">DEBUG:</span> Crawled (<span class="number">200</span>) &lt;GET <span class="symbol">http:</span>/<span class="regexp">/httpbin.org/ip</span>&gt; (<span class="symbol">referer:</span> None)</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; json.loads(response.text)</span><br><span class="line">&#123;<span class="string">'origin'</span>: <span class="string">'120.92.118.64'</span>&#125;</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; fetch(scrapy.Request(<span class="string">'https://httpbin.org/ip'</span>))</span><br><span class="line"><span class="number">2018</span>-<span class="number">04</span>-08 <span class="number">22</span><span class="symbol">:</span><span class="number">44</span><span class="symbol">:</span><span class="number">52</span> [scrapy.core.engine] <span class="symbol">DEBUG:</span> Crawled (<span class="number">200</span>) &lt;GET <span class="symbol">https:</span>/<span class="regexp">/httpbin.org/ip</span>&gt; (<span class="symbol">referer:</span> None)</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; json.loads(response.text)</span><br><span class="line">&#123;<span class="string">'origin'</span>: <span class="string">'118.114.77.47'</span>&#125;</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt;</span><br></pre></td></tr></table></figure><p>在上述实验中，分别以 HTTP 和 HTTPS 发送请求，使用 json 模块对响应结果进行解析，读取请求源 IP 地址（ origin 字段），其值正是代理服务器的 IP。由此证明，Scrapy 爬虫使用了指定的代理。</p><p>上面使用的是无须身份验证的代理服务器，还有一些代理服务器需要用户提供账号、密码进行身份验证，验证成功后才提供代理服务，使用此类代理时，可按以下格式配置：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="builtin-name">export</span> <span class="attribute">http_proxy</span>=<span class="string">"http://dongfei:123456@39.134.10.98:8080"</span></span><br></pre></td></tr></table></figure><h5 id="源码分析"><a href="#源码分析" class="headerlink" title="源码分析"></a>源码分析</h5><p><strong>HttpProxyMiddleware</strong> ：<code>Lib/site-packages/scrapy/downloadermiddlewares/httpproxy.py</code></p><p>分析代码如下：</p><ul><li>__init__ 方法</li></ul><p>在 HttpProxyMiddleware 的构造器中，使用 Python 标准库 urllib 中的 getproxies 函数在系统环境变量中搜索系统代理的相关配置（变量名格式为 [协议]_proxy 的变量）<br>调用 self._get_proxy 方法解析代理配置信息，并将其返回结果保存到 self.proxies 字段中，如果没有找到任何代理配置，就拋出 NotConfigured 异常，HttpProxyMiddleware 被弃用。</p><ul><li>_get_proxy 方法</li></ul><p>解析代理配置信息，返回身份验证信息以及代理服务器 url。</p><ul><li>process_request 方法</li></ul><p>处理每一个待发送的请求，为没有设置过代理的请求（meta 属性不包含 proxy 的请求）调用 self._set_proxy 方法设置代理。</p><ul><li>_set_proxy 方法</li></ul><p>为一个请求设置代理，以请求的协议（HTTP 或 HTTPS ）作为键，从代理服务器信息字典 self.proxies 中选择代理，赋值给 request.meta 的 proxy 字段。对于需要身份验证的代理服务器，添加 HTTP 头部 Proxy-Authorization ，其值是在 _get_proxy 方法中计算得到的。</p><p>经分析得知，在 Scrapy 中为一个请求设置代理的本质就是将代理服务器的 url 时填写到 request.meta[‘proxy’]。</p><h5 id="使用多个代理"><a href="#使用多个代理" class="headerlink" title="使用多个代理"></a>使用多个代理</h5><p>利用 HttpProxyMiddleware 为爬虫设置代理时，对于一种协议（ HTTP 或 HTTPS ）的所有请求只能使用一个代理，如果想使用多个代理，可以在构造每一个 Request 对象时，通过 meta 参数的 proxy 字段手动设置代理 ：<br><figure class="highlight lisp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">request = Request('http<span class="symbol">://example</span>.com/1', meta=&#123;'proxy':'http<span class="symbol">://42</span>.<span class="number">178.202</span>.<span class="number">18</span>:<span class="number">8080</span>'&#125;)</span><br><span class="line">request = Request('http<span class="symbol">://example</span>.com/2', meta=&#123;'proxy':'http<span class="symbol">://182</span>.<span class="number">18.202</span>.<span class="number">18</span>:<span class="number">8080</span>'&#125;)</span><br><span class="line">request = Request('http<span class="symbol">://example</span>.com/3', meta=&#123;'proxy':'http<span class="symbol">://89</span>.<span class="number">190.202</span>.<span class="number">18</span>:<span class="number">8080</span>'&#125;)</span><br></pre></td></tr></table></figure></p><p>按照与之前相同的做法，在 scrapy shell 进行实验，验证代理是否被使用：</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">(py3) [root@izuf6g6v8wminmgpw6vd89z py3]<span class="comment"># scrapy shell </span></span><br><span class="line"></span><br><span class="line">。。。。。。。。。。。。</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; import json</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; from scrapy import Request</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; r = Request(<span class="string">'http://httpbin.org/ip'</span>, meta=&#123;<span class="string">'proxy'</span>: <span class="string">'http://39.134.10.18:8080'</span>&#125;)</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; fetch(r)</span><br><span class="line"><span class="number">2018</span>-<span class="number">04</span>-08 <span class="number">19</span><span class="symbol">:</span><span class="number">43</span><span class="symbol">:</span><span class="number">06</span> [scrapy.core.engine] <span class="symbol">DEBUG:</span> Crawled (<span class="number">200</span>) &lt;GET <span class="symbol">http:</span>/<span class="regexp">/httpbin.org/ip</span>&gt; (<span class="symbol">referer:</span> None)</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; json.loads(response.text)</span><br><span class="line">&#123;<span class="string">'origin'</span>: <span class="string">'39.134.10.18'</span>&#125;</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; r = Request(<span class="string">'https://httpbin.org/ip'</span>, meta=&#123;<span class="string">'proxy'</span>: <span class="string">'http://42.178.202.18:8080'</span>&#125;)</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; fetch(r)</span><br><span class="line"><span class="number">2018</span>-<span class="number">04</span>-08 <span class="number">19</span><span class="symbol">:</span><span class="number">44</span><span class="symbol">:</span><span class="number">06</span> [scrapy.core.engine] <span class="symbol">DEBUG:</span> Crawled (<span class="number">200</span>) &lt;GET <span class="symbol">https:</span>/<span class="regexp">/httpbin.org/ip</span>&gt; (<span class="symbol">referer:</span> None)</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; json.loads(response.text)</span><br><span class="line">&#123;<span class="string">'origin'</span>: <span class="string">'42.178.202.18'</span>&#125;</span><br></pre></td></tr></table></figure><p>结果表明，Scrapy 爬虫同样使用了指定的代理服务器。</p><p>使用手动方式设置代理时，如果使用的代理需要身份验证，还需要通过 HTTP 头部的 Proxy-Authorization 字段传递包含用户账号和密码的身份验证信息。可以参考 HttpProxyMiddleware._get_proxy 中的相关实现，按以下过程生成身份验证信息：</p><p>（1）将账号、密码拼接成形如 <code>user:passwd</code> 的字符串 s1。<br>（2）按代理服务器要求对 s1 进行编码（如 utf8 ），生成 s2 。<br>（3）再对 s2 进行 Base64 编码，生成 s3。<br>（4）将 s3 拼接到固定字节串 b<code>Basic</code> 后面，得到最终的身份验证信息。</p><p>示例代码如下：</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;</span>&gt; from scrapy import Request</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; import base64</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; req = Request(<span class="string">'http://httpbin.org/ip'</span>, meta=&#123;<span class="string">'proxy'</span>: <span class="string">'http://42.178.202.18:8080'</span>&#125;)</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; user = <span class="string">'dongfei'</span></span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; passwd = <span class="string">'12345678'</span></span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; user_passwd = (<span class="string">'%s:%s'</span><span class="string">%(user,passwd)</span>).encode(<span class="string">'utf8'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; user_passwd</span><br><span class="line">b<span class="string">'dongfei:12345678'</span></span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; req.headers[<span class="string">'Proxy-Authorization'</span>] = b<span class="string">'Basic'</span> + base64.b64encode(user_passwd)</span><br><span class="line"><span class="meta">&gt;&gt;</span>&gt; fetch(req)</span><br><span class="line">.........................</span><br></pre></td></tr></table></figure><h5 id="获取免费代理"><a href="#获取免费代理" class="headerlink" title="获取免费代理"></a>获取免费代理</h5><p>可以通过 google 或 baidu 找到一些提供免费代理服务器信息的网站。例如：</p><ul><li><a href="http://proxy-list.org" target="_blank" rel="noopener">http://proxy-list.org</a>（国外）</li><li><a href="http://free-proxy-list.net" target="_blank" rel="noopener">http://free-proxy-list.net</a>（国外）</li><li><a href="http://www.xicidaili.com" target="_blank" rel="noopener">http://www.xicidaili.com</a></li><li><a href="http://www.proxy360.cn" target="_blank" rel="noopener">http://www.proxy360.cn</a></li><li><a href="http://www.kuaidaili.com" target="_blank" rel="noopener">http://www.kuaidaili.com</a></li><li><a href="http://cn-proxy.com/" target="_blank" rel="noopener">http://cn-proxy.com/</a></li><li><a href="https://31f.cn/socks-proxy/" target="_blank" rel="noopener">https://31f.cn/socks-proxy/</a></li></ul><p>以 <a href="http://www.xicidaili.com" target="_blank" rel="noopener">http://www.xicidaili.com</a> 为例，如图所示为该网站 <strong>国内高匿代理</strong> 分类下的页面：</p><p><img src="https://dongfei.oss-cn-shanghai.aliyuncs.com/blog/proxy3.jpg" alt="proxy3"></p><p>接下来爬取 <strong>国内高匿代理</strong> 分类中前 3 页的所有代理服务器信息。并验证每个代理是否可用。</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy genspider xici_proxy www<span class="selector-class">.xicidaili</span><span class="selector-class">.com</span></span><br></pre></td></tr></table></figure><p>该网站会监测用户发送的 HTTP 请求头部中的 User-Agent 字段，因此我们需要伪装成某种常规浏览器，在配置文件添加如下代码：<br><figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">custom_settings = &#123;</span><br><span class="line">    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.<span class="number">3112.11</span>3 Safari/537.36',</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>实现 XiciProxySpider 爬取代理服务器信息，并过滤不可用代理，代码如下：</p><figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding:utf<span class="number">-8</span> -*-</span><br><span class="line">__author__ = <span class="string">"東飛"</span></span><br><span class="line">__date__ = <span class="string">"2017-11-29"</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> Request, FormRequest</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> XiciProxySpider(scrapy.Spider):</span><br><span class="line">    name = <span class="string">'xici_proxy'</span></span><br><span class="line">    allowed_domains = [<span class="string">'www.xicidaili.com'</span>]</span><br><span class="line">    start_urls = [<span class="string">'http://www.xicidaili.com/nn/'</span>]</span><br><span class="line"></span><br><span class="line">    custom_settings = &#123;</span><br><span class="line">        <span class="string">"DOWNLOADER_MIDDLEWARES"</span>: &#123;</span><br><span class="line">            # 置于 HttpProxyMiddleware(<span class="number">750</span>) 之前</span><br><span class="line">            # <span class="string">'ArticleSpider.middlewares.RandomUserAgentMiddleware'</span>: <span class="number">745</span>,</span><br><span class="line">            # <span class="string">'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware'</span>: None,</span><br><span class="line">            <span class="string">'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware'</span>: None,</span><br><span class="line">            <span class="string">'scrapy_fake_useragent.middleware.RandomUserAgentMiddleware'</span>: <span class="number">400</span>,</span><br><span class="line">        &#125;,</span><br><span class="line">        # <span class="string">'DOWNLOAD_DELAY'</span>: <span class="number">2</span>,</span><br><span class="line">        <span class="string">'COOKIES_ENABLED'</span>: <span class="literal">False</span>,</span><br><span class="line">        # <span class="string">'DOWNLOAD_TIMEOUT'</span>: <span class="number">180</span>,</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    def start_requests(self):</span><br><span class="line">        # 爬取 http:<span class="comment">//www.xicidaili.com/nn/ 前 3 页</span></span><br><span class="line">        for i <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">3</span>):</span><br><span class="line">            yield Request(<span class="string">'http://www.xicidaili.com/nn/%s'</span> % i)</span><br><span class="line"></span><br><span class="line">    def parse(self, response):</span><br><span class="line"></span><br><span class="line">        for sel <span class="keyword">in</span> response.xpath(<span class="string">'//table[@id="ip_list"]/tr[position()&gt;1]'</span>):</span><br><span class="line">            # 提取代理 IP、port、scheme(http or https)</span><br><span class="line">            ip = sel.css(<span class="string">'td:nth-child(2)::text'</span>).extract_first()</span><br><span class="line">            port = sel.css(<span class="string">'td:nth-child(3)::text'</span>).extract_first()</span><br><span class="line">            scheme = sel.css(<span class="string">'td:nth-child(6)::text'</span>).extract_first().lower()</span><br><span class="line"></span><br><span class="line">            # 使用爬取到的代理再次发送请求到 http(s):<span class="comment">//httpbin.org/ip ，验证代理是否可用</span></span><br><span class="line"></span><br><span class="line">            url = <span class="string">'%s://httpbin.org/ip'</span> % scheme</span><br><span class="line">            # url = <span class="string">'%s://ip.taobao.com/service/getIpInfo.php'</span> % scheme</span><br><span class="line">            # url = <span class="string">'%s://fp.ip-api.com/json'</span> % scheme</span><br><span class="line">            # url = <span class="string">'%s://ip-api.com/json'</span> % scheme</span><br><span class="line"></span><br><span class="line">            proxy = <span class="string">'%s://%s:%s'</span> % (scheme, ip, port)</span><br><span class="line"></span><br><span class="line">            # formdata = &#123;</span><br><span class="line">            #     <span class="string">'ip'</span>: <span class="string">'myip'</span>,</span><br><span class="line">            # &#125;</span><br><span class="line"></span><br><span class="line">            meta = &#123;</span><br><span class="line">                <span class="string">'proxy'</span>: proxy,</span><br><span class="line">                <span class="string">'dont_retry'</span>: <span class="literal">True</span>,</span><br><span class="line">                <span class="string">'download_timeout'</span>: <span class="number">10</span>,</span><br><span class="line"></span><br><span class="line">                # 以下两个字段是传递给 check_available 方法的信息，方便检测</span><br><span class="line">                <span class="string">'_proxy_scheme'</span>: scheme,</span><br><span class="line">                <span class="string">'_proxy_ip'</span>: ip,</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            yield Request(url, callback=self.check_available, meta=meta, dont_filter=<span class="literal">True</span>)</span><br><span class="line">            # yield FormRequest(url, callback=self.check_available, formdata=formdata, meta=meta, dont_filter=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    def check_available(self, response):</span><br><span class="line">        proxy_ip = response.meta[<span class="string">'_proxy_ip'</span>]</span><br><span class="line"></span><br><span class="line">        # <span class="keyword">if</span> proxy_ip == json.loads(response.text)[<span class="string">'data'</span>][<span class="string">'ip'</span>]:</span><br><span class="line">            # <span class="keyword">from</span> pprint <span class="keyword">import</span> pprint</span><br><span class="line">            # pprint(&#123;</span><br><span class="line">            #         <span class="string">'ip'</span>: json.loads(response.text)[<span class="string">'query'</span>],</span><br><span class="line">            #         <span class="string">'user_agent'</span>: json.loads(response.text)[<span class="string">'user_agent'</span>],</span><br><span class="line">            #     &#125;)</span><br><span class="line">            # <span class="keyword">if</span> proxy_ip == json.loads(response.text)[<span class="string">'query'</span>]:</span><br><span class="line">        </span><br><span class="line">        # 判断代理是否具有隐藏 IP 功能</span><br><span class="line">        <span class="keyword">if</span> proxy_ip == json.loads(response.text)[<span class="string">'origin'</span>]:</span><br><span class="line">            yield &#123;</span><br><span class="line">                <span class="string">'proxy_scheme'</span>: response.meta[<span class="string">'_proxy_scheme'</span>],</span><br><span class="line">                <span class="string">'proxy'</span>: response.meta[<span class="string">'proxy'</span>],</span><br><span class="line">            &#125;</span><br></pre></td></tr></table></figure><p>解释上述代码如下：</p><ul><li><p>在 start_requests 方法中请求 <a href="http://www.xicidaili.com/nn/" target="_blank" rel="noopener">http://www.xicidaili.com/nn/</a> 下的前 3 页，以 parse 方法作为页面解析函数。</p></li><li><p>在 parse 方法中提取一个页面中所有的代理服务器信息，这些代理未必都是可用的，因此使用爬取到的代理发送请求到 <code>http(s)://httpbin.org/ip</code> 验证其是否可用。以 check_available 方法作为页面解析函数。</p></li><li><p>能执行到 check_available 方法，意味着 response 对应请求所使用的代理是可用的。在 check_available 方法中，通过响应 json 串中的 origin 字段可以判断代理是否是匿名的（隐藏 ip），返回匿名代理。</p></li></ul><p>运行爬虫，将可用的代理服务器保存到 json 文件中，供其他程序使用：</p><figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">F:\VirtualEnv\jobboleArticle\ArticleSpider\ArticleSpider</span><br><span class="line">(jobboleArticle) $ scrapy crawl xici_proxy -o .\data\proxy_list.json</span><br><span class="line"></span><br><span class="line">....................</span><br><span class="line"></span><br><span class="line">(jobboleArticle) $ cat -n .\data\proxy_list.json</span><br><span class="line">     <span class="number">1</span>  [</span><br><span class="line">     <span class="number">2</span>  &#123;<span class="string">"proxy_scheme"</span>: <span class="string">"http"</span>, <span class="string">"proxy"</span>: <span class="string">"http://183.159.87.96:18118"</span>&#125;,</span><br><span class="line">     <span class="number">3</span>  &#123;<span class="string">"proxy_scheme"</span>: <span class="string">"https"</span>, <span class="string">"proxy"</span>: <span class="string">"https://60.177.228.86:18118"</span>&#125;,</span><br><span class="line">     <span class="number">4</span>  &#123;<span class="string">"proxy_scheme"</span>: <span class="string">"http"</span>, <span class="string">"proxy"</span>: <span class="string">"http://60.177.229.113:18118"</span>&#125;,</span><br><span class="line">     <span class="number">5</span>  &#123;<span class="string">"proxy_scheme"</span>: <span class="string">"http"</span>, <span class="string">"proxy"</span>: <span class="string">"http://183.159.93.180:18118"</span>&#125;,</span><br><span class="line">     <span class="number">6</span>  &#123;<span class="string">"proxy_scheme"</span>: <span class="string">"https"</span>, <span class="string">"proxy"</span>: <span class="string">"https://183.159.83.153:18118"</span>&#125;,</span><br><span class="line">     <span class="number">7</span>  &#123;<span class="string">"proxy_scheme"</span>: <span class="string">"https"</span>, <span class="string">"proxy"</span>: <span class="string">"https://114.99.29.251:18118"</span>&#125;,</span><br><span class="line">     <span class="number">8</span>  &#123;<span class="string">"proxy_scheme"</span>: <span class="string">"http"</span>, <span class="string">"proxy"</span>: <span class="string">"http://183.159.93.180:18118"</span>&#125;,</span><br><span class="line">     <span class="number">9</span>  &#123;<span class="string">"proxy_scheme"</span>: <span class="string">"https"</span>, <span class="string">"proxy"</span>: <span class="string">"https://183.159.94.136:18118"</span>&#125;,</span><br><span class="line">    <span class="number">10</span>  &#123;<span class="string">"proxy_scheme"</span>: <span class="string">"https"</span>, <span class="string">"proxy"</span>: <span class="string">"https://183.159.93.27:18118"</span>&#125;</span><br><span class="line">    <span class="number">11</span>  ]</span><br></pre></td></tr></table></figure><h5 id="实现随机代理"><a href="#实现随机代理" class="headerlink" title="实现随机代理"></a>实现随机代理</h5><p>某些网站为防止爬虫爬取会对接收到的请求进行监测，如果短时间内接收到了来自同一 IP 的大量请求，就判定该 IP 的主机在使用爬虫程序爬取网站，因而将该 IP 封禁（拒绝请求）。爬虫程序可以使用多个代理对此类网站进行爬取，此时单位时间的访问量会被多个代理分摊，从而避免封禁 IP 。</p><p>下面基于 HttpProxyMiddleware 实现一个随机代理下载中同件。</p><p>在 <code>middlewares.py</code> 中实现 <code>RandomHttpProxyMiddleware</code> 代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.downloadermiddlewares.httpproxy <span class="keyword">import</span> HttpProxyMiddleware</span><br><span class="line"><span class="keyword">from</span> scrapy.exceptions <span class="keyword">import</span> NotConfigured</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RandomHttpProxyMiddleware</span><span class="params">(HttpProxyMiddleware)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, auth_encoding=<span class="string">'latin-1'</span>, proxy_list_file=None)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> proxy_list_file:</span><br><span class="line">            <span class="keyword">raise</span> NotConfigured</span><br><span class="line"></span><br><span class="line">        self.auth_encoding = auth_encoding</span><br><span class="line">        <span class="comment"># 分别用两个列表维护 HTTP 和 HTTPS 代理，&#123;'http':[...],'https':[....]&#125;</span></span><br><span class="line">        self.proxies = defaultdict(list)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 从 json 文件中读取代理服务器信息，填入 self.proxies</span></span><br><span class="line">        <span class="keyword">with</span> open(proxy_list_file) <span class="keyword">as</span> f:</span><br><span class="line">            proxy_list = json.load(f)</span><br><span class="line">            <span class="keyword">for</span> proxy <span class="keyword">in</span> proxy_list:</span><br><span class="line">                scheme = proxy[<span class="string">'proxy_scheme'</span>]</span><br><span class="line">                url = proxy[<span class="string">'proxy'</span>]</span><br><span class="line">                self.proxies[scheme].append(self._get_proxy(url, scheme))</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_crawler</span><span class="params">(cls, crawler)</span>:</span></span><br><span class="line">        <span class="comment"># 从配置文件中读取用户验证信息的编码</span></span><br><span class="line">        auth_encoding = crawler.settings.get(<span class="string">'HTTPPROXY_AUTH_ENCODING'</span>,<span class="string">'latin-1'</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 从配置文件中读取代理服务器列表文件（json）的路径</span></span><br><span class="line">        proxy_list_file = crawler.settings.get(<span class="string">'HTTPPROXY_PROXY_LIST_FILE'</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> cls(auth_encoding,proxy_list_file)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_set_proxy</span><span class="params">(self, request, scheme)</span>:</span></span><br><span class="line">        <span class="comment"># 随机选择一个代理</span></span><br><span class="line">        creds, proxy = random.choice(self.proxies[scheme])</span><br><span class="line">        request.meta[<span class="string">'proxy'</span>] = proxy</span><br><span class="line">        <span class="keyword">if</span> creds:</span><br><span class="line">            request.headers[<span class="string">'Proxy-Authorization'</span>] = <span class="string">b'Basic '</span> + creds</span><br></pre></td></tr></table></figure><p>解释上述代码如下：</p><ul><li><p>仿照 HttpProxyMiddleware 构造器实现 RandomHttpProxyMiddleware 构造器，首先从代理服务器列表文件（配置文件中指定）中读取代理服务器信息。然后将它们按照协议 （ HTTP 或 HTTPS ）分别存入不同列表，由 self.proxies 字典维护。</p></li><li><p>_set_proxy 方法负责为每一个 Request 清单设置代理，覆写 _set_proxy 方法（覆盖基类方法），对于每一个 request ，根据请求协议获取 sdifproxis 中的代理服务器列表，然后从中随机抽取一个代理，赋值给 request.meta[‘proxy’]。</p></li></ul><p>在配置文件 <code>settings.py</code> 或者 <code>custom_settings</code> 中启用 RandomHttpProxyMiddleware ，并指定所要使用的代理服务器列表文件（json 文件），添加代码如下：</p><figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">custom_settings = &#123;</span><br><span class="line">    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.<span class="number">3112.11</span>3 Safari/537.36',</span><br><span class="line">    <span class="string">"DOWNLOADER_MIDDLEWARES"</span>: &#123;</span><br><span class="line">        <span class="meta"># 置于 HttpProxyMiddleware(750) 之前</span></span><br><span class="line">        'ArticleSpider.middlewares.RandomHttpProxyMiddleware': <span class="number">745</span>,</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="meta"># 使用之前在 http:<span class="comment">//www.xicidaili.com/ 网站爬取到的代理</span></span></span><br><span class="line">    'HTTPPROXY_PROXY_LIST_FILE': './data/proxy_list.json',</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>最后编写一个 TestRandomProxySpider 测试该中间件，重复向 发送请求，根据响应的请求源 IP 地址信息判断代理使用情况：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> Request</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TestRandomProxySpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">'test_random_proxy'</span></span><br><span class="line">    allowed_domains = [<span class="string">'httpbin.org'</span>]</span><br><span class="line">    start_urls = [<span class="string">'http://httpbin.org/'</span>]</span><br><span class="line">    custom_settings = &#123;</span><br><span class="line">        <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36'</span>,</span><br><span class="line">        <span class="string">"DOWNLOADER_MIDDLEWARES"</span>: &#123;</span><br><span class="line">            <span class="comment"># 置于 HttpProxyMiddleware(750) 之前</span></span><br><span class="line">            <span class="string">'ArticleSpider.middlewares.RandomHttpProxyMiddleware'</span>: <span class="number">745</span>,</span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="comment"># 使用之前在 http://www.xicidaili.com/ 网站爬取到的代理</span></span><br><span class="line">        <span class="string">'HTTPPROXY_PROXY_LIST_FILE'</span>: <span class="string">'ArticleSpider/data/proxy_list.json'</span>,</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">start_requests</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">            <span class="keyword">yield</span> Request(<span class="string">'http://httpbin.org/ip'</span>, dont_filter=<span class="keyword">True</span>)</span><br><span class="line">            <span class="keyword">yield</span> Request(<span class="string">'https://httpbin.org/ip'</span>, dont_filter=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        print(json.loads(response.text))</span><br></pre></td></tr></table></figure><p>运行爬虫，观察输出：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">F:\VirtualEnv\jobboleArticle\ArticleSpider\ArticleSpider</span><br><span class="line">(jobboleArticle) $ scrapy crawl test_random_proxy</span><br><span class="line"><span class="number">2018</span>-<span class="number">04</span>-<span class="number">09</span> <span class="number">11</span>:<span class="number">20</span>:<span class="number">11</span> [scrapy<span class="selector-class">.core</span><span class="selector-class">.engine</span>] DEBUG: Crawled (<span class="number">200</span>) &lt;GET http:<span class="comment">//httpbin.org/ip&gt; (referer: None)</span></span><br><span class="line">&#123;<span class="string">'origin'</span>: <span class="string">'60.177.229.113'</span>&#125;</span><br><span class="line"><span class="number">2018</span>-<span class="number">04</span>-<span class="number">09</span> <span class="number">11</span>:<span class="number">20</span>:<span class="number">13</span> [scrapy<span class="selector-class">.core</span><span class="selector-class">.engine</span>] DEBUG: Crawled (<span class="number">200</span>) &lt;GET https:<span class="comment">//httpbin.org/ip&gt; (referer: None)</span></span><br><span class="line">&#123;<span class="string">'origin'</span>: <span class="string">'183.159.94.136'</span>&#125;</span><br><span class="line"><span class="number">2018</span>-<span class="number">04</span>-<span class="number">09</span> <span class="number">11</span>:<span class="number">20</span>:<span class="number">14</span> [scrapy<span class="selector-class">.core</span><span class="selector-class">.engine</span>] DEBUG: Crawled (<span class="number">200</span>) &lt;GET https:<span class="comment">//httpbin.org/ip&gt; (referer: None)</span></span><br><span class="line">&#123;<span class="string">'origin'</span>: <span class="string">'60.177.229.113'</span>&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h5 id=&quot;使用-HTTP-代理&quot;&gt;&lt;a href=&quot;#使用-HTTP-代理&quot; class=&quot;headerlink&quot; title=&quot;使用 HTTP 代理&quot;&gt;&lt;/a&gt;使用 HTTP 代理&lt;/h5&gt;&lt;p&gt;HTTP 代理服务器可以比作客户端与 Web 服务器（网站）之间的一个信息中转站，客户端发送的 HTTP 请求和 Web 服务器返回的 HTTP 响应通过代理服务器转发给对方，如图所示：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://dongfei.oss-cn-shanghai.aliyuncs.com/blog/proxy1.jpg&quot; alt=&quot;proxy1&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Python" scheme="http://blog.dongfei.xin/categories/Python/"/>
    
      <category term="Scrapy" scheme="http://blog.dongfei.xin/categories/Python/Scrapy/"/>
    
    
      <category term="Scrapy" scheme="http://blog.dongfei.xin/tags/Scrapy/"/>
    
  </entry>
  
  <entry>
    <title>精通 Scrapy 网络爬虫（数据库篇）</title>
    <link href="http://blog.dongfei.xin/2018-04-06/%E7%B2%BE%E9%80%9A-Scrapy-%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%EF%BC%88%E6%95%B0%E6%8D%AE%E5%BA%93%E7%AF%87%EF%BC%89/"/>
    <id>http://blog.dongfei.xin/2018-04-06/精通-Scrapy-网络爬虫（数据库篇）/</id>
    <published>2018-04-06T11:59:01.000Z</published>
    <updated>2018-04-12T11:22:53.840Z</updated>
    
    <content type="html"><![CDATA[<h5 id="数据保存至数据库"><a href="#数据保存至数据库" class="headerlink" title="数据保存至数据库"></a>数据保存至数据库</h5><p>以 <code>toscrape_book</code> 项目作为环境，使用 Item Pipeline 实现 Scrapy 爬虫，将爬取到的数据存储到数据库中。爬取网站 <a href="http://books.toscrape.com/" target="_blank" rel="noopener">http://books.toscrape.com/</a> 中的书籍信息，其中每一本书的信息包括：<br><code>书名</code>、<code>价格</code>、<code>评价等级</code>、<code>产品编码</code>、<code>库存量</code>、<code>评价数量</code>。<br><img src="https://dongfei.oss-cn-shanghai.aliyuncs.com/blog/toscrape1.jpg" alt="toscrape1"><br><a id="more"></a></p><p><code>books.py</code> 代码：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> scrapy.linkextractors <span class="keyword">import</span> LinkExtractor</span><br><span class="line"><span class="keyword">from</span> ArticleSpider.items <span class="keyword">import</span> BookItem</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BooksSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">'books'</span></span><br><span class="line">    allowed_domains = [<span class="string">'books.toscrape.com'</span>]</span><br><span class="line">    start_urls = [<span class="string">'http://books.toscrape.com/'</span>]</span><br><span class="line"></span><br><span class="line">    custom_settings = &#123;</span><br><span class="line">        <span class="comment"># 指定 csv 文件各列的次序</span></span><br><span class="line">        <span class="string">"FEED_EXPORT_FIELDS"</span>: [<span class="string">'upc'</span>, <span class="string">'name'</span>, <span class="string">'price'</span>, <span class="string">'stock'</span>, <span class="string">'review_rating'</span>, <span class="string">'review_num'</span>],</span><br><span class="line">        <span class="string">"ITEM_PIPELINES"</span>: &#123;</span><br><span class="line">            <span class="string">'ArticleSpider.pipelines.BookPipeline'</span>: <span class="number">300</span>,</span><br><span class="line">        &#125;,</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 书籍列表页面的解析函数</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        le = LinkExtractor(restrict_css=<span class="string">'article.product_pod h3'</span>)</span><br><span class="line">        <span class="keyword">for</span> link <span class="keyword">in</span> le.extract_links(response):</span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(link.url, callback=self.parse_book)</span><br><span class="line"></span><br><span class="line">        le = LinkExtractor(restrict_css=<span class="string">'ul.pager li.next'</span>)</span><br><span class="line">        links = le.extract_links(response)</span><br><span class="line">        <span class="keyword">if</span> links:</span><br><span class="line">            next_url = links[<span class="number">0</span>].url</span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(next_url, callback=self.parse)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 书籍页面的解析函数</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_book</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        book = BookItem()</span><br><span class="line">        sel = response.css(<span class="string">'div.product_main'</span>)</span><br><span class="line">        book[<span class="string">'name'</span>] = sel.xpath(<span class="string">'./h1/text()'</span>).extract_first()</span><br><span class="line">        book[<span class="string">'price'</span>] = sel.css(<span class="string">'p.price_color::text'</span>).extract_first()</span><br><span class="line">        book[<span class="string">'review_rating'</span>] = sel.css(<span class="string">'p.star-rating::attr(class)'</span>).re_first(<span class="string">'star-rating ([A-Za-z]+)'</span>)</span><br><span class="line"></span><br><span class="line">        sel = response.css(<span class="string">'table.table.table-striped'</span>)</span><br><span class="line">        book[<span class="string">'upc'</span>] = sel.xpath(<span class="string">'(.//tr)[1]/td/text()'</span>).extract_first()</span><br><span class="line">        book[<span class="string">'stock'</span>] = sel.xpath(<span class="string">'(.//tr)[last()-1]/td/text()'</span>).re_first(<span class="string">'\((\d+) available\)'</span>)</span><br><span class="line">        book[<span class="string">'review_num'</span>] = sel.xpath(<span class="string">'(.//tr)[last()]/td/text()'</span>).extract_first()</span><br><span class="line">        <span class="keyword">yield</span> book</span><br></pre></td></tr></table></figure></p><p><code>pipelines.py</code> 代码：<br><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 处理书籍评价等级</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BookPipeline</span>(<span class="title">object</span>):</span></span><br><span class="line">    review_rating_map = &#123;</span><br><span class="line">        <span class="string">'One'</span>: <span class="number">1</span>,</span><br><span class="line">        <span class="string">'Two'</span>: <span class="number">2</span>,</span><br><span class="line">        <span class="string">'Three'</span>: <span class="number">3</span>,</span><br><span class="line">        <span class="string">'Four'</span>: <span class="number">4</span>,</span><br><span class="line">        <span class="string">'Five'</span>: <span class="number">5</span>,</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(<span class="keyword">self</span>, item, spider)</span></span><span class="symbol">:</span></span><br><span class="line">        rating = item[<span class="string">'review_rating'</span>]</span><br><span class="line">        <span class="keyword">if</span> <span class="symbol">rating:</span></span><br><span class="line">            item[<span class="string">'review_rating'</span>] = <span class="keyword">self</span>.review_rating_map[rating]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure></p><p><code>items.py</code> 代码：<br><figure class="highlight monkey"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BookItem</span>(<span class="title">scrapy</span>.<span class="title">Item</span>):</span></span><br><span class="line">    name = scrapy.<span class="keyword">Field</span>()<span class="meta">   # 书名</span></span><br><span class="line">    price = scrapy.<span class="keyword">Field</span>()<span class="meta">  # 价格</span></span><br><span class="line">    review_rating = scrapy.<span class="keyword">Field</span>()<span class="meta">  # 评价等级，1~5星</span></span><br><span class="line">    review_num = scrapy.<span class="keyword">Field</span>()<span class="meta">     # 评价数量</span></span><br><span class="line">    upc = scrapy.<span class="keyword">Field</span>()<span class="meta">    # 产品编码</span></span><br><span class="line">    stock = scrapy.<span class="keyword">Field</span>()<span class="meta">  # 库存量</span></span><br></pre></td></tr></table></figure></p><p>运行命令，爬取数据：<br><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">F:\VirtualEnv\<span class="keyword">jobboleArticle\ArticleSpider\ArticleSpider</span></span><br><span class="line"><span class="keyword">(jobboleArticle) </span>$ <span class="keyword">scrapy </span>crawl <span class="keyword">books </span>-o .\data\<span class="keyword">books.toscrape.csv </span>--nolog</span><br><span class="line">(<span class="keyword">jobboleArticle) </span>$ cat -n .\data\<span class="keyword">books.toscrape.csv</span></span><br><span class="line"><span class="keyword">。。。。。。。。。。</span></span><br><span class="line"><span class="keyword">995 </span> e7469e<span class="number">22b</span><span class="number">5b</span>fb3e7,The Art of War,£<span class="number">33</span>.<span class="number">34</span>,<span class="number">15</span>,<span class="number">5</span>,<span class="number">0</span></span><br><span class="line"><span class="number">996</span>  dd047728de72ad62,The Artist<span class="string">'s Way: A Spiritual Path to Higher Creativity,£38.49,15,5,0</span></span><br><span class="line"><span class="string">997  efc3768127714ec3,The Bridge to Consciousness: I'</span>m Writing the <span class="keyword">Bridge </span><span class="keyword">Between </span><span class="keyword">Science </span><span class="keyword">and </span>Our Old <span class="keyword">and </span>New <span class="keyword">Beliefs.,£32.00,15,3,0</span></span><br><span class="line"><span class="keyword">998 </span> <span class="keyword">b12b89017878a60d,Private </span>Paris (Private <span class="comment">#10),£47.61,17,5,0</span></span><br><span class="line"><span class="number">999</span>  <span class="number">6</span>fd646a334e6e133,What<span class="string">'s It Like in Space?: Stories from Astronauts Who'</span>ve <span class="keyword">Been </span>There,£<span class="number">19</span>.<span class="number">60</span>,<span class="number">14</span>,<span class="number">2</span>,<span class="number">0</span></span><br><span class="line"><span class="number">1000</span>  c<span class="number">8f</span><span class="number">7f</span>0cb1abb9cac,Reasons to Stay Alive,£<span class="number">26</span>.<span class="number">41</span>,<span class="number">17</span>,<span class="number">2</span>,<span class="number">0</span></span><br><span class="line"><span class="number">1001</span>  <span class="keyword">b4fd5943413e089a,Slow </span>States of Collapse: Poems,£<span class="number">57</span>.<span class="number">31</span>,<span class="number">17</span>,<span class="number">3</span>,<span class="number">0</span></span><br></pre></td></tr></table></figure></p><h5 id="SQLite"><a href="#SQLite" class="headerlink" title="SQLite"></a>SQLite</h5><p>SQLite 是一个文件型轻量级数据库，它的处理速度很快，在数据量不是很大的情况<br>下，使用 SQLite 足够了。</p><p>首先，创建一个供 Scrapy 使用的 SQLite 数据库，取名为 scrapy.db，在客户端中创建数据表（Table）  ：</p><figure class="highlight lasso"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">F:\VirtualEnv\jobboleArticle\ArticleSpider\ArticleSpider\<span class="built_in">data</span></span><br><span class="line">(jobboleArticle) $ sqlite3 scrapy.db</span><br><span class="line">SQLite version <span class="number">3.20</span><span class="number">.1</span> <span class="number">2017</span><span class="number">-08</span><span class="number">-24</span> <span class="number">16</span>:<span class="number">21</span>:<span class="number">36</span></span><br><span class="line">Enter <span class="string">".help"</span> for usage hints.</span><br><span class="line">sqlite&gt; CREATE TABLE books(</span><br><span class="line">   <span class="params">...</span>&gt; upc             CHAR(<span class="number">16</span>) <span class="literal">NOT</span> <span class="built_in">NULL</span> PRIMARY KEY,</span><br><span class="line">   <span class="params">...</span>&gt; name            VARCHAR(<span class="number">256</span>) <span class="literal">NOT</span> <span class="built_in">NULL</span>,</span><br><span class="line">   <span class="params">...</span>&gt; price           VARCHAR(<span class="number">16</span>) <span class="literal">NOT</span> <span class="built_in">NULL</span>,</span><br><span class="line">   <span class="params">...</span>&gt; review_rating   INT,</span><br><span class="line">   <span class="params">...</span>&gt; review_num      INT,</span><br><span class="line">   <span class="params">...</span>&gt; stock           INT</span><br><span class="line">   <span class="params">...</span>&gt; );</span><br><span class="line">sqlite&gt;</span><br></pre></td></tr></table></figure><p>可以使用 Pycharm 查看创建的数据表：</p><p><img src="https://dongfei.oss-cn-shanghai.aliyuncs.com/blog/toscrape2.jpg" alt="toscrape2"></p><p>在 Python 中访问 SQLite 数据库可使用 Python 标准库中的 sqlite3 模块。下面是使用<br>sqlite3 模块将数据写入 SQLite 数据库的简单示例：</p><figure class="highlight vala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># -*- coding:utf-8 -*-</span></span><br><span class="line">__author__ = <span class="string">"東飛"</span></span><br><span class="line">__date__ = <span class="string">"2018-4-7"</span></span><br><span class="line"></span><br><span class="line">import sqlite3</span><br><span class="line"></span><br><span class="line"><span class="meta"># 连接数据库，得到 Connection 对象</span></span><br><span class="line">conn = sqlite3.connect(<span class="string">'../ArticleSpider/data/scrapy.db'</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta"># 创建 Curosr 对象，用来执行 SQL 语句</span></span><br><span class="line">cur = conn.cursor()</span><br><span class="line"></span><br><span class="line"><span class="meta"># 创建数据表</span></span><br><span class="line">cur.execute(<span class="string">'CREATE TABLE person (name VARCHAR(32),age INT,sex char(1))'</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta"># 插入一条数据</span></span><br><span class="line">cur.execute(<span class="string">'INSERT INTO person VALUES (?,?,?)'</span>, (<span class="string">'那小子真帅'</span>, <span class="number">22</span>, <span class="string">'M'</span>))</span><br><span class="line"></span><br><span class="line"><span class="meta"># 保存变更，commit 后数据才被实际写入数据库</span></span><br><span class="line">conn.commit()</span><br><span class="line"></span><br><span class="line"><span class="meta"># 关闭连接</span></span><br><span class="line">conn.close()</span><br></pre></td></tr></table></figure><p>运行上述代码后，查看数据库中数据是否以更新。</p><p><img src="https://dongfei.oss-cn-shanghai.aliyuncs.com/blog/toscrape3.jpg" alt="toscrape3"></p><p>了解了在 Python 中如何操作 SQLite 数据库后，按下来编写一个能将爬取到的数据写入 SQLite 数据库的 Item Pipeline 。在 <code>pipelines.py</code> 中实现 SQLitePipeline 的代码如下：<br><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将数据保存至 SQLite</span></span><br><span class="line">import sqlite3</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SQLitePipeline</span>(<span class="title">object</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open_spider</span><span class="params">(<span class="keyword">self</span>, spider)</span></span><span class="symbol">:</span></span><br><span class="line">        db_name = spider.custom_settings.get(<span class="string">'SQLITE_DB_NAME'</span>, <span class="string">'scrapy_default.db'</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">self</span>.db_conn = sqlite3.connect(db_name)</span><br><span class="line">        <span class="keyword">self</span>.db_cur = <span class="keyword">self</span>.db_conn.cursor()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span><span class="params">(<span class="keyword">self</span>, spider)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">self</span>.db_conn.commit()</span><br><span class="line">        <span class="keyword">self</span>.db_conn.close()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(<span class="keyword">self</span>, item, spider)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">self</span>.insert_db(item)</span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">insert_db</span><span class="params">(<span class="keyword">self</span>, item)</span></span><span class="symbol">:</span></span><br><span class="line">        values = (</span><br><span class="line">            item[<span class="string">'upc'</span>],</span><br><span class="line">            item[<span class="string">'name'</span>],</span><br><span class="line">            item[<span class="string">'price'</span>],</span><br><span class="line">            item[<span class="string">'review_rating'</span>],</span><br><span class="line">            item[<span class="string">'review_num'</span>],</span><br><span class="line">            item[<span class="string">'stock'</span>],</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        sql = <span class="string">'INSERT INTO books VALUES (?,?,?,?,?,?)'</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">self</span>.db_cur.execute(sql, values)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 每插入一条就 commit 一次会影响效率</span></span><br><span class="line">        <span class="comment"># self.db_conn.commit()</span></span><br></pre></td></tr></table></figure></p><p>解释上述代码如下：</p><ul><li>open_spider 方法</li></ul><p>在开始爬取数据之前被调用，在该方法中通过 spider.custom_settings 对象读取用户在配置中设定的数据库，然后建立与数据库的连接，将得到的 Connection 时象和 Cursor 对象分別赋值给 self.db_conn 和 self.db_cur，以便之后使用。</p><ul><li>process_item 方法</li></ul><p>处理爬取到的每一项数据，在该方法中调用 insert_db 方法，执行插入数据操作的 SQL 语句。但需要注意的是，在 insert_db 中并没有调用连接对象的 commit 方法，也就意味着此时数据并没有实际写入数据库。如果每插入一条数据都调用一次 commit 方法，会严重降低程序执行效率，并且我们对数据插入数据库的实时性并没有什么要求，因此可以在爬取完全部数据后再调用 commit 方法。</p><ul><li>close_spider 方法</li></ul><p>在爬取完全部数据后被调用，在该方法中，调用连接对象的 commit 方法将之前所有的插入数据操作一次性提交给数据库，然后关闭连接对象</p><p>在配置文件 <code>settings.py</code> 或者 <code>custom_settings</code> 中指定我们所要使用的 SQLite 数据库，并启用 SQLitePipeline ：</p><figure class="highlight sml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"ITEM_PIPELINES"</span>: &#123;</span><br><span class="line">            <span class="symbol">'ArticleSpider</span>.pipelines.<span class="type">SQLitePipeline'</span>: <span class="number">400</span>,</span><br><span class="line">        &#125;,</span><br><span class="line"><span class="string">"SQLITE_DB_NAME"</span>:<span class="symbol">'ArticleSpider</span>/data/scrapy.db',</span><br></pre></td></tr></table></figure><p>运行爬虫，并查看数据库:</p><figure class="highlight gherkin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">F:\VirtualEnv\jobboleArticle\ArticleSpider\ArticleSpider</span><br><span class="line">(jobboleArticle) $ sqlite3 .\data\scrapy.db</span><br><span class="line">SQLite version 3.20.1 2017-08-24 16:21:36</span><br><span class="line">Enter <span class="string">".help"</span> for usage hints.</span><br><span class="line">sqlite&gt; select count(<span class="symbol">*</span>) from books;</span><br><span class="line">1000</span><br><span class="line">sqlite&gt; select <span class="symbol">*</span> from books;</span><br><span class="line">9528d0948525bf5f|<span class="string">Birdsong: A Story in Pictures</span>|<span class="string">￡54.64</span>|<span class="string">3</span>|<span class="string">0</span>|19</span><br><span class="line">55f9da0c5eea2e10|<span class="string">You can't bury them all: Poems</span>|<span class="string">￡33.63</span>|<span class="string">2</span>|<span class="string">0</span>|17</span><br><span class="line">be5cc846f45496fb|<span class="string">Behind Closed Doors</span>|<span class="string">￡52.22</span>|<span class="string">4</span>|<span class="string">0</span>|18</span><br><span class="line">19ed25f4641d5efd|<span class="string">In a Dark, Dark Wood</span>|<span class="string">￡19.63</span>|<span class="string">1</span>|<span class="string">0</span>|18</span><br><span class="line">094b269567e1c300|<span class="string">Maude (1883-1993):She Grew Up with the country</span>|<span class="string">￡18.02</span>|<span class="string">2</span>|<span class="string">0</span>|18</span><br><span class="line">6be3beb0793a53e7|<span class="string">Sophie's World</span>|<span class="string">￡15.94</span>|<span class="string">5</span>|<span class="string">0</span>|18</span><br><span class="line">。。。。。。。。。</span><br></pre></td></tr></table></figure><p><img src="https://dongfei.oss-cn-shanghai.aliyuncs.com/blog/toscrape4.jpg" alt="toscrape4"></p><h5 id="MySQL"><a href="#MySQL" class="headerlink" title="MySQL"></a>MySQL</h5><p>MySQL 是一个应用极其广泛的关系型数据库，它是开源免费的，可以支持大型数据库。<br>使客户端登录 MySQL ，创建一个供 Scrapy 使用的数据库，取名为 scrapy.db ：<br><figure class="highlight oxygene"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">F:\VirtualEnv\jobboleArticle\ArticleSpider\ArticleSpider</span><br><span class="line">(jobboleArticle) $ mysql -hlocalhost -uroot -proot -P3308</span><br><span class="line">Welcome <span class="keyword">to</span> the MySQL monitor.  Commands <span class="keyword">end</span> <span class="keyword">with</span> ; <span class="keyword">or</span> \g.</span><br><span class="line">Your MySQL connection id <span class="keyword">is</span> <span class="number">3</span></span><br><span class="line">Server version: <span class="number">5.7</span>.<span class="number">19</span> MySQL Community Server (GPL)</span><br><span class="line"></span><br><span class="line">Copyright (c) <span class="number">2000</span>, <span class="number">2017</span>, Oracle <span class="keyword">and</span>/<span class="keyword">or</span> its affiliates. All rights reserved.</span><br><span class="line"></span><br><span class="line">Oracle <span class="keyword">is</span> a registered trademark <span class="keyword">of</span> Oracle Corporation <span class="keyword">and</span>/<span class="keyword">or</span> its</span><br><span class="line">affiliates. Other names may be trademarks <span class="keyword">of</span> their respective</span><br><span class="line">owners.</span><br><span class="line"></span><br><span class="line"><span class="keyword">Type</span> <span class="string">'help;'</span> <span class="keyword">or</span> <span class="string">'\h'</span> <span class="keyword">for</span> help. <span class="keyword">Type</span> <span class="string">'\c'</span> <span class="keyword">to</span> clear the current input statement.</span><br><span class="line">mysql&gt; <span class="keyword">CREATE</span> DATABASE scrapy_db CHARACTER <span class="keyword">SET</span> <span class="string">'utf8'</span> COLLATE <span class="string">'utf8_general_ci'</span>;</span><br><span class="line">Query OK, <span class="number">1</span> row affected (<span class="number">0.00</span> sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; USE scrapy_db;</span><br><span class="line">Database changed</span><br></pre></td></tr></table></figure></p><p>接下来，创建存储书籍数据的表：<br><figure class="highlight haml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; CREATE TABLE books(</span><br><span class="line">    -<span class="ruby">&gt; upc             CHAR(<span class="number">16</span>) NOT NULL PRIMARY KEY,</span></span><br><span class="line"><span class="ruby">    -&gt; name            VARCHAR(<span class="number">256</span>) NOT NULL,</span></span><br><span class="line"><span class="ruby">    -&gt; price           VARCHAR(<span class="number">16</span>) NOT NULL,</span></span><br><span class="line"><span class="ruby">    -&gt; review_rating   INT,</span></span><br><span class="line"><span class="ruby">    -&gt; review_num      INT,</span></span><br><span class="line"><span class="ruby">    -&gt; stock           INT</span></span><br><span class="line"><span class="ruby">    -&gt; );</span></span><br><span class="line"><span class="ruby">Query OK, <span class="number">0</span> rows affected (<span class="number">0</span>.<span class="number">01</span> sec)</span></span><br><span class="line"><span class="ruby"></span></span><br><span class="line"><span class="ruby">mysql&gt;</span></span><br></pre></td></tr></table></figure></p><p>在 <strong>Python 2</strong> 中访问 MySQL 数据库可以使用第三方库 MySQL-Python （即 MySQLdb ），但是 MySQLdb 不支持 <strong>Python 3</strong> 。在 <strong>Python 3</strong> 中，可以使用另一个第三方库 mysqlclient 作为替代，它是基于 MySQL-Python 开发的，提供了几乎完全相同的接口。在两个 Python 版本下，可以使用相同的代码访问 MySQL 。</p><p><strong>Python 2</strong> 使用 pip 安装 MySQL-python ：<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip <span class="keyword">install</span> My <span class="keyword">SQL</span>-python</span><br></pre></td></tr></table></figure></p><p><strong>Python 3</strong> 使用 pip 安装 mysqlclient ：<br><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip <span class="keyword">install</span> mysqlclient</span><br></pre></td></tr></table></figure></p><p>下面面是使用 MySQLdb 将数据写入 MySQL 数据库的简单示例，与 sqlite3的使用<br>乎完全相同 ：</p><figure class="highlight vala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># -*- coding:utf-8 -*-</span></span><br><span class="line">__author__ = <span class="string">"東飛"</span></span><br><span class="line">__date__ = <span class="string">"2018-4-7"</span></span><br><span class="line"></span><br><span class="line">import MySQLdb</span><br><span class="line"></span><br><span class="line"><span class="meta"># 连接数据库，得到 Connection 对象</span></span><br><span class="line">conn = MySQLdb.connect(host=<span class="string">'localhost'</span>, db=<span class="string">'scrapy_db'</span>, user=<span class="string">'root'</span>, passwd=<span class="string">'root'</span>, port=<span class="number">3308</span>, charset=<span class="string">'utf8'</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta"># 创建 Curosr 对象，用来执行 SQL 语句</span></span><br><span class="line">cur = conn.cursor()</span><br><span class="line"></span><br><span class="line"><span class="meta"># 创建数据表</span></span><br><span class="line">cur.execute(<span class="string">'CREATE TABLE person (name VARCHAR(32),age INT,sex char(1)) ENGINE=InnoDB DEFAULT CHARSET=utf8'</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta"># 插入一条数据</span></span><br><span class="line">cur.execute(<span class="string">'INSERT INTO person VALUES (%s,%s,%s)'</span>, (<span class="string">'那小子真帅'</span>, <span class="number">21</span>, <span class="string">'M'</span>))</span><br><span class="line"></span><br><span class="line"><span class="meta"># 保存变更，commit 后数据才被实际写入数据库</span></span><br><span class="line">conn.commit()</span><br><span class="line"></span><br><span class="line"><span class="meta"># 关闭连接</span></span><br><span class="line">conn.close()</span><br></pre></td></tr></table></figure><p>使用 Pycharm 查看创建的数据表：</p><p><img src="https://dongfei.oss-cn-shanghai.aliyuncs.com/blog/toscrape5.jpg" alt="toscrape5"></p><p>仿照 SQLitePipeline 实现 MysqlBookPipeline ，代码如下：</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数据保存至 Mysql 数据库</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MysqlBookPipeline</span>(<span class="title">object</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open_spider</span><span class="params">(<span class="keyword">self</span>, spider)</span></span><span class="symbol">:</span></span><br><span class="line">        db = spider.custom_settings.get(<span class="string">'SQLITE_DB_NAME'</span>, <span class="string">'scrapy_default'</span>)</span><br><span class="line">        host = spider.custom_settings.get(<span class="string">"MYSQL_HOST"</span>, <span class="string">"localhost"</span>)</span><br><span class="line">        port = spider.custom_settings.get(<span class="string">"MYSQL_PORT"</span>, <span class="number">3306</span>)</span><br><span class="line">        user = spider.custom_settings.get(<span class="string">"MYSQL_USER"</span>, <span class="string">"root"</span>)</span><br><span class="line">        passwd = spider.custom_settings.get(<span class="string">"MYSQL_PASSWORD"</span>, <span class="string">"root"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">self</span>.db_conn = MySQLdb.connect(host=host, port=port, db=db,</span><br><span class="line">                                       user=user, passwd=passwd, charset=<span class="string">'utf8'</span>)</span><br><span class="line">        <span class="keyword">self</span>.db_cur = <span class="keyword">self</span>.db_conn.cursor()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span><span class="params">(<span class="keyword">self</span>, spider)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">self</span>.db_conn.commit()</span><br><span class="line">        <span class="keyword">self</span>.db_conn.close()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(<span class="keyword">self</span>, item, spider)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">self</span>.insert_db(item)</span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">insert_db</span><span class="params">(<span class="keyword">self</span>, item)</span></span><span class="symbol">:</span></span><br><span class="line">        values = (</span><br><span class="line">            item[<span class="string">'upc'</span>],</span><br><span class="line">            item[<span class="string">'name'</span>],</span><br><span class="line">            item[<span class="string">'price'</span>],</span><br><span class="line">            item[<span class="string">'review_rating'</span>],</span><br><span class="line">            item[<span class="string">'review_num'</span>],</span><br><span class="line">            item[<span class="string">'stock'</span>],</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        sql = <span class="string">'INSERT INTO books VALUES (%s,%s,%s,%s,%s,%s)'</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">self</span>.db_cur.execute(sql, values)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 每插入一条就 commit 一次会影响效率</span></span><br><span class="line">        <span class="comment"># self.db_conn.commit()</span></span><br></pre></td></tr></table></figure><p>上述代码结构与 SQLitePipeline 完全相同，不再赘述。<br>在配置文件 <code>settings.py</code> 或者 <code>custom_settings</code> 中指定我们所要使用的 MySQL 数据库，并启用 MysqlBookPipeline ：</p><figure class="highlight xquery"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">custom_settings = &#123;</span><br><span class="line">        <span class="string">"ITEM_PIPELINES"</span>: &#123;</span><br><span class="line">            <span class="string">'ArticleSpider.pipelines.BookPipeline'</span>: <span class="number">300</span>,</span><br><span class="line">            <span class="string">'ArticleSpider.pipelines.MysqlBookPipeline'</span>: <span class="number">400</span>,</span><br><span class="line">        &#125;, </span><br><span class="line">        <span class="string">"MYSQL_DB_NAME"</span>: <span class="string">'scrapy_db'</span>,</span><br><span class="line">        <span class="string">"MYSQL_HOST"</span>: <span class="string">"localhost"</span>,</span><br><span class="line">        <span class="string">"MYSQL_PORT"</span>: <span class="number">3308</span>,</span><br><span class="line">        <span class="string">"MYSQL_USER"</span>: <span class="string">"root"</span>,</span><br><span class="line">        <span class="string">"MYSQL_PASSWORD"</span>: <span class="string">"root"</span>,</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>运行爬虫，并查看数据库：<br><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">F:\VirtualEnv\jobboleArticle\ArticleSpider\ArticleSpider</span><br><span class="line">(jobboleArticle) $ scrapy crawl books</span><br><span class="line">。。。。。。。。。。 </span><br><span class="line">(jobboleArticle) $ mysql -hlocalhost -uroot -proot -P3308 scrapy<span class="emphasis">_db</span></span><br><span class="line"><span class="emphasis">Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.</span></span><br><span class="line"><span class="emphasis"></span></span><br><span class="line"><span class="emphasis">mysql&gt; select count(*) from books;</span></span><br><span class="line"><span class="emphasis">+----------+</span></span><br><span class="line"><span class="emphasis">| count(*) |</span></span><br><span class="line"><span class="emphasis">+----------+</span></span><br><span class="line"><span class="emphasis">|     1000 |</span></span><br><span class="line"><span class="emphasis">+----------+</span></span><br><span class="line"><span class="emphasis">1 row in set (0.00 sec)</span></span><br><span class="line"><span class="emphasis"></span></span><br><span class="line"><span class="emphasis">mysql&gt; select name from books;</span></span><br><span class="line"><span class="emphasis">| name</span></span><br><span class="line"><span class="emphasis">                                                                        |</span></span><br><span class="line"><span class="emphasis">+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+</span></span><br><span class="line"><span class="emphasis">| A Light in the Attic</span></span><br><span class="line"><span class="emphasis">                                                                        |</span></span><br><span class="line"><span class="emphasis">| Our Band Could Be Your Life: Scenes from the American Indie Underground, 1981-1991</span></span><br><span class="line"><span class="emphasis">                                                                        |</span></span><br><span class="line"><span class="emphasis">| It's Only the Himalayas</span></span><br><span class="line"><span class="emphasis">                                                                        |</span></span><br><span class="line"><span class="emphasis">| Scott Pilgrim's Precious Little Life (Scott Pilgrim #1)</span></span><br><span class="line"><span class="emphasis">                                                                        |</span></span><br><span class="line"><span class="emphasis">| Mesaerion: The Best Science Fiction Stories 1800-1849</span></span><br><span class="line"><span class="emphasis">                                                                        |</span></span><br><span class="line"><span class="emphasis">| Rip it Up and Start Again</span></span><br><span class="line"><span class="emphasis">                                                                        |</span></span><br><span class="line"><span class="emphasis">| Libertarianism for Beginners</span></span><br><span class="line"><span class="emphasis">                                                                        |</span></span><br><span class="line"><span class="emphasis">| Olio</span></span><br></pre></td></tr></table></figure></p><p>上述代码中，同样是先执行完全部的插入语句 <code>INSERT INTO</code>，最后一次性调用 commit 方法提交给数据库。或许在某些情况下，我们的确需要每执行一条插入语句，就立即调用 commit 方法更新数据库，如爬取过程很长，中途可能被迫中断，这样程序就不能执行到最后的 commit 。如果在上述代码的 <code>insert_db</code> 方法中直接添加 <code>self.db_conn.commit()</code> 又会使程序执行变慢。为解决以上难题，下面讲解另一种实现方法。</p><p>Scrapy 框架自身是使用另一个 Python 框架 Twisted 编写的程序，Twisted 是一个事件驱动型的异步网络框架，鼓励用户编写异步代码，Twisted 中提供了以异步方式多线程访问数据库的模块 adbapi，使用该模块可以显著提高程序访问数据库的效率，下面是使用 adbpi 中的连接池访问 MySQL 数据库的简单示例 ：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line">__author__ = <span class="string">"東飛"</span></span><br><span class="line">__date__ = <span class="string">"2018-4-7"</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> twisted.internet <span class="keyword">import</span> reactor, defer</span><br><span class="line"><span class="keyword">from</span> twisted.enterprise <span class="keyword">import</span> adbapi</span><br><span class="line"><span class="keyword">import</span> threading</span><br><span class="line"></span><br><span class="line">dbpool = adbapi.ConnectionPool(<span class="string">'MySQLdb'</span>, host=<span class="string">'localhost'</span>, database=<span class="string">'scrapy_db'</span>, user=<span class="string">'root'</span>, passwd=<span class="string">'root'</span>, port=<span class="number">3308</span>,</span><br><span class="line">                               charset=<span class="string">'utf8'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">insert_db</span><span class="params">(tx, item)</span>:</span></span><br><span class="line">    print(<span class="string">'In Thread: '</span>, threading.get_ident())</span><br><span class="line">    sql = <span class="string">'INSERT INTO person VALUES (%s,%s,%s)'</span></span><br><span class="line">    tx.execute(sql, item)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">    item = (<span class="string">'person%s'</span> % i, <span class="number">25</span>, <span class="string">'M'</span>)</span><br><span class="line">    dbpool.runInteraction(insert_db, item)</span><br><span class="line"></span><br><span class="line">reactor.run()</span><br></pre></td></tr></table></figure><p>上述代码解释如下：</p><ul><li>adbapi.ConnectionPool 方法</li></ul><p>可以创建一个数据库连接池对象，其中包含多个连接对象，每个连接对象在独立的线程中工作。adbapi 只是提供了异步访问数据库的编程框架，在其内部依然使用 MySQLdb、sqlite3 这样的库访问数据库，ConnectionPool 方法的第一个参数就是用来指定使用哪个库访问数据库，其他参数在创建连接对象时使用。</p><ul><li>dbpool.runInteraction(insert_db, item)</li></ul><p>以异步方式调用 instert_db 函數，dbpool 会选择连接池中的一个连接对象在独立线程中调用 insert_db ，其中参数 item 会被传给 insert_db 的第二个参数，传给 insert_db 的第一个参数是一个 Transaction 对象，其接口与 Cursor 对象类似，可以调用 execute 方法执行 SQL 语句，insert_db 执行完后，连接对象会自动调用 commit 方法。 </p><p>了解了 adbapi 的使用后，实现 MysqlBookAsyncPipeline，代码如下:<br><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数据异步存储至 Mysql 数据库</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MysqlBookAsyncPipeline</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open_spider</span><span class="params">(<span class="keyword">self</span>, spider)</span></span><span class="symbol">:</span></span><br><span class="line">        db = spider.custom_settings.get(<span class="string">'MYSQL_DB_NAME'</span>, <span class="string">'scrapy_default'</span>)</span><br><span class="line">        host = spider.custom_settings.get(<span class="string">"MYSQL_HOST"</span>, <span class="string">"localhost"</span>)</span><br><span class="line">        port = spider.custom_settings.get(<span class="string">"MYSQL_PORT"</span>, <span class="number">3306</span>)</span><br><span class="line">        user = spider.custom_settings.get(<span class="string">"MYSQL_USER"</span>, <span class="string">"root"</span>)</span><br><span class="line">        passwd = spider.custom_settings.get(<span class="string">"MYSQL_PASSWORD"</span>, <span class="string">"root"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">self</span>.dbpool = adbapi.ConnectionPool(<span class="string">'MySQLdb'</span>, host=host, port=port, db=db,</span><br><span class="line">                                            user=user, passwd=passwd, charset=<span class="string">'utf8'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span><span class="params">(<span class="keyword">self</span>, spider)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">self</span>.dbpool.close()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(<span class="keyword">self</span>, item, spider)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">self</span>.dbpool.runInteraction(<span class="keyword">self</span>.insert_db, item)</span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">insert_db</span><span class="params">(<span class="keyword">self</span>, tx,item)</span></span><span class="symbol">:</span></span><br><span class="line">        values = (</span><br><span class="line">            item[<span class="string">'upc'</span>],</span><br><span class="line">            item[<span class="string">'name'</span>],</span><br><span class="line">            item[<span class="string">'price'</span>],</span><br><span class="line">            item[<span class="string">'review_rating'</span>],</span><br><span class="line">            item[<span class="string">'review_num'</span>],</span><br><span class="line">            item[<span class="string">'stock'</span>],</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        sql = <span class="string">'INSERT INTO books VALUES (%s,%s,%s,%s,%s,%s)'</span></span><br><span class="line"></span><br><span class="line">        tx.execute(sql, values)</span><br></pre></td></tr></table></figure></p><h5 id="MongoDB"><a href="#MongoDB" class="headerlink" title="MongoDB"></a>MongoDB</h5><p>MongoDB 是一个面向文档的非关系型数据库（ NoSQL ），功能强大、灵活、易于拓展。<br><a href="https://blog.wuwii.com/install-mongodb.html" target="_blank" rel="noopener">安装 MongoDB 数据库</a> </p><p>启动 MongoDB 服务 ：<code>net start mongodb</code></p><p>在 Python 中可以使用第三方库 pymongo 访问 MongoDB 数据库，使用 pip 安装 pymongo ： <code>pip install pymongo</code> </p><p>下面是使用 pymongo 将数据写入 MongoDB 数据库的简单示例:</p><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line">__author__ = <span class="string">"東飛"</span></span><br><span class="line">__date__ = <span class="string">"2018-4-7"</span></span><br><span class="line"></span><br><span class="line">from pymongo import MongoClient</span><br><span class="line"></span><br><span class="line"><span class="comment"># 连接 MongoDB，得到一个客户端对象</span></span><br><span class="line"></span><br><span class="line">client = MongoClient('mongodb://localhost:27017')</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取名为 scrapy_db 的数据库的对象</span></span><br><span class="line">db = client.scrapy_db</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取名为 person 的集合的对象</span></span><br><span class="line">collection = db.person</span><br><span class="line"></span><br><span class="line">doc = &#123;</span><br><span class="line">    'name': '那小子真帅',</span><br><span class="line">    'age': 21,</span><br><span class="line">    'sex': 'M',</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将文档插入集合</span></span><br><span class="line">collection.insert_one(doc)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 关闭客户端</span></span><br><span class="line">client.close()</span><br></pre></td></tr></table></figure><p>Pycharm 安装 MongoDB 插件 ：</p><p>参照：<a href="https://blog.csdn.net/qq_24189933/article/details/75664743" target="_blank" rel="noopener">Pycharm 配置可视化 Mongodb 工具</a></p><p><img src="https://dongfei.oss-cn-shanghai.aliyuncs.com/blog/toscrape6.jpg" alt="toscrape6"></p><p>连接 MongoDB 数据库 ：</p><p><img src="https://dongfei.oss-cn-shanghai.aliyuncs.com/blog/toscrape7.jpg" alt="toscrape7"></p><p>查看数据 ：</p><p><img src="https://dongfei.oss-cn-shanghai.aliyuncs.com/blog/toscrape8.jpg" alt="toscrape8"></p><p>实现 MongoDBPipeline 代码如下：</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数据存储至 MongoDB 数据库</span></span><br><span class="line">from pymongo import MongoClient</span><br><span class="line">from scrapy import Item</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MongoDBPipeline</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open_spider</span><span class="params">(<span class="keyword">self</span>, spider)</span></span><span class="symbol">:</span></span><br><span class="line">        db_url = spider.custom_settings.get(<span class="string">'MONGODB_URL'</span>, <span class="string">'mongodb://localhost:27017'</span>)</span><br><span class="line">        db_name = spider.custom_settings.get(<span class="string">"MONGODB_DB_NAME"</span>, <span class="string">"scrapy_default"</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">self</span>.db_client = MongoClient(db_url)</span><br><span class="line">        <span class="keyword">self</span>.db = <span class="keyword">self</span>.db_client[db_name]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span><span class="params">(<span class="keyword">self</span>, spider)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">self</span>.db_client.close()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(<span class="keyword">self</span>, item, spider)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">self</span>.insert_db(item)</span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">insert_db</span><span class="params">(<span class="keyword">self</span>, item)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">if</span> isinstance(item,Item)<span class="symbol">:</span></span><br><span class="line">            item = dict(item)</span><br><span class="line">            </span><br><span class="line">        <span class="keyword">self</span>.db.books.insert_one(item)</span><br></pre></td></tr></table></figure><p>解释上述代码如下：</p><ul><li>open_spider 方法</li></ul><p>在开始爬取数据之前被调用，在该方法中通过 spider.custom_settings 或者 spider.settings 对象读取用户在配置文件中指定的数据库，然后建立与数据库的连接，将得到的 MongoClient 对象和 Database 对象分别赋值给 self.db_client 和 self.db，以便之后使用。</p><ul><li>process_item 方法</li></ul><p>处理爬取到的每一项数据，在该方法中调用 insert_db 方法，执行数据库的插入操作，在 insert_db 方法中，先将一项数据转换成字典，然后调用 insert_one 方法将其插入集合 books 。</p><ul><li>close spider 方法</li></ul><p>在爬取完全部数据后被调用，在该方法中关闭与数据库的连接。</p><p>在配置文件 <code>settings.py</code> 或者 <code>custom_settings</code> 中指定所要使用的 MongoDB 数据库，并启用 MongoDBPipeline ：</p><figure class="highlight xquery"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">custom_settings = &#123;</span><br><span class="line">        <span class="string">"ITEM_PIPELINES"</span>: &#123;</span><br><span class="line">            <span class="string">'ArticleSpider.pipelines.BookPipeline'</span>: <span class="number">300</span>,</span><br><span class="line">            <span class="string">'ArticleSpider.pipelines.MongoDBPipeline'</span>: <span class="number">403</span>,</span><br><span class="line">        &#125;, </span><br><span class="line">        <span class="string">'MONGODB_URL'</span>: <span class="string">'mongodb://localhost:27017'</span>,</span><br><span class="line">        <span class="string">'MONGODB_DB_NAME'</span>: <span class="string">'scrapy_db'</span>,</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>运行爬虫，井查看数据库 ：<br><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">F:\VirtualEnv\jobboleArticle\ArticleSpider\ArticleSpider</span><br><span class="line">(jobboleArticle) $ scrapy crawl books</span><br><span class="line"></span><br><span class="line">。。。。。。。。。。</span><br><span class="line"></span><br><span class="line">(jobboleArticle) $ mongo scrapy_db</span><br><span class="line">MongoDB <span class="keyword">shell</span> <span class="keyword">version</span> v3.<span class="number">4.9</span></span><br><span class="line">connecting <span class="keyword">to</span>: mongod<span class="variable">b:</span>//<span class="number">127.0</span>.<span class="number">0.1</span>:<span class="number">27017</span>/scrapy_db</span><br><span class="line">MongoDB server <span class="keyword">version</span>: <span class="number">3.4</span>.<span class="number">9</span></span><br><span class="line">Server <span class="built_in">has</span> startup warning<span class="variable">s:</span></span><br><span class="line"><span class="number">2018</span>-<span class="number">04</span>-<span class="number">07</span>T19:<span class="number">34</span>:<span class="number">13.576</span>+<span class="number">0800</span> I CONTROL  [initandlisten]</span><br><span class="line"><span class="number">2018</span>-<span class="number">04</span>-<span class="number">07</span>T19:<span class="number">34</span>:<span class="number">13.576</span>+<span class="number">0800</span> I CONTROL  [initandlisten] ** WARNING: Access control <span class="keyword">is</span> not enabled <span class="keyword">for</span> the database.</span><br><span class="line"><span class="number">2018</span>-<span class="number">04</span>-<span class="number">07</span>T19:<span class="number">34</span>:<span class="number">13.576</span>+<span class="number">0800</span> I CONTROL  [initandlisten] **          Read <span class="built_in">and</span> <span class="keyword">write</span> access <span class="keyword">to</span> data <span class="built_in">and</span> configuration <span class="keyword">is</span> unrestricted.</span><br><span class="line"><span class="number">2018</span>-<span class="number">04</span>-<span class="number">07</span>T19:<span class="number">34</span>:<span class="number">13.576</span>+<span class="number">0800</span> I CONTROL  [initandlisten]</span><br><span class="line">&gt; db.books.<span class="built_in">count</span>()</span><br><span class="line"><span class="number">1000</span></span><br><span class="line">&gt; db.books.<span class="keyword">find</span>()</span><br><span class="line">&#123; <span class="string">"_id"</span> : ObjectId(<span class="string">"5ac8ba021e885105bc184e6e"</span>), <span class="string">"name"</span> : <span class="string">"Tipping the Velvet"</span>, <span class="string">"price"</span> : <span class="string">"£53.74"</span>, <span class="string">"review_rating"</span> : <span class="number">1</span>, <span class="string">"upc"</span> : <span class="string">"90fa61229261140a"</span>, <span class="string">"stock"</span> : <span class="string">"20"</span>, <span class="string">"review_num"</span> : <span class="string">"0"</span> &#125;</span><br><span class="line">&#123; <span class="string">"_id"</span> : ObjectId(<span class="string">"5ac8ba021e885105bc184e6f"</span>), <span class="string">"name"</span> : <span class="string">"Sapiens: A Brief History of Humankind"</span>, <span class="string">"price"</span> : <span class="string">"£54.23"</span>, <span class="string">"review_rating"</span> : <span class="number">5</span>, <span class="string">"upc"</span> : <span class="string">"4165285e1663650f"</span>, <span class="string">"stock"</span> : <span class="string">"20"</span>, <span class="string">"review_num"</span> : <span class="string">"0"</span> &#125;</span><br><span class="line">&#123; <span class="string">"_id"</span> : ObjectId(<span class="string">"5ac8ba021e885105bc184e70"</span>), <span class="string">"name"</span> : <span class="string">"The Dirty Little Secrets of Getting Your Dream Job"</span>, <span class="string">"price"</span> : <span class="string">"£33.34"</span>, <span class="string">"review_rating"</span> : <span class="number">4</span>, <span class="string">"upc"</span> : <span class="string">"2597b5a345f45e1b"</span>, <span class="string">"stock"</span> : <span class="string">"19"</span>, <span class="string">"review_num"</span> : <span class="string">"0"</span> &#125;</span><br><span class="line">&#123; <span class="string">"_id"</span> : ObjectId(<span class="string">"5ac8ba021e885105bc184e71"</span>), <span class="string">"name"</span> : <span class="string">"The Requiem Red"</span>, <span class="string">"price"</span> : <span class="string">"£22.65"</span>, <span class="string">"review_rating"</span> : <span class="number">1</span>, <span class="string">"upc"</span> : <span class="string">"f77dbf2323deb740"</span>, <span class="string">"stock"</span> : <span class="string">"19"</span>, <span class="string">"review_num"</span> : <span class="string">"0"</span> &#125;</span><br><span class="line">&#123; <span class="string">"_id"</span> : ObjectId(<span class="string">"5ac8ba021e885105bc184e72"</span>), <span class="string">"name"</span> : <span class="string">"Soumission"</span>, <span class="string">"price"</span> : <span class="string">"£50.10"</span>, <span class="string">"review_rating"</span> : <span class="number">1</span>, <span class="string">"upc"</span> : <span class="string">"6957f44c3847a760"</span>, <span class="string">"stock"</span> : <span class="string">"20"</span>, <span class="string">"review_num"</span> : <span class="string">"0"</span> &#125;</span><br><span class="line">。。。。。。。。。。</span><br></pre></td></tr></table></figure></p><p><img src="https://dongfei.oss-cn-shanghai.aliyuncs.com/blog/toscrape9.jpg" alt="toscrape9"></p><h5 id="Redis"><a href="#Redis" class="headerlink" title="Redis"></a>Redis</h5><p>Redis 是一个使用 ANSI C 编写的高性能 Key-Value 数据库，使用内存作为主存储，内存中的数据也可以被持久化到硬盘。</p><p>在 Python 中可以使用第三方库 redis-py 访问 Redis 数据库，使用 pip 安装 redis-py ：<code>pip install redis</code></p><p>下面是使用 redis-py 将数据写入 Redis 数据库的简单示例：</p><figure class="highlight vala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># -*- coding:utf-8 -*-</span></span><br><span class="line">__author__ = <span class="string">"東飛"</span></span><br><span class="line">__date__ = <span class="string">"2018-4-7"</span></span><br><span class="line"></span><br><span class="line">import redis</span><br><span class="line"></span><br><span class="line"><span class="meta"># 连接数据库</span></span><br><span class="line">r = redis.StrictRedis(host=<span class="string">'localhost'</span>, port=<span class="number">6379</span>, db=<span class="number">0</span>, password=<span class="string">'root'</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta"># 创建 3 条数据</span></span><br><span class="line">person1 = &#123;</span><br><span class="line">    <span class="string">'name'</span>: <span class="string">'那小子真帅'</span>,</span><br><span class="line">    <span class="string">'age'</span>: <span class="number">21</span>,</span><br><span class="line">    <span class="string">'sex'</span>: <span class="string">'M'</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">person2 = &#123;</span><br><span class="line">    <span class="string">'name'</span>: <span class="string">'卖姑娘的小火柴'</span>,</span><br><span class="line">    <span class="string">'age'</span>: <span class="number">22</span>,</span><br><span class="line">    <span class="string">'sex'</span>: <span class="string">'M'</span>,</span><br><span class="line">&#125;</span><br><span class="line">person3 = &#123;</span><br><span class="line">    <span class="string">'name'</span>: <span class="string">'Hello'</span>,</span><br><span class="line">    <span class="string">'age'</span>: <span class="number">23</span>,</span><br><span class="line">    <span class="string">'sex'</span>: <span class="string">'M'</span>,</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta"># 将 3 条数据以 Hash 类型(哈希) 保存到 Redis 中</span></span><br><span class="line">r.hmset(<span class="string">'person:1'</span>, person1)</span><br><span class="line">r.hmset(<span class="string">'person:2'</span>, person2)</span><br><span class="line">r.hmset(<span class="string">'person:3'</span>, person3)</span><br><span class="line"></span><br><span class="line"><span class="meta"># 关闭连接</span></span><br><span class="line">r.connection_pool.disconnect()</span><br></pre></td></tr></table></figure><p>Redis 是 Key-Value 数据库，一项数据在数据库中就是一个键值对，存储多项同类的数据时（如 Book ），通常以 <code>item:id</code> 这样的形式作为每项数据的键，其中的  <strong>:</strong> 并没有什么特殊，也可以换成 <strong>.</strong> 或 <strong>1</strong> 等，只是大家习惯这样使用。</p><p>查看 Redis 中的数据：</p><figure class="highlight taggerscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">D:<span class="symbol">\P</span>rogram Files<span class="symbol">\R</span>edisWindows</span><br><span class="line">$ redis-cli -a root</span><br><span class="line">127.0.0.1:6379&gt; KEYS person:*</span><br><span class="line">1) "person:2"</span><br><span class="line">2) "person:3"</span><br><span class="line">3) "person:1"</span><br><span class="line">127.0.0.1:6379&gt; HGETALL person:1</span><br><span class="line">1) "name"</span><br><span class="line">2) "<span class="symbol">\x</span>e9<span class="symbol">\x</span>82<span class="symbol">\x</span>a3<span class="symbol">\x</span>e5<span class="symbol">\x</span>b0<span class="symbol">\x</span>8f<span class="symbol">\x</span>e5<span class="symbol">\x</span>ad<span class="symbol">\x</span>90<span class="symbol">\x</span>e7<span class="symbol">\x</span>9c<span class="symbol">\x</span>9f<span class="symbol">\x</span>e5<span class="symbol">\x</span>b8<span class="symbol">\x</span>85"</span><br><span class="line">3) "age"</span><br><span class="line">4) "21"</span><br><span class="line">5) "sex"</span><br><span class="line">6) "M"</span><br><span class="line">127.0.0.1:6379&gt;</span><br></pre></td></tr></table></figure><p>实现 RedisPipeline 代码如下：</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数据存储至 Redis 数据库</span></span><br><span class="line">import redis</span><br><span class="line">from scrapy import Item</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RedisPipeline</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open_spider</span><span class="params">(<span class="keyword">self</span>, spider)</span></span><span class="symbol">:</span></span><br><span class="line">        db_host = spider.custom_settings.get(<span class="string">'REDIS_HOST'</span>, <span class="string">'localhost'</span>)</span><br><span class="line">        db_port = spider.custom_settings.get(<span class="string">"REDIS_PORT"</span>, <span class="number">6379</span>)</span><br><span class="line">        db_passwd = spider.custom_settings.get(<span class="string">"REDIS_PASSWORD"</span>, <span class="string">'root'</span>)</span><br><span class="line">        db_index = spider.custom_settings.get(<span class="string">"REDIS_DB_INDEX"</span>, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">self</span>.db_conn = redis.StrictRedis(host=db_host, port=db_port, db=db_index, password=db_passwd)</span><br><span class="line">        <span class="keyword">self</span>.item_i = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span><span class="params">(<span class="keyword">self</span>, spider)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">self</span>.db_conn.connection_pool.disconnect()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(<span class="keyword">self</span>, item, spider)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">self</span>.insert_db(item)</span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">insert_db</span><span class="params">(<span class="keyword">self</span>, item)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">if</span> isinstance(item, Item)<span class="symbol">:</span></span><br><span class="line">            item = dict(item)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">self</span>.item_i += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">self</span>.db_conn.hmset(<span class="string">'book:%s'</span> % <span class="keyword">self</span>.item_i, item)</span><br></pre></td></tr></table></figure><p>解释上述代码如下:</p><ul><li>open_spider 方法</li></ul><p>在开始爬取数据之前被调用。在该方法中通过 spider.custom_settings 或者 spider.settings 对象读取用户在配置文件中指定的数据库，然后建立与数据库的连接，将得到的连接对象<br>賦值给 self.db_conn ，以便之后使用，并初始化一个 self.item_i 作为每项数据的 id 。在插入一项数据时，使用 self.item_i 自加的结果构造数据在数据库中的键。</p><ul><li>process_item 方法</li></ul><p>处理爬取到的每一项数据，在该方法中调用 insert_db 方法，执行数据库的插入操作，在 insert_db 方法中，先将每一项数据转换成字典，然后调用 hmset 方法将数据以 Hash 类型存入 Redis 数据库。 </p><ul><li>close_spider 方法</li></ul><p>在爬取完全部数据后被调用，在该方法中关闭与数据库的连接。</p><p>在配置文件 <code>settings.py</code> 或者 <code>custom_settings</code> 中指定所要使用的 MongoDB 数据库，并启用 RedisPipeline ：</p><figure class="highlight xquery"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">custom_settings = &#123;</span><br><span class="line">        <span class="string">"ITEM_PIPELINES"</span>: &#123;</span><br><span class="line">            <span class="string">'ArticleSpider.pipelines.BookPipeline'</span>: <span class="number">300</span>,</span><br><span class="line">            <span class="string">'ArticleSpider.pipelines.RedisPipeline'</span>: <span class="number">404</span>,</span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="string">'REDIS_HOST'</span>: <span class="string">'localhost'</span>,</span><br><span class="line">        <span class="string">"REDIS_PORT"</span>: <span class="number">6379</span>,</span><br><span class="line">        <span class="string">"REDIS_PASSWORD"</span>: <span class="string">'root'</span>,</span><br><span class="line">        <span class="string">"REDIS_DB_INDEX"</span>: <span class="number">0</span>,</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p><a href="https://blog.csdn.net/babados/article/details/78575145" target="_blank" rel="noopener">Pycharm 集成 Redis 可视化插件 Iedis</a></p><p>运行爬虫，井查看数据库 ：<br><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="symbol">F:</span>\VirtualEnv\jobboleArticle\ArticleSpider\ArticleSpider</span><br><span class="line">(jobboleArticle) $ scrapy crawl books</span><br><span class="line"></span><br><span class="line">。。。。。。。。。。</span><br><span class="line"></span><br><span class="line"><span class="symbol">D:</span>\Program Files\RedisWindows</span><br><span class="line">$ redis-cli -a root</span><br><span class="line"><span class="meta">127.0.0.1:6379&gt;</span> KEYS <span class="symbol">book:</span>*</span><br><span class="line">。。。。。。。。。。</span><br><span class="line"><span class="number">994</span>) <span class="string">"book:235"</span></span><br><span class="line"><span class="number">995</span>) <span class="string">"book:903"</span></span><br><span class="line"><span class="number">996</span>) <span class="string">"book:244"</span></span><br><span class="line"><span class="number">997</span>) <span class="string">"book:543"</span></span><br><span class="line"><span class="number">998</span>) <span class="string">"book:293"</span></span><br><span class="line"><span class="number">999</span>) <span class="string">"book:440"</span></span><br><span class="line"><span class="number">1000</span>) <span class="string">"book:729"</span></span><br><span class="line"><span class="meta">127.0.0.1:6379&gt;</span> HGETALL <span class="symbol">book:</span><span class="number">1</span></span><br><span class="line"> <span class="number">1</span>) <span class="string">"name"</span></span><br><span class="line"> <span class="number">2</span>) <span class="string">"Starving Hearts (Triangular Trade Trilogy, #1)"</span></span><br><span class="line"> <span class="number">3</span>) <span class="string">"price"</span></span><br><span class="line"> <span class="number">4</span>) <span class="string">"\xc2\xa313.99"</span></span><br><span class="line"> <span class="number">5</span>) <span class="string">"review_rating"</span></span><br><span class="line"> <span class="number">6</span>) <span class="string">"2"</span></span><br><span class="line"> <span class="number">7</span>) <span class="string">"upc"</span></span><br><span class="line"> <span class="number">8</span>) <span class="string">"0312262ecafa5a40"</span></span><br><span class="line"> <span class="number">9</span>) <span class="string">"stock"</span></span><br><span class="line"><span class="number">10</span>) <span class="string">"19"</span></span><br><span class="line"><span class="number">11</span>) <span class="string">"review_num"</span></span><br><span class="line"><span class="number">12</span>) <span class="string">"0"</span></span><br><span class="line"><span class="meta">127.0.0.1:6379&gt;</span> HGETALL <span class="symbol">book:</span><span class="number">2</span></span><br><span class="line"> <span class="number">1</span>) <span class="string">"price"</span></span><br><span class="line"> <span class="number">2</span>) <span class="string">"\xc2\xa317.93"</span></span><br><span class="line"> <span class="number">3</span>) <span class="string">"upc"</span></span><br><span class="line"> <span class="number">4</span>) <span class="string">"e72a5dfc7e9267b2"</span></span><br><span class="line"> <span class="number">5</span>) <span class="string">"review_rating"</span></span><br><span class="line"> <span class="number">6</span>) <span class="string">"3"</span></span><br><span class="line"> <span class="number">7</span>) <span class="string">"review_num"</span></span><br><span class="line"> <span class="number">8</span>) <span class="string">"0"</span></span><br><span class="line"> <span class="number">9</span>) <span class="string">"name"</span></span><br><span class="line"><span class="number">10</span>) <span class="string">"The Coming Woman: A Novel Based on the Life of the Infamous Feminist, Victoria Woodhull"</span></span><br><span class="line"><span class="number">11</span>) <span class="string">"stock"</span></span><br><span class="line"><span class="number">12</span>) <span class="string">"19"</span></span><br><span class="line"><span class="meta">127.0.0.1:6379&gt;</span></span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;h5 id=&quot;数据保存至数据库&quot;&gt;&lt;a href=&quot;#数据保存至数据库&quot; class=&quot;headerlink&quot; title=&quot;数据保存至数据库&quot;&gt;&lt;/a&gt;数据保存至数据库&lt;/h5&gt;&lt;p&gt;以 &lt;code&gt;toscrape_book&lt;/code&gt; 项目作为环境，使用 Item Pipeline 实现 Scrapy 爬虫，将爬取到的数据存储到数据库中。爬取网站 &lt;a href=&quot;http://books.toscrape.com/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;http://books.toscrape.com/&lt;/a&gt; 中的书籍信息，其中每一本书的信息包括：&lt;br&gt;&lt;code&gt;书名&lt;/code&gt;、&lt;code&gt;价格&lt;/code&gt;、&lt;code&gt;评价等级&lt;/code&gt;、&lt;code&gt;产品编码&lt;/code&gt;、&lt;code&gt;库存量&lt;/code&gt;、&lt;code&gt;评价数量&lt;/code&gt;。&lt;br&gt;&lt;img src=&quot;https://dongfei.oss-cn-shanghai.aliyuncs.com/blog/toscrape1.jpg&quot; alt=&quot;toscrape1&quot;&gt;&lt;br&gt;
    
    </summary>
    
      <category term="Python" scheme="http://blog.dongfei.xin/categories/Python/"/>
    
      <category term="Scrapy" scheme="http://blog.dongfei.xin/categories/Python/Scrapy/"/>
    
    
      <category term="Scrapy" scheme="http://blog.dongfei.xin/tags/Scrapy/"/>
    
  </entry>
  
  <entry>
    <title>数据分析基础</title>
    <link href="http://blog.dongfei.xin/2018-04-01/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%9F%BA%E7%A1%80/"/>
    <id>http://blog.dongfei.xin/2018-04-01/数据分析基础/</id>
    <published>2018-04-01T02:10:51.000Z</published>
    <updated>2018-05-01T02:45:28.734Z</updated>
    
    <content type="html"><![CDATA[<h5 id="工具篇"><a href="#工具篇" class="headerlink" title="工具篇"></a>工具篇</h5><h6 id="Anaconda"><a href="#Anaconda" class="headerlink" title="Anaconda"></a>Anaconda</h6><p><a href="https://www.zhihu.com/question/58033789" target="_blank" rel="noopener">初学 Python 者自学 Anaconda 的正确姿势是什么？？</a><br><a href="https://zhuanlan.zhihu.com/p/34337889" target="_blank" rel="noopener">Python 管理包工具 Anaconda 安装过程常见问题解决办法</a></p><h6 id="Jupyter-Notebook"><a href="#Jupyter-Notebook" class="headerlink" title="Jupyter Notebook"></a>Jupyter Notebook</h6><p><a href="http://codingpy.com/article/getting-started-with-jupyter-notebook-part-1/" target="_blank" rel="noopener">Jupyter Notebook 快速入门（上）</a><br><a href="http://codingpy.com/article/getting-started-with-jupyter-notebook-part-2/" target="_blank" rel="noopener">Jupyter Notebook 快速入门（下）</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h5 id=&quot;工具篇&quot;&gt;&lt;a href=&quot;#工具篇&quot; class=&quot;headerlink&quot; title=&quot;工具篇&quot;&gt;&lt;/a&gt;工具篇&lt;/h5&gt;&lt;h6 id=&quot;Anaconda&quot;&gt;&lt;a href=&quot;#Anaconda&quot; class=&quot;headerlink&quot; title=&quot;Anaco
      
    
    </summary>
    
      <category term="Python" scheme="http://blog.dongfei.xin/categories/Python/"/>
    
      <category term="数据分析" scheme="http://blog.dongfei.xin/categories/Python/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"/>
    
    
      <category term="数据分析" scheme="http://blog.dongfei.xin/tags/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"/>
    
  </entry>
  
</feed>
